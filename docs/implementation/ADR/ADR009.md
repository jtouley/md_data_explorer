# ADR009: LLM-Enhanced User Experience and Query Intelligence

## Status
Proposed

## Related ADRs

- **[ADR001: Query Plan Producer, Filtering, and Chat-First Execution Rules](../ADR/ADR001.md)**: ADR001 owns QueryPlan production and Tier 3 LLM fallback. ADR009 extends ADR001's LLM usage beyond query parsing to include follow-ups, explanations, and result interpretation. ADR009's filter extraction enhancement aligns with ADR001's Phase 5 goal of moving away from regex.
- **[ADR003: Clinical Trust Architecture & Adaptive Terminology](../ADR/ADR003.md)**: ADR003 mentions local LLM (Ollama) as viable option for privacy-sensitive clinical data. ADR009 implements the LLM enhancements that ADR003 deferred to ADR001 or follow-on ADRs.

## Context

The clinical analytics platform currently uses a local Ollama LLM (Tier 3) only for query parsing fallback when pattern matching and semantic embeddings fail. The LLM is underutilized - it has significant potential to enhance user experience beyond basic query parsing.

### Current State

**LLM Usage (Limited)**:
- **Tier 3 Fallback**: Only used when Tier 1 (pattern matching) and Tier 2 (semantic embeddings) fail
- **QueryPlan Generation**: Returns structured JSON matching QueryPlan schema (Phase 5)
- **Privacy-Preserving**: Local Ollama only, no external API calls
- **Graceful Degradation**: Falls back to low-confidence stub (0.3) if unavailable

**Follow-Up Questions (Disabled)**:
- Hardcoded follow-up suggestions were removed per user request (see `_suggest_follow_ups()` in `03_ðŸ’¬_Ask_Questions.py`)
- User feedback: "we should also remove the stupid follow-ups unless they come from the LLM"
- Current TODO: "Add follow_ups field to QueryPlan schema and LLM prompt"

**LLM Infrastructure (Ready)**:
- `OllamaClient` with JSON mode support
- `OllamaManager` for service management
- RAG context building (`_build_rag_context()`)
- Structured prompt generation (`_build_llm_prompt()`)
- QueryPlan validation (`QueryPlan.from_dict()`)

### Problems

1. **Generic Follow-Ups**: Hardcoded follow-ups are context-agnostic and often irrelevant
   - Example: "Compare X by treatment group" when no treatment group exists
   - Example: "What predicts Y?" when user just asked a descriptive question
   - User feedback: "stupid follow-ups" - they don't understand query context

2. **Limited Query Intelligence**: LLM only used for parsing, not for:
   - Explaining query interpretation
   - Suggesting query refinements
   - Interpreting results
   - Explaining confidence scores
   - Error message improvement

3. **Filter Extraction Gaps**: Phase 5 goal is LLM-based filter extraction, but current regex-heavy approach still fails
   - "remove the n/a (0)" not extracted correctly
   - "exclude n/a" requires context inference
   - LLM could handle these naturally with proper prompting

4. **No Result Interpretation**: Users get raw statistics but no clinical insights
   - "Average LDL: 120 mg/dL" - is this good or bad?
   - "Count: 45 patients" - is this expected?
   - LLM could provide clinical context

5. **Poor Error Messages**: Technical errors don't help users understand what went wrong
   - "Column 'ldl_mg_dl' not found" - user asked for "LDL"
   - "Invalid operator: >=" - user doesn't understand filter syntax
   - LLM could translate technical errors to user-friendly explanations

## Decision

### Core Principle: LLM as Context-Aware Intelligence Layer

**The LLM should be used for all tasks requiring:**
- Natural language understanding
- Context awareness
- Clinical domain knowledge
- User-friendly explanations

**The LLM should NOT be used for:**
- Deterministic computations (use Polars/Ibis)
- Fast pattern matching (use regex - Tier 1)
- Simple template matching (use embeddings - Tier 2)
- Data transformations (use Polars expressions)

### Architecture: Multi-Purpose LLM Integration

Extend the LLM beyond query parsing to provide:

1. **Context-Aware Follow-Ups** (Primary)
2. **Query Explanation** (Secondary)
3. **Result Interpretation** (Secondary)
4. **Error Message Translation** (Secondary)
5. **Filter Extraction Enhancement** (Phase 5 alignment)
6. **Confidence Explanation** (Observability)
7. **Automated Golden Question Generation** (Self-Improvement)

## Implementation

### Phase 1: LLM-Generated Follow-Ups (Priority 0)

**Goal**: Replace hardcoded follow-ups with LLM-generated, context-aware suggestions.

**Schema Extension**:

```python
@dataclass
class QueryPlan:
    # ... existing fields ...
    follow_ups: list[str] = field(default_factory=list)  # LLM-generated follow-up questions
    follow_up_explanation: str = ""  # Why these follow-ups are relevant
```

**LLM Prompt Enhancement**:

```python
def _build_llm_prompt_with_follow_ups(self, query: str, context: dict, previous_result: dict | None = None) -> tuple[str, str]:
    """
    Build LLM prompt that requests QueryPlan + follow-up suggestions.

    Args:
        query: User's question
        context: RAG context (columns, aliases, examples)
        previous_result: Previous query result (for context-aware follow-ups)

    Returns:
        (system_prompt, user_prompt) tuple
    """
    system_prompt = """You are a medical data query parser and clinical research assistant.

Your tasks:
1. Parse the query into a QueryPlan (structured JSON)
2. Suggest 2-4 relevant follow-up questions based on:
   - The current query intent and results
   - Available columns in the dataset
   - Clinical research patterns (comparison, prediction, exploration)
   - What would be logically interesting next steps

Return JSON with:
- QueryPlan fields (intent, metric, group_by, filters, confidence, explanation)
- follow_ups: List of 2-4 natural language follow-up questions
- follow_up_explanation: Brief explanation of why these follow-ups are relevant

Available columns: {columns}
Aliases: {aliases}
Previous result context: {previous_result}

Examples:
{examples}

IMPORTANT:
- Follow-ups must be contextually relevant (not generic)
- Use natural language (not technical column names)
- Suggest questions that would actually help the user explore the data
- Avoid suggesting questions that are already answered by the current query""".format(
        columns=", ".join(context["columns"]),
        aliases=str(context["aliases"]),
        previous_result=str(previous_result) if previous_result else "None",
        examples="\n".join(f"- {ex}" for ex in context["examples"]),
    )

    user_prompt = f"Parse this query and suggest follow-ups: {query}"

    return (system_prompt, user_prompt)
```

**UI Integration**:

```python
def _render_llm_follow_ups(plan: QueryPlan, run_key: str) -> None:
    """
    Render LLM-generated follow-up questions.

    Only shows if plan.follow_ups is non-empty (LLM-generated).
    Replaces disabled _suggest_follow_ups() function.
    """
    if not plan.follow_ups:
        return  # No LLM follow-ups - don't show anything

    st.markdown("**ðŸ’¡ You might also ask:**")
    if plan.follow_up_explanation:
        st.caption(plan.follow_up_explanation)

    cols = st.columns(min(len(plan.follow_ups), 2))
    for idx, follow_up in enumerate(plan.follow_ups[:4]):  # Limit to 4
        col = cols[idx % len(cols)]
        with col:
            button_key = f"llm_followup_{run_key}_{idx}"
            if st.button(follow_up, key=button_key, use_container_width=True):
                st.session_state["prefilled_query"] = follow_up
                st.rerun()
```

**Benefits**:
- Context-aware: Follow-ups understand current query and results
- Clinical relevance: LLM can suggest medically meaningful next steps
- Natural language: Uses user's terminology, not technical column names
- Adaptive: Different follow-ups for different query types

**Trade-offs**:
- Latency: Adds ~1-2 seconds to query parsing (LLM call)
- Cost: Local Ollama only (no external cost, but requires model download)
- Quality: Depends on model quality (llama3.1:8b baseline, can upgrade)

### Phase 2: Query Explanation and Interpretation

**Goal**: Help users understand what the system interpreted from their query.

**Schema Extension**:

```python
@dataclass
class QueryPlan:
    # ... existing fields ...
    explanation: str = ""  # Already exists - enhance with LLM
    interpretation: str = ""  # Natural language interpretation of the query plan
    confidence_explanation: str = ""  # Why confidence is high/low
```

**LLM Prompt**:

```python
def _build_interpretation_prompt(self, plan: QueryPlan, query: str) -> tuple[str, str]:
    """
    Build prompt for LLM to explain query interpretation.

    Returns:
        (system_prompt, user_prompt) for interpretation generation
    """
    system_prompt = """You are a clinical data analysis assistant. Explain query interpretations in plain language.

Given a QueryPlan and the original query, provide:
1. interpretation: What the system understood from the query (plain language)
2. confidence_explanation: Why confidence is high/low (what made parsing easy/hard)

Be specific about:
- Which columns were matched
- What filters were extracted
- What grouping was detected
- Any ambiguities or assumptions made

Use clinical terminology when appropriate."""

    user_prompt = f"""Query: "{query}"

QueryPlan:
- Intent: {plan.intent}
- Metric: {plan.metric}
- Group by: {plan.group_by}
- Filters: {plan.filters}
- Confidence: {plan.confidence}

Explain what this query asks for and why confidence is {plan.confidence:.2f}."""

    return (system_prompt, user_prompt)
```

**UI Integration**:

```python
def _render_query_interpretation(plan: QueryPlan) -> None:
    """Render LLM-generated query interpretation."""
    if plan.interpretation:
        with st.expander("ðŸ” How I interpreted your query"):
            st.markdown(plan.interpretation)
            if plan.confidence_explanation:
                st.caption(f"**Confidence**: {plan.confidence_explanation}")
```

**Benefits**:
- Transparency: Users see what system understood
- Trust: Explains confidence scores
- Education: Helps users learn to write better queries

### Phase 3: Result Interpretation and Clinical Insights

**Goal**: Provide clinical context for query results.

**New Function**:

```python
def interpret_result_with_llm(
    plan: QueryPlan,
    result: dict,
    semantic_layer: SemanticLayer,
) -> dict[str, str]:
    """
    Use LLM to interpret query results and provide clinical insights.

    Returns:
        dict with:
        - summary: Brief summary of results
        - clinical_context: Clinical interpretation (normal/abnormal, expected/unexpected)
        - insights: Key insights or patterns detected
    """
    # Build prompt with result context
    system_prompt = """You are a clinical data analyst. Interpret query results and provide clinical insights.

Given query results, provide:
1. summary: Brief summary of what the results show
2. clinical_context: Clinical interpretation (is this normal? expected? concerning?)
3. insights: Key patterns or insights that stand out

Be specific and use clinical knowledge when relevant."""

    user_prompt = f"""Query: {plan.explanation}
Results: {result}

Provide clinical interpretation."""

    # Call LLM
    client = get_ollama_client()
    response = client.generate(user_prompt, system_prompt=system_prompt, json_mode=True)

    # Parse JSON response
    interpretation = json.loads(response) if response else {}

    return interpretation
```

**UI Integration**:

```python
def _render_result_interpretation(interpretation: dict) -> None:
    """Render LLM-generated result interpretation."""
    if not interpretation:
        return

    st.markdown("**ðŸ“Š Interpretation**")
    if interpretation.get("summary"):
        st.markdown(interpretation["summary"])
    if interpretation.get("clinical_context"):
        st.info(interpretation["clinical_context"])
    if interpretation.get("insights"):
        st.success(interpretation["insights"])
```

**Benefits**:
- Clinical relevance: Explains if results are normal/abnormal
- Education: Helps users understand what results mean
- Insights: Highlights patterns users might miss

**Trade-offs**:
- Additional LLM call: Adds latency after query execution
- Optional: Can be disabled for fast queries
- Quality: Depends on model's clinical knowledge

### Phase 4: Error Message Translation

**Goal**: Translate technical errors into user-friendly explanations.

**New Function**:

```python
def translate_error_with_llm(
    error: Exception,
    query: str,
    plan: QueryPlan | None,
    semantic_layer: SemanticLayer,
) -> str:
    """
    Use LLM to translate technical errors into user-friendly explanations.

    Returns:
        User-friendly error message
    """
    system_prompt = """You are a helpful assistant. Translate technical errors into plain language.

Given a technical error and the user's query, provide:
- What went wrong (in plain language)
- Why it happened
- What the user can do to fix it

Be specific and actionable."""

    user_prompt = f"""User query: "{query}"
QueryPlan: {plan}
Error: {str(error)}

Translate this error into a helpful explanation."""

    client = get_ollama_client()
    response = client.generate(user_prompt, system_prompt=system_prompt, json_mode=False)

    return response if response else str(error)  # Fallback to original error
```

**Integration**:

```python
# In execute_query_plan() error handling:
try:
    result = _execute_plan(plan)
except Exception as e:
    # Translate error with LLM
    user_friendly_error = translate_error_with_llm(e, query, plan, self)
    logger.error("query_execution_failed", error=str(e), user_error=user_friendly_error)
    return {
        "success": False,
        "error": user_friendly_error,
        "technical_error": str(e),  # Keep for debugging
    }
```

**Benefits**:
- User-friendly: "Column 'LDL' not found" â†’ "I couldn't find a column matching 'LDL'. Did you mean 'LDL mg/dL' or 'Total Cholesterol'?"
- Actionable: Suggests fixes
- Educational: Helps users understand system limitations

### Phase 5: Enhanced Filter Extraction (Phase 5 Alignment)

**Goal**: Use LLM for complex filter extraction instead of regex.

**Current Problem**: Regex patterns fail on:
- "remove the n/a (0)" - value in parentheses
- "exclude n/a" - requires context inference
- "patients on statins, exclude n/a" - multi-part with exclusion

**LLM Solution**:

```python
def _extract_filters_with_llm(
    self,
    query: str,
    context: dict,
    semantic_layer: SemanticLayer,
) -> list[FilterSpec]:
    """
    Use LLM to extract filter conditions from query.

    Returns:
        List of FilterSpec objects
    """
    system_prompt = """You are a query parser. Extract filter conditions from natural language queries.

Given a query, identify:
- Filter columns (match to available columns or aliases)
- Filter operators (==, !=, >, <, etc.)
- Filter values (extract from query text)
- Exclusion patterns ("exclude", "remove", "not")

Return JSON with filters array:
[
  {
    "column": "column_name",
    "operator": "!=",
    "value": 0,
    "exclude_nulls": true
  }
]

Available columns: {columns}
Aliases: {aliases}""".format(
        columns=", ".join(context["columns"]),
        aliases=str(context["aliases"]),
    )

    user_prompt = f"Extract filters from: {query}"

    client = self._get_ollama_client()
    response = client.generate(user_prompt, system_prompt=system_prompt, json_mode=True)

    if not response:
        return []  # Fallback to regex

    data = json.loads(response)
    filters = []
    for f in data.get("filters", []):
        filters.append(FilterSpec(
            column=f["column"],
            operator=f["operator"],
            value=f["value"],
            exclude_nulls=f.get("exclude_nulls", True),
        ))

    return filters
```

**Integration**:

```python
# In _llm_parse():
# Step 1: Extract filters with LLM
filters = self._extract_filters_with_llm(query, context, self.semantic_layer)

# Step 2: Build QueryPlan with LLM-extracted filters
plan = QueryPlan(
    intent=...,
    filters=filters,  # LLM-extracted
    ...
)
```

**Benefits**:
- Handles complex patterns: "remove the n/a (0)" â†’ extracts value from parentheses
- Context-aware: Understands "on statins" â†’ infers column
- Natural language: No regex patterns needed

**Trade-offs**:
- Latency: LLM call for filter extraction
- Fallback: Still use regex for simple patterns (Tier 1)

### Phase 6: Automated Golden Question Generation (Self-Improvement)

**Goal**: Use LLM to automatically generate and maintain golden questions from real user query logs, creating a self-improving test suite.

**Current State**:
- Golden questions are manually maintained in `tests/eval/golden_questions.yaml`
- QueryLogger already captures all user queries with parsing results and execution metadata
- EvalHarness evaluates parsing accuracy but doesn't automatically update test cases

**Problem**: Manual maintenance of golden questions is:
- Time-consuming: Requires manual analysis of query patterns
- Reactive: New patterns only added after issues are discovered
- Incomplete: May miss edge cases or high-frequency patterns
- Stale: Expected outputs may not reflect current system behavior

**LLM Solution**:

```python
def generate_golden_questions_from_logs(
    query_logger: QueryLogger,
    semantic_layer: SemanticLayer,
    llm_client: OllamaClient,
    min_confidence: float = 0.75,
    min_frequency: int = 3,
) -> list[dict[str, Any]]:
    """
    Use LLM to analyze query logs and generate golden questions.

    Args:
        query_logger: QueryLogger instance with query history
        semantic_layer: SemanticLayer for context
        llm_client: OllamaClient for LLM calls
        min_confidence: Minimum confidence to consider query successful
        min_frequency: Minimum occurrences to consider pattern important

    Returns:
        List of golden question dicts ready for YAML
    """
    # Load recent query logs
    logs = query_logger.load_recent_logs(days=30)

    # Filter successful queries
    successful_queries = [
        log for log in logs
        if log.get("query_plan", {}).get("confidence", 0) >= min_confidence
        and log.get("execution", {}).get("success", False)
    ]

    # Group by query pattern (LLM can identify similar patterns)
    system_prompt = """You are a test suite generator for a clinical analytics platform.

Analyze query logs and generate golden questions (regression test cases) that:
1. Capture high-frequency query patterns
2. Test complex queries (filters, grouping, multiple intents)
3. Cover edge cases (exclusions, ambiguous terms, null handling)
4. Represent real user queries (not synthetic examples)

For each query log entry, create a golden question with:
- id: unique identifier (descriptive, e.g., "count_with_exclusion_filter")
- query: the original query text
- expected_intent: from query_plan.intent
- expected_metric: from query_plan.metric (or null)
- expected_group_by: from query_plan.group_by (or null)
- expected_filters: from query_plan.filters (if any)
- notes: why this test case is valuable (frequency, complexity, edge case)

Focus on:
- Queries that test multiple features (filters + grouping)
- Queries with complex filter patterns ("remove n/a", "exclude missing")
- High-frequency patterns that should be regression-tested
- Edge cases that might break with code changes

Return JSON array of golden questions."""

    user_prompt = f"""Analyze these {len(successful_queries)} successful queries and generate golden questions:

{json.dumps(successful_queries[:100], indent=2)}

Available columns: {', '.join(semantic_layer.get_column_names())}
"""

    response = llm_client.generate(user_prompt, system_prompt=system_prompt, json_mode=True)
    golden_questions = json.loads(response) if response else []

    return golden_questions
```

**Gap Analysis**:

```python
def analyze_golden_question_coverage(
    golden_questions: list[dict],
    query_logs: list[dict],
    llm_client: OllamaClient,
) -> dict[str, Any]:
    """
    Use LLM to identify gaps in golden question coverage.

    Returns:
        dict with:
        - missing_patterns: Query types not covered
        - high_frequency_missing: Common queries missing from suite
        - edge_cases: Complex queries that should be tested
        - suggestions: LLM-generated recommendations
    """
    system_prompt = """Compare golden questions to actual query logs and identify gaps.

Find:
1. Query patterns in logs that aren't in golden questions
2. High-frequency queries missing from test suite
3. Edge cases (complex filters, ambiguous terms) that need coverage
4. Intent types that are under-tested

Return JSON with:
- missing_patterns: List of query patterns not covered
- high_frequency_missing: Queries that appear frequently but aren't tested
- edge_cases: Complex queries that should be added
- suggestions: Specific recommendations for new golden questions
"""

    user_prompt = f"""Golden questions ({len(golden_questions)}):
{json.dumps(golden_questions, indent=2)}

Query logs ({len(query_logs)} recent queries):
{json.dumps(query_logs[:50], indent=2)}

Identify coverage gaps."""

    response = llm_client.generate(user_prompt, system_prompt=system_prompt, json_mode=True)
    return json.loads(response) if response else {}
```

**Automatic Maintenance**:

```python
def maintain_golden_questions_automatically(
    query_logger: QueryLogger,
    golden_questions_path: Path,
    semantic_layer: SemanticLayer,
    llm_client: OllamaClient,
    dry_run: bool = False,
) -> dict[str, Any]:
    """
    Periodically analyze query logs and update golden questions.

    Can run as:
    - Background task (weekly/monthly)
    - Manual trigger (CLI command)
    - CI/CD step (before releases)

    Args:
        query_logger: QueryLogger with query history
        golden_questions_path: Path to golden_questions.yaml
        semantic_layer: SemanticLayer for context
        llm_client: OllamaClient for LLM calls
        dry_run: If True, only report changes without writing

    Returns:
        dict with:
        - new_questions: Questions to add
        - updated_questions: Questions to update
        - removed_questions: Obsolete questions to remove
        - coverage_gaps: Identified gaps
    """
    # Load existing golden questions
    existing = load_golden_questions(golden_questions_path)

    # Generate new questions from logs
    new_questions = generate_golden_questions_from_logs(
        query_logger, semantic_layer, llm_client
    )

    # Analyze coverage gaps
    logs = query_logger.load_recent_logs(days=30)
    gaps = analyze_golden_question_coverage(existing, logs, llm_client)

    # Use LLM to merge: identify duplicates, suggest updates, flag obsolete
    system_prompt = """Compare existing golden questions to newly generated ones.

Tasks:
1. Identify duplicates (same query pattern, different wording)
2. Find questions that need updating (expected outputs changed)
3. Flag obsolete questions (patterns no longer used)
4. Suggest new questions to add

Return JSON with:
- new_questions: Questions to add (not duplicates)
- updated_questions: Existing questions that need updates
- removed_questions: Obsolete questions to remove
- duplicates: Questions that are duplicates (keep existing)
"""

    user_prompt = f"""Existing golden questions ({len(existing)}):
{json.dumps(existing, indent=2)}

Newly generated questions ({len(new_questions)}):
{json.dumps(new_questions, indent=2)}

Merge and recommend changes."""

    response = llm_client.generate(user_prompt, system_prompt=system_prompt, json_mode=True)
    recommendations = json.loads(response) if response else {}

    if not dry_run:
        # Apply recommendations
        updated_questions = merge_golden_questions(
            existing, recommendations, golden_questions_path
        )
        return {
            "new_questions": recommendations.get("new_questions", []),
            "updated_questions": recommendations.get("updated_questions", []),
            "removed_questions": recommendations.get("removed_questions", []),
            "coverage_gaps": gaps,
            "total_questions": len(updated_questions),
        }

    return recommendations
```

**CLI Integration**:

```python
# New CLI command: tests/eval/maintain_golden_questions.py
def main():
    """CLI tool to maintain golden questions automatically."""
    parser = argparse.ArgumentParser(description="Maintain golden questions from query logs")
    parser.add_argument("--dry-run", action="store_true", help="Report changes without writing")
    parser.add_argument("--log-dir", default="data/query_logs", help="Query log directory")
    parser.add_argument("--output", default="tests/eval/golden_questions.yaml", help="Golden questions YAML path")

    args = parser.parse_args()

    query_logger = QueryLogger(args.log_dir)
    semantic_layer = create_semantic_layer_from_config()  # Or load from dataset
    llm_client = get_ollama_client()

    result = maintain_golden_questions_automatically(
        query_logger, Path(args.output), semantic_layer, llm_client, dry_run=args.dry_run
    )

    print(f"New questions: {len(result['new_questions'])}")
    print(f"Updated questions: {len(result['updated_questions'])}")
    print(f"Removed questions: {len(result['removed_questions'])}")
    print(f"Coverage gaps: {len(result['coverage_gaps'])}")
```

**Benefits**:
- Self-improving: Test suite grows automatically from real usage
- Real-world coverage: Tests actual user patterns, not synthetic examples
- Proactive: New patterns added before they cause issues
- Maintenance: LLM keeps golden questions up-to-date with system behavior
- Gap detection: Identifies untested query patterns

**Trade-offs**:
- LLM dependency: Requires Ollama running for maintenance
- Quality control: Generated questions need review (can be automated with validation)
- Frequency: Too frequent updates may cause churn (run weekly/monthly)
- Resource usage: Analyzing large log files requires memory/CPU

**Integration Points**:
- QueryLogger: Already logs all queries (no changes needed)
- EvalHarness: Can use generated questions immediately
- CI/CD: Can run maintenance as part of release process

## Implementation Plan

### Phase 1: LLM Follow-Ups (Priority 0)

**Tasks**:
1. Extend QueryPlan schema with `follow_ups` and `follow_up_explanation` fields
2. Update `_build_llm_prompt()` to request follow-ups
3. Update `_extract_query_intent_from_llm_response()` to parse follow-ups
4. Implement `_render_llm_follow_ups()` in UI
5. Remove disabled `_suggest_follow_ups()` function
6. Add tests for LLM follow-up generation

**Definition of Done**:
- [ ] QueryPlan includes `follow_ups` field
- [ ] LLM generates context-aware follow-ups
- [ ] UI renders LLM follow-ups (replaces hardcoded)
- [ ] Tests verify follow-up quality and relevance
- [ ] Graceful degradation if LLM unavailable (no follow-ups shown)

**Estimated Effort**: 2-3 days

### Phase 2: Query Explanation (Priority 1)

**Tasks**:
1. Enhance `explanation` field with LLM-generated interpretation
2. Add `confidence_explanation` field
3. Implement `_build_interpretation_prompt()`
4. Add `_render_query_interpretation()` UI component
5. Add tests for interpretation quality

**Estimated Effort**: 1-2 days

### Phase 3: Result Interpretation (Priority 2)

**Tasks**:
1. Implement `interpret_result_with_llm()` function
2. Add result interpretation to execution result
3. Add `_render_result_interpretation()` UI component
4. Make interpretation optional (feature flag)
5. Add tests for interpretation quality

**Estimated Effort**: 2-3 days

### Phase 4: Error Translation (Priority 2)

**Tasks**:
1. Implement `translate_error_with_llm()` function
2. Integrate into `execute_query_plan()` error handling
3. Add tests for error translation quality
4. Fallback to original error if LLM unavailable

**Estimated Effort**: 1-2 days

### Phase 5: Enhanced Filter Extraction (Priority 1 - Phase 5 Alignment)

**Tasks**:
1. Implement `_extract_filters_with_llm()` function
2. Integrate into `_llm_parse()` flow
3. Keep regex as fallback for simple patterns
4. Add tests for complex filter extraction
5. Remove regex-heavy filter extraction (Phase 4.1 revert)

**Estimated Effort**: 2-3 days

### Phase 6: Automated Golden Question Generation (Priority 2 - Self-Improvement)

**Tasks**:
1. Implement `generate_golden_questions_from_logs()` function
2. Implement `analyze_golden_question_coverage()` function
3. Implement `maintain_golden_questions_automatically()` function
4. Create CLI tool `tests/eval/maintain_golden_questions.py`
5. Add validation for generated golden questions (schema, uniqueness)
6. Add tests for golden question generation
7. Document maintenance workflow (when to run, how to review)

**Definition of Done**:
- [ ] LLM can generate golden questions from query logs
- [ ] LLM can identify coverage gaps
- [ ] CLI tool can maintain golden questions automatically
- [ ] Generated questions pass validation (schema, eval harness)
- [ ] Tests verify generation quality
- [ ] Documentation explains maintenance workflow

**Estimated Effort**: 3-4 days

## Alternatives Considered

### 1. Keep Hardcoded Follow-Ups

**Rejected**: User feedback explicitly requested removal. Hardcoded follow-ups are context-agnostic and often irrelevant.

### 2. Use External LLM API (OpenAI, Anthropic)

**Rejected**: Privacy requirement - clinical data must stay on-device. Local Ollama is the only acceptable option.

### 3. Template-Based Follow-Ups (No LLM)

**Rejected**: Templates are too rigid. LLM provides context-aware, adaptive suggestions.

### 4. Skip Result Interpretation (Users Can Interpret)

**Rejected**: Clinical context is valuable. Users may not know if "LDL: 120 mg/dL" is normal or concerning. LLM can provide clinical insights.

### 5. Regex-Only Filter Extraction

**Rejected**: Current regex approach fails on complex patterns. LLM handles natural language more naturally. Phase 5 plan explicitly moves away from regex.

## Consequences

### Positive

1. **Better UX**: Context-aware follow-ups are more relevant
2. **Clinical Value**: Result interpretation provides insights
3. **User Education**: Explanations help users learn
4. **Error Handling**: User-friendly errors reduce frustration
5. **Filter Extraction**: LLM handles complex patterns better than regex

### Negative

1. **Latency**: Additional LLM calls add 1-2 seconds per query
   - **Mitigation**: Make interpretation optional, cache follow-ups
2. **Model Dependency**: Quality depends on Ollama model
   - **Mitigation**: Graceful degradation, can upgrade models
3. **Resource Usage**: LLM requires RAM/CPU
   - **Mitigation**: Local only, user controls resource usage
4. **Complexity**: More LLM integration points
   - **Mitigation**: Centralized LLM client, clear error handling

### Risks

1. **LLM Unavailability**: If Ollama not running, features degrade gracefully
   - **Mitigation**: All features have fallbacks (no follow-ups, original errors, etc.)
2. **Quality Variability**: LLM output quality may vary
   - **Mitigation**: Validation, confidence thresholds, user feedback
3. **Privacy Concerns**: Even local LLM processes sensitive data
   - **Mitigation**: Local only, no external calls, user controls

## Success Criteria

1. **Follow-Ups**: Users find LLM follow-ups more relevant than hardcoded (user testing)
2. **Query Explanation**: Users understand what system interpreted (reduced confusion)
3. **Result Interpretation**: Users find clinical insights valuable (user feedback)
4. **Error Messages**: Users can fix errors based on LLM explanations (reduced support)
5. **Filter Extraction**: Complex filters extracted correctly (test coverage)
6. **Golden Questions**: Test suite grows automatically from real usage (coverage metrics improve)

## References

- **ADR001**: Query Plan Producer - defines QueryPlan schema
- **ADR003**: Clinical Trust Architecture - mentions local LLM as viable option
- **Phase 5 Plan**: LLM as Constrained Planner - aligns with filter extraction enhancement
- **User Feedback**: "we should also remove the stupid follow-ups unless they come from the LLM"
- **Code**: `src/clinical_analytics/core/nl_query_engine.py` - current LLM integration
- **Code**: `src/clinical_analytics/ui/pages/03_ðŸ’¬_Ask_Questions.py` - disabled follow-ups

## Notes

- **Privacy First**: All LLM usage is local-only (Ollama). No external API calls.
- **Graceful Degradation**: All features work without LLM (fallbacks to simpler behavior).
- **Quality Over Speed**: LLM calls add latency but improve UX significantly.
- **Phase 5 Alignment**: Enhanced filter extraction aligns with Phase 5 goal of moving away from regex.
- **Self-Improvement**: Phase 6 enables the system to improve its own test coverage automatically, creating a feedback loop where real usage drives test quality.
