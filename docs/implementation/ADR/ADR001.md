# ADR: Fix Comparison Analysis, Implement Filtering, and Conversational UI

## Status
Proposed

## Alignment with ADR002, ADR003, and ADR007 (North Stars)

**This ADR must be implemented in alignment with:**
1. **[ADR002: Persistent Storage Layer](../ADR/ADR002.md)** - Establishes architectural north star for upload handling
2. **[ADR003: Clinical Trust Architecture & Adaptive Terminology](../ADR/ADR003.md)** - Trust/verification UI and adaptive dictionary overlap with filter extraction and conversational UI
3. **[ADR007: Feature Parity Architecture](../ADR/ADR007.md)** - **AUTHORITATIVE SOURCE** for feature parity requirements

### Critical Principle: Feature Parity (No Exceptions)

**ADR007 establishes the core principle: "Single-Table = Multi-Table with 1 Table"**

This means:
- âœ… **Single-file uploads** (CSV, Excel, SPSS) are a special case of multi-table uploads, not a different architecture
- âœ… **Both upload types must use identical code paths** - no conditional logic based on upload type
- âœ… **Both upload types must have identical capabilities** - persistence, query capabilities, analysis features, UI features
- âœ… **No feature gap is acceptable** - any capability added for one type must be available to the other

### Feature Parity Requirements (From ADR007)

ADR007 defines the following required parity (see ADR007 for complete details):

- âœ… **Persistence**: Both save individual tables to disk (single-table = 1 table in `{upload_id}_tables/`)
- âœ… **Semantic Layer**: Both register all tables in DuckDB identically
- âœ… **Query Capabilities**: Both support same granularity levels (patient_level, admission_level, event_level)
- âœ… **Lazy Evaluation**: Both use Polars lazy frames (not eager pandas)
- âœ… **Metadata Schema**: Both use `inferred_schema` format (not `variable_mapping` for single-table)
- âœ… **Validation Pipeline**: Both go through same `DataQualityValidator.validate_complete()` pipeline
- âœ… **Analysis Features**: Comparison analysis, filtering, descriptive stats work identically
- âœ… **UI Features**: Conversational UI, follow-up questions, conversation history work identically

**This ADR's implementation must ensure all new features (comparison analysis fix, filtering, conversational UI) work identically for both upload types, following ADR007's unified architecture.**

### Reference Architecture

- **ADR007**: **AUTHORITATIVE SOURCE** for feature parity - defines unified code paths, persistence, and query capabilities
- **ADR002**: Defines persistent storage, DuckDB registration, Parquet export, lazy evaluation - all features must apply to both upload types
- **Multi-Table Handler Refactor Plan** (`.cursor/plans/multi-table_handler_refactor_aggregate-before-join_architecture_b7ca2b5e.plan.md`): Defines advanced features like table classification, aggregate-before-join, partitioning - these patterns should inform single-file handling where applicable
- **Consolidate Docs Plan**: Shows multi-table deferred to V2, but infrastructure exists - single-file must match this infrastructure

## Context

The clinical analytics platform has three critical defects blocking physician usage:

1. **Comparison Analysis Bug**: String numeric columns (e.g., "Viral Load" with values like "<20", "120") are incorrectly treated as categorical, causing the system to compare group counts instead of group means. This produces wrong clinical conclusions (selecting least common treatment instead of treatment with lowest mean viral load).

   **Related Issue (2025-12-29)**: Inverse problem also observed - categorical variables encoded as 1/2 (e.g., "Statin Prescribed? 1: Yes 2: No") are incorrectly treated as numeric, computing meaningless statistics (mean=1.41) instead of counts/percentages. Query "how many patients on statins?" should return count/percentage, not mean/median. System needs better data type detection and query intent understanding.

2. **Missing Filter Support**: Queries like "what was the average t score of those that had osteoporosis (with scores below -2.5)" contain filter conditions that are parsed but never applied to the data, resulting in incorrect cohort selection and invalid statistical results.

3. **Non-Conversational UI**: The Ask Questions page uses confusing buttons ("Start Over", "Clear Results", "Confirm and Run"), lacks conversation history, and forces page resets between queries. Physicians asking follow-up questions (observed in user testing) must re-enter context, breaking analytical flow.

These issues prevent the platform from supporting iterative clinical research workflows where physicians refine questions based on initial results.

## Decision

Implement fixes in four phases, executed sequentially. **Phase 0 (Feature Parity) must be completed first** to ensure all subsequent phases work identically for single-file and multi-table uploads.

### Phase 0: Ensure Feature Parity (Prerequisite) âš ï¸

**Goal**: Eliminate any feature disparity between single-file and multi-table uploads before implementing new features. Both upload types must be first-class citizens with identical capabilities.

**Alignment with ADR007**: This phase directly implements [ADR007: Feature Parity Architecture](../ADR/ADR007.md), which establishes the principle "Single-Table = Multi-Table with 1 Table" and defines unified code paths.

**Implementation Strategy**:

This phase implements ADR007's 5-phase strategy:

1. **Normalize Upload Handling** (ADR007 Phase 1)
   - Create `normalize_upload_to_table_list()` function
   - Both upload types normalize to same table list structure
   - Single-table â†’ `[{"name": "table_0", "data": df}]`
   - Multi-table â†’ `[{"name": "table_0", "data": df1}, {"name": "table_1", "data": df2}, ...]`

2. **Unify Persistence** (ADR007 Phase 2)
   - Both upload types save individual tables to `{upload_id}_tables/` directory
   - Both save unified cohort CSV (for backward compatibility)
   - Both use `inferred_schema` metadata format (convert `variable_mapping` during save)

3. **Unify Semantic Layer Registration** (ADR007 Phase 3)
   - Both upload types register all tables in DuckDB identically
   - Remove hardcoded `patient_level` restriction
   - Both support all granularity levels (patient_level, admission_level, event_level)

4. **Unify Data Access** (ADR007 Phase 4)
   - Both upload types return Polars lazy frames (not pandas DataFrames)
   - Both support lazy evaluation and predicate pushdown
   - Remove eager pandas loading paths

5. **Remove Conditional Logic** (ADR007 Phase 5)
   - Eliminate all `if upload_type == "single"` conditionals
   - Use unified code paths for both upload types
   - Verify identical behavior through tests

**Success Criteria** (from ADR007):
- [ ] Single-table and multi-table uploads have identical persistence structure
- [ ] Both upload types register all tables in DuckDB semantic layer
- [ ] Both upload types support lazy Polars evaluation
- [ ] Both upload types use same metadata schema (`inferred_schema` format)
- [ ] Both upload types go through same validation pipeline
- [ ] Both upload types support all granularity levels
- [ ] No conditional logic based on upload type exists in codebase
- [ ] Same query produces identical results for both upload types

**Implementation Notes**:
- **See ADR007 for complete implementation details, code examples, and migration strategy**
- This phase must be completed before Phase 1-3 to ensure new features work for both upload types
- All subsequent phases assume feature parity is established per ADR007

### Phase 1: Fix Comparison Analysis Bug

#### 1.1 Update `compute_comparison_analysis()` to try numeric conversion first

**File**: `src/clinical_analytics/analysis/compute.py`

Before determining test type (currently line 388-389), attempt to convert the outcome column to numeric using `_try_convert_to_numeric()` (the same helper already used in descriptive analysis):

```python
def compute_comparison_analysis(df: pl.DataFrame, context: AnalysisContext) -> dict[str, Any]:
    outcome_col = context.primary_variable
    group_col = context.grouping_variable
    
    # Clean data
    analysis_df = df.select([outcome_col, group_col]).drop_nulls()
    
    # CRITICAL FIX: Try numeric conversion FIRST, before deciding test type
    outcome_series = analysis_df[outcome_col]
    numeric_outcome = None
    outcome_is_numeric = False
    
    # Check if already numeric
    if outcome_series.dtype in (pl.Int64, pl.Float64):
        outcome_is_numeric = True
        numeric_outcome = outcome_series
    else:
        # Try to convert string columns to numeric (handles "<20", "120", etc.)
        numeric_outcome = _try_convert_to_numeric(outcome_series)
        if numeric_outcome is not None:
            outcome_is_numeric = True
            # Replace outcome column with numeric version
            analysis_df = analysis_df.with_columns(numeric_outcome.alias(outcome_col))
    
    # Now proceed with numeric vs categorical logic
    if outcome_is_numeric:
        # Use t-test/ANOVA path (existing code lines 398-464)
        ...
    else:
        # Use chi-square path (existing code lines 466-516)
        ...
```

**Rationale**: String columns containing numeric data (like "Viral Load" with "<20", "120") must be treated as numeric and use mean-based comparisons, not count-based. This ensures clinically correct answers.

#### 1.2 Add tests for string numeric conversion in comparison

**File**: `tests/analysis/test_compute.py`

Add test cases:
- String numeric outcome with "<20" style values (below detection limit)
- String numeric outcome with European comma format
- Verify group means are computed correctly (not counts)
- Verify test selection logic chooses t-test/ANOVA for converted numerics

### Phase 2: Implement Filter Parsing and Application

#### 2.1 Add filter extraction to NL Query Engine

**File**: `src/clinical_analytics/core/nl_query_engine.py`

**Note**: This coordinates with ADR003 Phase 2 (Enhanced LLM Parser) - filter extraction can leverage enhanced LLM parsing for complex filter conditions.

**Real-World Failure Evidence (2025-12-29)**: 

Query **"Average LDL for those patients not on statin?"** failed completely:
- Filter condition **"not on statin"** was not extracted (should map to `{"column": "Statin Prescribed?", "operator": "==", "value": "2: No"}`)
- Primary variable was wrong (`Age` instead of `LDL mg/dL`)
- Result: System computed average Age for all patients (not filtered, wrong variable)

This demonstrates critical need for filter extraction implementation. The query contains clear filter condition ("those patients not on statin") that must be parsed and applied before analysis.

**New Method**: `_extract_filters(query: str) -> list[dict[str, Any]]`

Extract filter conditions from query text:

```python
def _extract_filters(self, query: str) -> list[dict[str, Any]]:
    """
    Extract filter conditions from query text.
    
    Patterns to detect:
    - "those that had X" / "patients with X" â†’ categorical filter
    - "scores below/above X" â†’ numeric range filter
    - "with X" / "without X" â†’ presence filter
    
    Returns:
        List of filter conditions:
        [
            {"column": "Results of DEXA?", "operator": "==", "value": "Osteoporosis"},
            {"column": "DEXA Score (T score)", "operator": "<", "value": -2.5}
        ]
    """
    filters = []
    query_lower = query.lower()
    
    # Pattern 1: Categorical filters ("those that had X", "patients with X")
    # Match against semantic layer column aliases
    # ...
    
    # Pattern 2: Numeric range filters ("below X", "above X", "less than X")
    # Extract numeric value and operator
    # ...
    
    # Pattern 3: "without X" â†’ inverse filter
    # ...
    
    return filters
```

**Integration**: Call `_extract_filters()` in `parse_query()` and populate `QueryIntent.filters`.

**Data Structure Change**: Filters are stored as a list of dicts (not a single dict) to support multiple conditions on different columns or multiple conditions on the same column.

**Example from Real Failure**:
```python
# Query: "Average LDL for those patients not on statin?"
# Should extract:
filters = [
    {
        "column": "Statin Prescribed?",
        "operator": "==",
        "value": "2: No",  # Maps "not on statin" to column value
        "negation": False  # "not" is handled via value mapping
    }
]
# Then apply filter before computing average LDL
```

**Note**: This requires coordination with ADR003 Phase 2 (Enhanced LLM Parser) for robust negation handling and column value mapping. Pattern matching alone may not handle "not on statin" â†’ "Statin Prescribed? == 2: No" mapping correctly.

#### 2.2 Add filters field to AnalysisContext

**File**: `src/clinical_analytics/ui/components/question_engine.py`

```python
@dataclass
class AnalysisContext:
    # ... existing fields ...
    filters: list[dict[str, Any]] = field(default_factory=list)
```

**Propagation**: In `ask_free_form_question()`, copy `query_intent.filters` to `context.filters`.

#### 2.3 Apply filters in compute functions

**File**: `src/clinical_analytics/analysis/compute.py`

**New Helper**: `_apply_filters(df: pl.DataFrame, filters: list[dict[str, Any]]) -> pl.DataFrame`

```python
def _apply_filters(df: pl.DataFrame, filters: list[dict[str, Any]]) -> pl.DataFrame:
    """
    Apply filter conditions to DataFrame.
    
    Args:
        df: Input DataFrame
        filters: List of filter conditions:
            [{"column": "col_name", "operator": "==", "value": "X"},
             {"column": "col_name", "operator": "<", "value": 2.5}]
    
    Returns:
        Filtered DataFrame
    """
    # Operator dispatch table
    OPERATORS = {
        "==": lambda col, val: pl.col(col) == val,
        "!=": lambda col, val: pl.col(col) != val,
        "<": lambda col, val: pl.col(col) < val,
        ">": lambda col, val: pl.col(col) > val,
        "<=": lambda col, val: pl.col(col) <= val,
        ">=": lambda col, val: pl.col(col) >= val,
        "in": lambda col, val: pl.col(col).is_in(val),
    }
    
    filtered_df = df
    for condition in filters:
        col_name = condition.get("column")
        operator = condition.get("operator")
        value = condition.get("value")
        exclude_nulls = condition.get("exclude_nulls", True)
        
        if col_name not in df.columns:
            continue  # Skip if column not found
        
        # Apply null exclusion if requested (default: yes)
        if exclude_nulls:
            filtered_df = filtered_df.filter(pl.col(col_name).is_not_null())
        
        # Apply operator
        op_func = OPERATORS.get(operator)
        if op_func:
            filtered_df = filtered_df.filter(op_func(col_name, value))
    
    return filtered_df
```

**Integration**: 
- In `compute_descriptive_analysis()`: Apply filters at the start (line ~340), before computing statistics
- In `compute_comparison_analysis()`: Apply filters at the start (line ~390), before grouping
- Track filtered vs unfiltered counts for breakdown reporting
- **Feature Parity**: Filter application must work identically for both single-file and multi-table uploads, using the same lazy Polars evaluation pattern

#### 2.4 Enhanced breakdown reporting

**File**: `src/clinical_analytics/analysis/compute.py`

**Note**: This coordinates with ADR003 Phase 1 (Trust Layer) - breakdown reporting provides foundation for verification UI.

**Modify `compute_descriptive_analysis()`** to return breakdown info:

```python
def compute_descriptive_analysis(df: pl.DataFrame, context: AnalysisContext) -> dict[str, Any]:
    # Store original count
    original_count = len(df)
    
    # Apply filters
    if context.filters:
        df = _apply_filters(df, context.filters)
    
    # ... existing analysis logic ...
    
    return {
        # ... existing fields ...
        "original_count": original_count,
        "filtered_count": len(df),
        "filters_applied": context.filters,
        "filter_description": _format_filter_description(context.filters),
    }
```

**New Helper**: `_format_filter_description(filters: list[dict]) -> str`

```python
def _format_filter_description(filters: list[dict[str, Any]]) -> str:
    """Convert filter list to human-readable description."""
    if not filters:
        return ""
    
    descriptions = []
    for f in filters:
        col = f["column"]
        op = f["operator"]
        val = f["value"]
        
        op_text = {
            "==": "equals",
            "!=": "does not equal",
            "<": "less than",
            ">": "greater than",
            "<=": "at most",
            ">=": "at least",
        }.get(op, op)
        
        descriptions.append(f"{col} {op_text} {val}")
    
    return "; ".join(descriptions)
```

**File**: `src/clinical_analytics/ui/pages/3_ðŸ’¬_Ask_Questions.py`

**Update rendering functions** to show breakdown:

```python
def _render_focused_descriptive(result: dict) -> None:
    # Show breakdown if filters were applied
    if result.get("filters_applied"):
        st.markdown("### Data Breakdown")
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Matching Criteria", f"{result['filtered_count']:,}")
        with col2:
            st.metric("Excluded", f"{result['original_count'] - result['filtered_count']:,}")
        
        if result.get("filter_description"):
            st.caption(f"**Filters:** {result['filter_description']}")
        
        st.divider()
    
    # Show headline answer
    if "headline" in result:
        st.info(f"ðŸ“‹ **Answer:** {result['headline']}")
    
    # ... rest of rendering
```

Apply similar breakdown display to `_render_focused_comparison()`.

### Phase 3: Redesign UI for Conversational Flow

**Note**: This coordinates with ADR003 Phase 1 (Trust Layer) - conversational UI should include verification expanders in each result message.

#### 3.1 Conversation history data structure

**File**: `src/clinical_analytics/ui/pages/3_ðŸ’¬_Ask_Questions.py`

**New Session State Structure**:

```python
# Initialize conversation history
if "conversation_history" not in st.session_state:
    st.session_state["conversation_history"] = []

# Each entry (lightweight storage):
{
    "query": str,  # Original user query
    "intent": AnalysisIntent,  # Parsed intent
    "headline": str,  # Summary result (not full result dict)
    "run_key": str,  # Unique key to reference full result if needed
    "timestamp": float,  # Unix timestamp
    "filters_applied": list[dict],  # For audit trail
}
```

**Rationale**: Store lightweight summaries (headline text, not full result dicts with arrays/charts) to prevent session state bloat. Full results can be reconstructed on demand via `run_key`.

#### 3.2 Redesign main UI flow

**File**: `src/clinical_analytics/ui/pages/3_ðŸ’¬_Ask_Questions.py`

**Remove Completely**:
- "Start Over" button
- "Clear Results" button
- "Confirm and Run" button
- Low confidence variable selection expanders

**New Flow**:

```python
def main():
    st.title("ðŸ’¬ Ask Questions")
    
    # Display conversation history (if any)
    if st.session_state.get("conversation_history"):
        for entry in st.session_state.conversation_history:
            with st.chat_message("user"):
                st.write(entry["query"])
            
            with st.chat_message("assistant"):
                st.info(entry["headline"])
                
                # Expandable details
                with st.expander("View detailed results"):
                    # Reconstruct or display cached details
                    st.write("Detailed analysis here...")
    
    # Always show query input at bottom (sticky)
    query = st.chat_input("Ask a question about your data...")
    
    if query:
        # Parse and execute
        context = QuestionEngine.ask_free_form_question(
            query=query,
            data_preview=st.session_state["data_preview"],
            semantic_layer=st.session_state["semantic_layer"],
        )
        
        if context and context.execution_ready:
            # Execute analysis
            result = execute_analysis(
                df=st.session_state["uploaded_data"],
                context=context,
            )
            
            # Generate unique run key
            run_key = f"{context.inferred_intent.value}_{int(time.time()*1000)}"
            
            # Add to conversation history (lightweight)
            st.session_state.conversation_history.append({
                "query": query,
                "intent": context.inferred_intent,
                "headline": result.get("headline") or result.get("headline_text"),
                "run_key": run_key,
                "timestamp": time.time(),
                "filters_applied": context.filters,
            })
            
            # Rerun to display new message
            st.rerun()
```

**Handling Low Confidence Variable Matches**:

Instead of showing variable selection UI in expanders, display inline clarification in chat:

```python
if context and not context.execution_ready:
    # Low confidence - ask for clarification inline
    with st.chat_message("assistant"):
        st.warning("I found multiple possible matches. Which did you mean?")
        
        # Show options as buttons
        for idx, option in enumerate(context.candidate_variables):
            if st.button(
                f"{option['column']} ({option['confidence']:.0%} confidence)",
                key=f"clarify_{idx}_{run_key}",
                use_container_width=True,
            ):
                # Update context with selected variable
                st.session_state["pending_clarification"] = {
                    "context": context,
                    "selected_variable": option["column"],
                }
                st.rerun()
```

#### 3.3 Inline result rendering

**File**: `src/clinical_analytics/ui/pages/3_ðŸ’¬_Ask_Questions.py`

Results are now rendered inline within the `st.chat_message("assistant")` block (shown in 3.2 above). No separate page or section.

**Detailed results** are placed in an `st.expander()` within the assistant message to keep the conversation flow clean.

#### 3.4 Follow-up question suggestions

**File**: `src/clinical_analytics/ui/pages/3_ðŸ’¬_Ask_Questions.py`

**New Function**: `_suggest_follow_ups(context: AnalysisContext, result: dict) -> None`

```python
def _suggest_follow_ups(context: AnalysisContext, result: dict) -> None:
    """Suggest natural follow-up questions based on current result."""
    
    st.markdown("**ðŸ’¡ You might also ask:**")
    
    suggestions = []
    
    if context.inferred_intent == AnalysisIntent.DESCRIBE:
        if context.primary_variable:
            suggestions.append(f"Compare {context.primary_variable} by treatment group")
            suggestions.append(f"What predicts {context.primary_variable}?")
    
    elif context.inferred_intent == AnalysisIntent.COMPARE_GROUPS:
        if context.grouping_variable and context.primary_variable:
            suggestions.append(f"Show distribution of {context.primary_variable} by {context.grouping_variable}")
            suggestions.append(f"What else affects {context.primary_variable}?")
    
    # Render as clickable buttons that populate chat input
    for suggestion in suggestions:
        if st.button(suggestion, key=f"suggest_{hash(suggestion)}", use_container_width=True):
            st.session_state["prefilled_query"] = suggestion
            st.rerun()
```

**Integration**: Call `_suggest_follow_ups()` at the end of each assistant message block.

**Optional Enhancement**: If `st.session_state.get("prefilled_query")` exists, pre-populate the `st.chat_input()` with it (if Streamlit supports this; otherwise, display as text above input).

#### 3.5 Conversation management controls

**File**: `src/clinical_analytics/ui/pages/3_ðŸ’¬_Ask_Questions.py`

**Add to sidebar**:

```python
with st.sidebar:
    st.markdown("### Conversation")
    
    if st.button("Clear Conversation", use_container_width=True):
        st.session_state.conversation_history = []
        st.rerun()
    
    # Optional: Export conversation as JSON
    if st.session_state.get("conversation_history"):
        conversation_json = json.dumps(
            st.session_state.conversation_history,
            indent=2,
            default=str,
        )
        st.download_button(
            "Export Conversation",
            data=conversation_json,
            file_name=f"conversation_{int(time.time())}.json",
            mime="application/json",
            use_container_width=True,
        )
```

**Rationale**: Conversation controls in sidebar keep main area clean. Export supports audit trail for clinical research reproducibility.

### Phase 4: Testing

**Critical**: All tests must verify functionality works identically for both single-file and multi-table uploads. Feature parity is not optional.

#### 4.1 Test comparison analysis with string numeric columns

**File**: `tests/analysis/test_compute.py`

Add test cases:

```python
def test_comparison_analysis_with_string_numeric_outcome():
    """Test that string numeric columns are converted and means computed."""
    df = pl.DataFrame({
        "Treatment": ["A", "A", "B", "B"],
        "Viral Load": ["<20", "120", "200", "150"],
    })
    
    context = AnalysisContext(
        primary_variable="Viral Load",
        grouping_variable="Treatment",
        inferred_intent=AnalysisIntent.COMPARE_GROUPS,
    )
    
    result = compute_comparison_analysis(df, context)
    
    # Verify group means were computed (not counts)
    assert "group_statistics" in result
    # Treatment A: mean of [20, 120] = 70
    # Treatment B: mean of [200, 150] = 175
    # (Note: "<20" converts to 20 as upper bound)
    assert result["group_statistics"]["A"]["mean"] == pytest.approx(70, rel=0.01)
    assert result["group_statistics"]["B"]["mean"] == pytest.approx(175, rel=0.01)
    
    # Verify test type is numeric (t-test or ANOVA)
    assert result["test_type"] in ["t-test", "ANOVA"]

def test_comparison_analysis_with_european_comma_format():
    """Test European comma format conversion (e.g., '1,234.5')."""
    df = pl.DataFrame({
        "Treatment": ["A", "B"],
        "Score": ["1,234.5", "2,345.6"],
    })
    
    context = AnalysisContext(
        primary_variable="Score",
        grouping_variable="Treatment",
        inferred_intent=AnalysisIntent.COMPARE_GROUPS,
    )
    
    result = compute_comparison_analysis(df, context)
    
    # Verify conversion and mean computation
    assert result["group_statistics"]["A"]["mean"] == pytest.approx(1234.5)
    assert result["group_statistics"]["B"]["mean"] == pytest.approx(2345.6)
```

#### 4.2 Test filter parsing

**File**: `tests/core/test_nl_query_engine_filters.py` (new)

```python
def test_extract_categorical_filter():
    """Test extraction of categorical filters from query."""
    engine = NLQueryEngine(semantic_layer=mock_semantic_layer)
    
    query = "what was the average t score of those that had osteoporosis"
    filters = engine._extract_filters(query)
    
    assert len(filters) == 1
    assert filters[0]["column"] == "Results of DEXA?"
    assert filters[0]["operator"] == "=="
    assert filters[0]["value"] == "Osteoporosis"

def test_extract_numeric_range_filter():
    """Test extraction of numeric range filters."""
    engine = NLQueryEngine(semantic_layer=mock_semantic_layer)
    
    query = "show patients with scores below -2.5"
    filters = engine._extract_filters(query)
    
    assert len(filters) == 1
    assert filters[0]["column"] == "DEXA Score (T score)"
    assert filters[0]["operator"] == "<"
    assert filters[0]["value"] == -2.5

def test_extract_multiple_filters():
    """Test extraction of multiple filters in one query."""
    engine = NLQueryEngine(semantic_layer=mock_semantic_layer)
    
    query = "average t score of those that had osteoporosis with scores below -2.5"
    filters = engine._extract_filters(query)
    
    assert len(filters) == 2
    # Order may vary
    filter_columns = {f["column"] for f in filters}
    assert "Results of DEXA?" in filter_columns
    assert "DEXA Score (T score)" in filter_columns

def test_apply_filters():
    """Test filter application to DataFrame."""
    df = pl.DataFrame({
        "Results of DEXA?": ["Osteoporosis", "Normal", "Osteoporosis", "Osteopenia"],
        "DEXA Score (T score)": [-3.0, -0.5, -2.8, -1.5],
    })
    
    filters = [
        {"column": "Results of DEXA?", "operator": "==", "value": "Osteoporosis"},
        {"column": "DEXA Score (T score)", "operator": "<", "value": -2.5},
    ]
    
    filtered_df = _apply_filters(df, filters)
    
    # Should only include rows where both conditions are met
    assert len(filtered_df) == 2  # Rows 0 and 2
    assert filtered_df["DEXA Score (T score)"].to_list() == [-3.0, -2.8]

def test_filters_work_identically_single_file_and_multi_table():
    """Test that filter application works identically for both upload types."""
    # Create test data for single-file upload
    single_file_df = pl.DataFrame({
        "patient_id": [1, 2, 3, 4],
        "outcome": [0, 1, 0, 1],
        "age": [50, 60, 70, 80],
    })
    
    # Create test data for multi-table upload (unified cohort)
    multi_table_df = pl.DataFrame({
        "patient_id": [1, 2, 3, 4],
        "outcome": [0, 1, 0, 1],
        "age": [50, 60, 70, 80],
    })
    
    filters = [{"column": "age", "operator": ">=", "value": 60}]
    
    # Apply filters to both
    single_filtered = _apply_filters(single_file_df, filters)
    multi_filtered = _apply_filters(multi_table_df, filters)
    
    # Results must be identical
    assert len(single_filtered) == len(multi_filtered)
    assert single_filtered["age"].to_list() == multi_filtered["age"].to_list()
```

#### 4.3 Test conversational UI flow

**File**: `tests/ui/pages/test_ask_questions_conversational.py` (new)

```python
def test_conversation_history_persistence():
    """Test that conversation history is stored correctly."""
    # Initialize session state
    st.session_state["conversation_history"] = []
    
    # Simulate adding a query
    entry = {
        "query": "what is the average viral load",
        "intent": AnalysisIntent.DESCRIBE,
        "headline": "Average viral load: 150 copies/mL",
        "run_key": "describe_123456",
        "timestamp": time.time(),
        "filters_applied": [],
    }
    st.session_state.conversation_history.append(entry)
    
    # Verify storage
    assert len(st.session_state.conversation_history) == 1
    assert st.session_state.conversation_history[0]["query"] == "what is the average viral load"

def test_conversation_history_lightweight_storage():
    """Test that only lightweight data is stored, not full results."""
    entry = {
        "query": "compare viral load by treatment",
        "intent": AnalysisIntent.COMPARE_GROUPS,
        "headline": "Treatment A has lower viral load (p<0.05)",
        "run_key": "compare_789012",
        "timestamp": time.time(),
        "filters_applied": [],
    }
    
    # Verify no large objects (arrays, dataframes) are stored
    import sys
    entry_size = sys.getsizeof(json.dumps(entry, default=str))
    assert entry_size < 1024  # Should be under 1KB per entry

def test_clear_conversation():
    """Test clearing conversation history."""
    st.session_state["conversation_history"] = [
        {"query": "test1", "headline": "result1"},
        {"query": "test2", "headline": "result2"},
    ]
    
    # Simulate clear button
    st.session_state.conversation_history = []
    
    assert len(st.session_state.conversation_history) == 0
```

## Implementation Order

Execute phases sequentially to maintain code quality and testability. **Phase 0 must be completed first** to ensure feature parity:

0. **Phase 0** (Feature Parity Prerequisite): Ensure single-file and multi-table uploads have identical capabilities
   - Estimated time: 2-3 hours
   - Deliverable: Both upload types use same persistence, semantic layer registration, and data access patterns
   - Commit messages:
     - `feat: unify single-file and multi-table upload persistence`
     - `feat: ensure both upload types register in DuckDB semantic layer identically`
     - `feat: replace eager pandas with lazy Polars for both upload types`
   - **Must complete before Phase 1**

1. **Phase 1** (Critical Bug Fix): Fix comparison analysis to handle string numeric columns
   - Estimated time: 2 hours
   - Deliverable: Comparison analysis correctly computes group means for string numeric columns
   - Commit message: `fix: handle string numeric columns in comparison analysis`

2. **Phase 2** (Core Feature): Implement filter parsing and application
   - Estimated time: 3 hours
   - Deliverable: Filters are extracted from NL queries and applied to cohort before analysis
   - Commit messages: 
     - `feat: add filter extraction to NL query engine`
     - `feat: apply filters in compute functions`
     - `feat: add breakdown reporting for filtered cohorts`

3. **Phase 3** (UX Improvement): Redesign UI for conversational flow
   - Estimated time: 3-4 hours
   - Deliverable: Clean conversation flow with inline results, no confusing buttons
   - Commit messages:
     - `feat: add conversation history to session state`
     - `refactor: redesign Ask Questions UI for conversational flow`
     - `feat: add follow-up question suggestions`

4. **Phase 4** (Quality): Add comprehensive tests
   - Estimated time: 2 hours
   - Deliverable: Test coverage for all three phases
   - Commit message: `test: add comprehensive tests for comparison fix, filters, and conversational UI`

## Success Criteria

### Phase 0: Feature Parity (Prerequisite)
- [ ] Single-file and multi-table uploads have identical persistence mechanisms
- [ ] Both types register tables in DuckDB semantic layer identically
- [ ] Both types support lazy Polars evaluation (not eager pandas)
- [ ] Both types use same metadata schema (`inferred_schema` format)
- [ ] Both types go through same validation pipeline
- [ ] Both types support all granularity levels (patient_level, admission_level, event_level)
- [ ] No feature disparity exists between upload types

### Phase 1-3: Core Features
- [ ] Comparison analysis correctly computes group means for string numeric columns (not counts)
- [ ] Comparison analysis works identically for single-file and multi-table uploads
- [ ] Filter conditions from NL queries are parsed and applied correctly
- [ ] Filter application works identically for both upload types (using lazy Polars)
- [ ] Filtered cohort size is displayed in breakdown reporting
- [ ] UI shows clean conversation flow: question â†’ data â†’ question
- [ ] Conversational UI works identically for both upload types
- [ ] No confusing buttons; all actions are clear and contextual
- [ ] Follow-up questions work naturally without page resets
- [ ] Conversation history persists across page interactions
- [ ] Conversation history can be exported for audit trail
- [ ] All functionality has test coverage for both upload types

## Consequences

### Positive

- Physicians can ask follow-up questions without re-entering context
- String numeric columns are handled correctly in all analysis types
- Filter conditions enable precise cohort selection for research questions
- Conversational UI matches physician mental model (iterative refinement)
- Lightweight conversation storage prevents session state bloat
- Export capability supports research reproducibility and audit requirements

### Negative

- Conversation history limited to current session (no cross-session persistence)
- Filter parsing uses pattern matching (may miss complex filter syntax)
- Follow-up suggestions are static (not context-aware or learned)

### Mitigations

- Document filter syntax patterns in user help
- Add telemetry to identify failed filter extractions for future improvement
- Consider persistent conversation storage (database) in future iteration if cross-session history becomes critical

## Notes

### Feature Parity Requirements

- **All features must work identically for single-file and multi-table uploads** - no exceptions
- Filter parsing should leverage semantic layer column aliases for robust matching (works for both upload types)
- Comparison analysis must handle string numeric columns identically regardless of upload type
- Conversational UI must work identically for both upload types
- All analysis functions must use lazy Polars evaluation (not eager pandas) for both upload types

### Implementation Guidance

- Reference ADR002 Phase 0 for detailed feature parity requirements
- Reference multi-table handler refactor plan for advanced patterns (aggregate-before-join, table classification) - apply to single-file where applicable
- UI redesign maintains backward compatibility with existing analysis results
- Conversation history should be limited (e.g., last 20 queries) to avoid memory issues in long sessions
- Statistical warnings (small sample size, etc.) should be added to results in future iteration
- Consider adding filter validation against semantic layer schema to catch extraction errors early

## References

### Related ADRs
- **[ADR007: Feature Parity Architecture](../ADR/ADR007.md)** - **AUTHORITATIVE SOURCE** for feature parity. Establishes principle "Single-Table = Multi-Table with 1 Table" and defines unified code paths, persistence, and query capabilities. Phase 0 of this ADR implements ADR007.
- **[ADR002: Persistent Storage Layer](../ADR/ADR002.md)** - **NORTH STAR**: Defines persistent DuckDB storage, lazy Polars evaluation, Parquet export. All features in this ADR must align with ADR002's architecture. ADR002 Phase 0 references feature parity, which is fully defined in ADR007.
- **[ADR003: Clinical Trust Architecture & Adaptive Terminology](../ADR/ADR003.md)** - Trust/verification UI (Phase 1) should be integrated into conversational UI results. Enhanced LLM parsing (Phase 2) can improve filter extraction. Adaptive dictionary (Phase 3) extends existing alias system used in variable matching.

### Related Plans
- **[Multi-Table Handler Refactor Plan](../../.cursor/plans/multi-table_handler_refactor_aggregate-before-join_architecture_b7ca2b5e.plan.md)**: Defines advanced multi-table features (table classification, aggregate-before-join, partitioning). Single-file uploads should leverage these patterns where applicable.
- **[Consolidate Docs Plan](../../docs/implementation/plans/consolidate-docs-and-implement-question-driven-analysis.md)**: Shows multi-table infrastructure (deferred to V2). Single-file uploads must match this infrastructure.

### Code References
- `src/clinical_analytics/ui/storage/user_datasets.py` - Upload storage (must handle both types uniformly)
- `src/clinical_analytics/datasets/uploaded/definition.py` - UploadedDataset class (must support both types identically)
- `src/clinical_analytics/analysis/compute.py` - Analysis functions (must work identically for both upload types)
- `src/clinical_analytics/core/nl_query_engine.py` - NL query parsing (must work identically for both upload types)

### Testing Requirements

- All tests must verify functionality works for both single-file and multi-table uploads
- Test feature parity explicitly: same query on both upload types produces identical results
- Test lazy evaluation works correctly for both upload types
- Test semantic layer registration works identically for both upload types