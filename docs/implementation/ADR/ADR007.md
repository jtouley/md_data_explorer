# ADR 007: Feature Parity Architecture (Single-Table = Multi-Table)

## Status
**ACCEPTED** - 2025-01-XX
**IN PROGRESS** - Implementation aligned with ADR 002 Phase 0

## Context

The Clinical Analytics Platform supports two upload types:
1. **Single-table uploads**: CSV, Excel, SPSS files (one table per file)
2. **Multi-table uploads**: ZIP files containing multiple CSV tables (e.g., MIMIC-IV)

### Current Problem: Architectural Divergence

The codebase has evolved with **separate code paths** for single-table and multi-table uploads, leading to:

1. **Feature Gaps**: Multi-table uploads have capabilities unavailable to single-table uploads
   - Multi-table: Individual tables registered in DuckDB semantic layer
   - Single-table: Only unified cohort CSV, no individual table persistence
   - Multi-table: All tables accessible via semantic layer queries
   - Single-table: Limited semantic layer access

2. **Code Duplication**: Similar logic exists in two places
   - `save_upload()` vs `save_zip_upload()`
   - Different persistence mechanisms
   - Different semantic layer registration patterns

3. **Maintenance Burden**: New features must be implemented twice
   - Bug fixes must be applied to both code paths
   - Tests must cover both upload types separately
   - Documentation must explain differences

4. **User Confusion**: Inconsistent behavior between upload types
   - Same query works differently depending on upload type
   - Different error messages for same underlying issue
   - Unpredictable feature availability

### Why This Matters

**Clinical Research Requirements:**
- **Reproducibility**: Same analysis should produce identical results regardless of upload type
- **Trust**: Physicians need consistent behavior to build confidence in the platform
- **Efficiency**: Users shouldn't need to understand internal architecture to use the platform

**Technical Requirements:**
- **Maintainability**: Single code path is easier to test, debug, and extend
- **Performance**: Unified code paths enable shared optimizations
- **Scalability**: New features automatically work for both upload types

### Related Decisions

- **ADR 002**: Phase 0 (Feature Parity) = This ADR's implementation. ADR 002 Phase 1+ (DuckDB persistence, Parquet export) **requires** ADR 007 completion as prerequisite.
- **ADR 001**: Requires feature parity (ADR 007) as prerequisite for new features
- **Multi-Table Handler Refactor Plan**: Defines advanced patterns (aggregate-before-join, table classification) that should apply to single-table where applicable

### Prerequisite Relationship

**ADR 007 must be completed BEFORE ADR 002 Phase 1+ can proceed.**

- ADR 002 Phase 0 (Feature Parity) = ADR 007 implementation
- ADR 002 Phase 1+ (DuckDB persistence, Parquet export) requires unified architecture from ADR 007
- Without ADR 007, ADR 002 Phase 1+ would need to implement features twice (once for each upload type)
- ADR 007 establishes the foundation that all subsequent ADR 002 phases build upon

## Decision

### Core Principle: "Single-Table = Multi-Table with 1 Table"

**Single-table uploads are a special case of multi-table uploads, not a different architecture.**

This means:
- Single-table uploads should use the **same code paths** as multi-table uploads
- Single-table uploads should have the **same persistence** as multi-table uploads
- Single-table uploads should have the **same query capabilities** as multi-table uploads
- Single-table uploads should have the **same analysis features** as multi-table uploads

### Architectural Requirements

#### 1. Unified Code Paths (No Conditional Logic)

**Prohibited Pattern:**
```python
# WRONG: Conditional logic based on upload type
if upload_type == "single":
    save_single_table(data)
    register_single_table_in_semantic_layer()
else:
    save_multi_table(tables)
    register_all_tables_in_semantic_layer()
```

**Required Pattern:**
```python
# CORRECT: Unified code path
tables = normalize_to_table_list(upload_data)  # Single-table → [table], Multi-table → [table1, table2, ...]
for table in tables:
    save_table(table)
    register_table_in_semantic_layer(table)
```

**Rationale**: Single code path eliminates duplication, ensures consistency, and makes new features automatically available to both upload types.

#### 2. First-Class Citizen Requirement (No Feature Gaps)

**Definition**: Both upload types must have **identical capabilities** with no exceptions.

**Required Parity:**
- ✅ **Persistence**: Both save individual tables to disk (single-table = 1 table in `{upload_id}_tables/`)
- ✅ **Semantic Layer**: Both register all tables in DuckDB identically
- ✅ **Query Capabilities**: Both support same granularity levels (patient_level, admission_level, event_level)
- ✅ **Lazy Evaluation**: Both use Polars lazy frames (not eager pandas)
- ✅ **Metadata Schema**: Both use `inferred_schema` format (not `variable_mapping` for single-table)
- ✅ **Validation Pipeline**: Both go through same `DataQualityValidator.validate_complete()` pipeline
- ✅ **Analysis Features**: Comparison analysis, filtering, descriptive stats work identically
- ✅ **UI Features**: Conversational UI, follow-up questions, conversation history work identically

**Prohibited Patterns:**
- ❌ "This feature only works for multi-table uploads"
- ❌ "Single-table uploads have limited query capabilities"
- ❌ "Multi-table uploads support X, but single-table doesn't"

**Rationale**: Users shouldn't need to understand internal architecture. Upload type is an implementation detail, not a feature differentiator.

#### 3. Unified Persistence Mechanism

**Single-Table Upload:**
```
CSV/Excel/SPSS → normalize_to_table_list()
  → Table saved to data/uploads/raw/{upload_id}_tables/table_0.csv
  → Unified cohort CSV saved to data/uploads/raw/{upload_id}.csv (for backward compatibility)
  → Metadata JSON with table list (single table)
  → All tables registered in DuckDB semantic layer
```

**Multi-Table Upload:**
```
ZIP → normalize_to_table_list()
  → Tables saved to data/uploads/raw/{upload_id}_tables/table_0.csv, table_1.csv, ...
  → Unified cohort CSV saved to data/uploads/raw/{upload_id}.csv
  → Metadata JSON with table list (multiple tables)
  → All tables registered in DuckDB semantic layer
```

**Key Point**: Both use **identical storage structure**. Single-table is just multi-table with `len(tables) == 1`.

**Rationale**: Unified persistence enables shared query optimization, lazy evaluation, and semantic layer access patterns.

#### 4. Unified Query and Analysis Capabilities

**Semantic Layer Registration:**
- Both upload types register **all tables** in DuckDB semantic layer
- Both support same query patterns (column aliases, aggregations, joins)
- Both support same granularity levels (patient_level, admission_level, event_level)
- No hardcoded restrictions based on upload type

**Analysis Functions:**
- Both use same analysis functions (`compute_descriptive_analysis()`, `compute_comparison_analysis()`)
- Both use same filter application logic (`_apply_filters()`)
- Both use same lazy Polars evaluation pattern
- Both produce identical result structures

**Rationale**: Unified query capabilities ensure consistent behavior and enable shared optimizations.

#### 5. Advanced Pattern Compatibility

**Single-table uploads must be compatible with advanced multi-table patterns:**

- **Table Classification**: Single-table = 1 dimension table (trivial classification, uses same classification logic)
- **Aggregate-Before-Join**: Single-table = no joins needed (trivial case, uses same code path)
- **Query Planning**: Single-table queries use same QueryPlan structure (just 1 table in plan)
- **Partitioning**: Single-table can use same Parquet partitioning strategy (hash buckets, time-based)
- **Granularity Support**: Single-table supports same granularity levels (patient_level, admission_level, event_level)

**Rationale**: When multi-table gets advanced features (from multi-table refactor plan), single-table automatically benefits because it uses the same code paths. No separate implementation needed.

#### 6. Future Feature Requirement

**All future features must be designed for unified architecture:**

- New analysis types: Must work for both upload types identically
- New query capabilities: Must use unified code paths
- New persistence features: Must apply to both upload types
- New UI features: Must not differentiate by upload type

**Enforcement**: Code review checklist must verify no upload-type conditionals in new code.

### Implementation Strategy

#### Phase 1: Normalize Upload Handling

**Goal**: Create unified entry point that normalizes both upload types to same data structure.

**Tasks:**
```python
# New function: normalize_upload_to_table_list()
# NOTE: This is the ONLY function that detects upload type. Everything downstream is unified.
def normalize_upload_to_table_list(
    file_bytes: bytes,
    filename: str,
) -> tuple[list[dict], dict]:
    """
    Normalize any upload to unified table list.

    This is the ONLY function that needs to detect upload type.
    Everything downstream uses unified table list format.

    Returns:
        (tables, metadata) where tables is list of {"name": str, "data": pl.DataFrame}
    """
    # Detect upload type from file extension/content (one-time detection)
    if filename.endswith('.zip'):
        # Multi-table: extract from ZIP
        tables = extract_zip_tables(file_bytes)
    else:
        # Single-file: wrap in list (becomes multi-table with 1 table)
        df = load_single_file(file_bytes, filename)
        tables = [{"name": "table_0", "data": df}]

    return tables, {"table_count": len(tables)}
```

**Key Point**: Normalization is the **boundary layer** where upload-type detection happens once. After normalization, all code is unified and upload type is irrelevant.

**Integration:**
- Update `save_upload()` to use `normalize_upload_to_table_list()` then unified save logic
- Update `save_zip_upload()` to use `normalize_upload_to_table_list()` then unified save logic
- Both call same `save_table_list()` function

#### Phase 2: Unify Persistence

**Goal**: Both upload types use identical persistence mechanism.

**Tasks:**
- Modify `save_upload()` to save individual table to `{upload_id}_tables/` directory (like multi-table)
- Ensure both upload types save unified cohort CSV (for backward compatibility)
- Ensure both upload types use same metadata schema (`inferred_schema` format)
- Remove `variable_mapping` format for single-table uploads (convert during save)

**Success Criteria:**
- Single-table uploads save to `{upload_id}_tables/table_0.csv`
- Both upload types have identical directory structure
- Both upload types use same metadata JSON format

#### Phase 3: Unify Semantic Layer Registration

**Goal**: Both upload types register tables in DuckDB identically.

**Tasks:**
- Modify `_maybe_init_semantic()` in `uploaded/definition.py` to register all tables (not just multi-table)
- Ensure single-table uploads register their unified cohort table in DuckDB
- Remove hardcoded `patient_level` restriction for single-table uploads
- Ensure both upload types support all granularity levels

**Success Criteria:**
- Single-table uploads register table in DuckDB semantic layer
- Both upload types have identical query capabilities
- Both upload types support same granularity levels

#### Phase 4: Unify Data Access

**Goal**: Both upload types use same data loading and query patterns.

**Tasks:**
- Update `get_upload_data()` to return Polars lazy frame (not pandas DataFrame)
- Ensure both upload types use lazy Polars evaluation
- Ensure predicate pushdown works for both upload types
- Remove eager pandas loading paths

**Success Criteria:**
- Both upload types return Polars lazy frames
- Both upload types support lazy evaluation
- Both upload types have identical query performance characteristics

#### Phase 5: Remove Conditional Logic

**Goal**: Eliminate all `if upload_type == "single"` conditionals.

**Tasks:**
- Audit codebase for upload-type conditionals
- Refactor to use unified code paths
- Update tests to verify unified behavior
- Remove duplicate code

**Success Criteria:**
- No conditional logic based on upload type
- Single code path handles both upload types
- Tests verify identical behavior for both upload types

## Consequences

### Positive

**For Users:**
- **Consistent Experience**: Same features work identically regardless of upload type
- **Predictable Behavior**: No surprises when switching between upload types
- **Simplified Mental Model**: Upload type is just "how many tables", not "different architecture"

**For Developers:**
- **Reduced Maintenance**: Single code path is easier to test, debug, and extend
- **Automatic Feature Parity**: New features automatically work for both upload types
- **Shared Optimizations**: Performance improvements benefit both upload types
- **Simplified Testing**: Tests verify unified behavior, not separate code paths

**For Architecture:**
- **Scalability**: Easy to add new upload types (e.g., Parquet directory, database connection)
- **Extensibility**: New features automatically available to all upload types
- **Consistency**: Same patterns throughout codebase

### Negative

**Migration Effort:**
- Requires refactoring existing single-table upload code
- May break backward compatibility with existing uploads (requires migration script)
- Requires comprehensive testing to ensure no regressions

**Initial Complexity:**
- Normalization layer adds abstraction (but reduces overall complexity)
- May require more careful design to handle edge cases

**Mitigation:**
- Phased implementation reduces risk
- Comprehensive testing ensures correctness
- Migration script handles backward compatibility

### Risks & Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| Breaking existing single-table uploads | High | Migration script converts existing uploads to new format |
| Performance regression | Medium | Benchmark both upload types before/after refactor |
| Missing edge cases | Medium | Comprehensive test coverage for both upload types |
| Incomplete migration | High | Explicit checklist of all code paths to unify |

## Alternatives Considered

### 1. Keep Separate Code Paths (Status Quo)
**Rejected**: Leads to feature gaps, code duplication, and maintenance burden. Violates DRY principle.

### 2. Make Multi-Table a Special Case of Single-Table
**Rejected**: Multi-table is more complex (relationships, joins, unified cohort). Single-table is simpler special case.

### 3. Abstract Upload Type Behind Interface
**Rejected**: Adds unnecessary abstraction. Normalization function is simpler and more direct.

### 4. Feature Flags for Gradual Migration
**Rejected**: Feature flags add complexity. Phased implementation is better approach.

## Success Metrics

### Must-Have (Go/No-Go)

- [ ] Single-table and multi-table uploads have identical persistence structure
- [ ] Both upload types register all tables in DuckDB semantic layer
- [ ] Both upload types support lazy Polars evaluation
- [ ] Both upload types use same metadata schema (`inferred_schema` format)
- [ ] Both upload types go through same validation pipeline
- [ ] Both upload types support all granularity levels
- [ ] No conditional logic based on upload type exists in codebase
- [ ] Same query produces identical results for both upload types
- [ ] All analysis features work identically for both upload types

### Nice-to-Have

- [ ] Migration script converts existing single-table uploads to new format
- [ ] Performance benchmarks show no regression
- [ ] Code coverage for unified code paths >90%
- [ ] Documentation explains unified architecture (not separate upload types)

## Implementation Checklist

### Phase 1: Normalize Upload Handling
- [ ] Create `normalize_upload_to_table_list()` function
- [ ] Update `save_upload()` to use normalization
- [ ] Update `save_zip_upload()` to use normalization
- [ ] Create unified `save_table_list()` function
- [ ] Test normalization with both upload types

### Phase 2: Unify Persistence
- [ ] Modify `save_upload()` to save individual table to `{upload_id}_tables/`
- [ ] Ensure both upload types use same directory structure
- [ ] Convert `variable_mapping` to `inferred_schema` format
- [ ] Update metadata schema for both upload types
- [ ] Test persistence for both upload types

### Phase 3: Unify Semantic Layer Registration
- [ ] Modify `_maybe_init_semantic()` to register all tables (not just multi-table)
- [ ] Remove hardcoded `patient_level` restriction
- [ ] Ensure both upload types support all granularity levels
- [ ] Test semantic layer registration for both upload types

### Phase 4: Unify Data Access
- [ ] Update `get_upload_data()` to return Polars lazy frame
- [ ] Remove eager pandas loading paths
- [ ] Ensure predicate pushdown works for both upload types
- [ ] Test lazy evaluation for both upload types

### Phase 5: Remove Conditional Logic
- [ ] Audit codebase for upload-type conditionals
- [ ] Refactor to unified code paths
- [ ] Update tests to verify unified behavior
- [ ] Remove duplicate code

## References

### Related ADRs
- **ADR 002**: Persistent Storage Layer - Phase 0 (Feature Parity) = This ADR's implementation. ADR 002 Phase 1+ requires ADR 007 completion as prerequisite.
- **ADR 001**: Comparison Analysis & Conversational UI - Requires feature parity (ADR 007) as prerequisite

### Alignment with ADR 002 Phase 0

| ADR 007 Phase | ADR 002 Phase 0 Task | Mapping |
|---------------|----------------------|---------|
| Phase 1: Normalize Upload Handling | "Unify storage format" | Maps to normalization requirement |
| Phase 2: Unify Persistence | "Both save individual tables" | Maps to persistence unification |
| Phase 3: Unify Semantic Layer | "Register in DuckDB semantic layer" | Maps to semantic layer registration |
| Phase 4: Unify Data Access | "Replace eager pandas with lazy Polars" | Maps to lazy evaluation |
| Phase 5: Remove Conditional Logic | "Both use same persistence mechanism" | Maps to unified code paths |

**Note**: ADR 007 provides detailed implementation guidance for ADR 002 Phase 0. ADR 002 Phase 0 tasks are high-level; ADR 007 phases provide the concrete steps.

### Code References
- `src/clinical_analytics/ui/storage/user_datasets.py` - `save_upload()` and `save_zip_upload()` methods
- `src/clinical_analytics/datasets/uploaded/definition.py` - `_maybe_init_semantic()` method
- `src/clinical_analytics/core/multi_table_handler.py` - Multi-table processing logic

### Documentation
- `docs/specs/multi-table-support.md` - Multi-table specification
- `.cursor/plans/multi-table_handler_refactor_aggregate-before-join_architecture_b7ca2b5e.plan.md` - Advanced patterns

## Decision Makers
- **Jason** (Technical Lead): Architectural principle approval
- **Clinical Stakeholder** (Infectious Disease Physician): User experience validation

## Review Date
**2025-02-15** - After 1 month of unified architecture, evaluate:
1. Are there any remaining feature gaps between upload types?
2. Has unified code path reduced maintenance burden?
3. Are there performance issues with normalization layer?
4. Should we extend this principle to other areas (e.g., analysis types)?

---

## Appendix A: Code Examples

### Before: Separate Code Paths

```python
# WRONG: Conditional logic
def save_upload(self, file_bytes, filename, metadata):
    if metadata.get("upload_type") == "single":
        df = pd.read_csv(io.BytesIO(file_bytes))
        df.to_csv(f"data/uploads/raw/{upload_id}.csv")
        # No semantic layer registration
    else:
        tables = extract_zip_tables(file_bytes)
        for table in tables:
            table.to_csv(f"data/uploads/raw/{upload_id}_tables/{table.name}.csv")
        # Register all tables in semantic layer
```

### After: Unified Code Path

```python
# CORRECT: Unified code path
def save_upload(self, file_bytes, filename, metadata):
    # Normalize to table list (single-table → [table], multi-table → [table1, table2, ...])
    tables, table_metadata = normalize_upload_to_table_list(file_bytes, filename, metadata)

    # Unified save logic
    for table in tables:
        save_table_to_disk(table, upload_id)
        register_table_in_semantic_layer(table, upload_id)

    # Save unified cohort (for backward compatibility)
    unified_cohort = create_unified_cohort(tables)
    save_unified_cohort(unified_cohort, upload_id)
```

## Appendix B: Migration Strategy

### Existing Single-Table Uploads

**Problem**: Existing single-table uploads use old format (no `{upload_id}_tables/` directory).

**Solution**: Migration script on app startup:

```python
def migrate_existing_uploads():
    """Migrate existing single-table uploads to unified format."""
    for upload_id in list_existing_uploads():
        metadata = load_metadata(upload_id)

        # Check if already migrated
        if metadata.get("format_version") == "unified":
            continue

        # Check if single-table upload
        if metadata.get("table_count", 1) == 1:
            # Load existing CSV
            df = pl.read_csv(f"data/uploads/raw/{upload_id}.csv")

            # Create tables directory
            tables_dir = f"data/uploads/raw/{upload_id}_tables/"
            os.makedirs(tables_dir, exist_ok=True)

            # Save as table_0.csv
            df.write_csv(f"{tables_dir}/table_0.csv")

            # Update metadata
            metadata["format_version"] = "unified"
            metadata["tables"] = [{"name": "table_0", "path": f"{tables_dir}/table_0.csv"}]
            save_metadata(upload_id, metadata)
```

## Appendix C: Testing Strategy

### Feature Parity Tests

```python
def test_feature_parity_single_vs_multi_table():
    """Verify single-table and multi-table uploads have identical capabilities."""

    # Create single-table upload
    single_upload = create_single_table_upload(test_data)

    # Create multi-table upload (with 1 table)
    multi_upload = create_multi_table_upload([test_data])

    # Verify identical persistence
    assert single_upload.persistence_structure == multi_upload.persistence_structure

    # Verify identical semantic layer registration
    assert single_upload.semantic_layer_tables == multi_upload.semantic_layer_tables

    # Verify identical query capabilities
    single_result = query_semantic_layer(single_upload, "SELECT * FROM table_0")
    multi_result = query_semantic_layer(multi_upload, "SELECT * FROM table_0")
    assert single_result == multi_result

    # Verify identical analysis results
    single_analysis = compute_descriptive_analysis(single_upload.data)
    multi_analysis = compute_descriptive_analysis(multi_upload.data)
    assert single_analysis == multi_analysis
```
