# ADR 004: Automated Schema Inference via Documentation Ingestion (RAG)

## Status

**Proposed**

## Related ADRs

- **[ADR002: Persistent Storage Layer](../ADR/ADR002.md)**: ADR002 owns metadata JSON schema and persistence. ADR004's extracted documentation context will be stored in ADR002's metadata schema (per `upload_id` + `dataset_version` scope).
- **[ADR003: Clinical Trust Architecture & Adaptive Terminology](../ADR/ADR003.md)**: ADR003 owns semantic layer execution and alias persistence. ADR004's documentation context will enhance schema inference, which feeds into ADR003's semantic layer registration.

## Context

The "Zero-Friction" onboarding goal (ADR 003) is currently hindered by the limitations of pure data-based schema inference. When a user uploads a complex dataset (e.g., MIMIC-IV, custom hospital exports), the `SchemaInferenceEngine` attempts to map columns to semantic concepts (e.g., mapping `ITEMID: 220045` to `Heart Rate`) based solely on column names and data samples. This heuristic approach is error-prone and often requires manual user correction, violating the "Zero-Config" principle.

However, many of these datasets are uploaded as ZIP files that explicitly contain data dictionaries or definition files (PDFs, Markdown, text files) in a `docs/` subdirectory or root directory. These documents contain the "ground truth" needed to accurately map the schema, but they are currently ignored by the platform.

**Real-World Evidence**:

- MIMIC-IV datasets include comprehensive data dictionaries (PDF) defining all `ITEMID` codes and their clinical meanings
- Hospital export ZIPs often contain `README.md` or `data_dictionary.txt` files explaining column encodings
- Research datasets from PhysioNet include detailed documentation PDFs in `docs/` subdirectories
- Current system treats these files as "noise" and skips them during ZIP extraction

**Current Limitations**:

1. **Ambiguous Column Names**: `val1`, `val2`, `ITEMID: 220045` cannot be mapped without context
2. **Encoding Confusion**: Categorical variables encoded as `1: Yes 2: No` require documentation to understand meaning
3. **Missing Semantic Context**: System cannot distinguish between similar columns (e.g., `heart_rate_1` vs `heart_rate_2`) without descriptions
4. **Manual Correction Required**: Users must manually fix schema mappings after upload, breaking "Zero-Config" promise

## Decision

We will implement **Retrieval-Augmented Generation (RAG) for Schema Inference** by treating uploaded documentation as a first-class input signal.

### 1. Documentation Ingestion Pipeline

We will modify the `UserDatasetStorage` and `normalize_upload_to_table_list()` to actively scan for and ingest documentation files during the upload/extraction process.

**Detection**: When processing a ZIP upload, the system will identify files in `docs/` or root directories with relevant extensions:
- `.pdf` - Data dictionary PDFs (most common)
- `.md`, `.markdown` - Markdown documentation
- `.txt` - Plain text data dictionaries
- `.docx` - Word documents (optional, lower priority)

**Extraction**: A new `DocParser` service will extract text content from these files:
- **PDF**: Use `pymupdf` (fitz) for fast, license-compatible extraction
- **Markdown/Text**: Direct text extraction
- **Word**: Use `python-docx` if needed (optional)

**Storage**: Extracted text will be stored alongside the dataset metadata:
- Location: `metadata/data_dictionary_context.txt` (or `metadata/doc_context_{filename}.txt` for multiple files)
- Format: Plain text concatenation of all extracted documentation
- Truncation: If total text exceeds 50,000 characters, truncate with warning (prevents LLM context overflow)

### 2. Context-Aware Schema Inference

We will refactor `SchemaInferenceEngine` to accept this extracted documentation context.

**Enhanced Method**: `infer_schema_with_context(df: pl.DataFrame, doc_context: str | None = None) -> InferredSchema`

**Prompt Engineering** (for LLM-based inference, if implemented in future):
- **Before**: "Here are columns `A`, `B`, `C` and some sample rows. Guess what they are."
- **After**: "Here are columns `A`, `B`, `C`. Here is the user's uploaded data dictionary which defines `A` as 'Heart Rate (beats per minute)'. Map these columns to the standard schema."

**Current Implementation** (non-LLM):
- Use `DictionaryMetadata` class (already exists in `schema_inference.py`) to parse structured information from documentation
- Enhance `parse_dictionary_pdf()` to extract column descriptions, types, and valid values
- Merge dictionary metadata with data-driven inference (already implemented in `infer_schema_with_dictionary()`)
- Use dictionary hints to improve confidence scores and resolve ambiguous columns

### 3. Architecture Changes

**New Component**: `src/clinical_analytics/core/doc_parser.py`

```python
def extract_context_from_docs(file_paths: list[Path]) -> str:
    """
    Iterates through provided paths, detects file types (PDF/TXT/MD),
    extracts text, and concatenates into a context string.
    Truncates to safe token limits if necessary.
    
    Args:
        file_paths: List of paths to documentation files
        
    Returns:
        Concatenated text context (truncated to 50,000 chars if needed)
    """
    # Implementation details:
    # - PDF: Use pymupdf (fitz) for extraction
    # - Markdown/Text: Direct read
    # - Concatenate with separators
    # - Truncate with warning if >50k chars
```

**Modified Component**: `src/clinical_analytics/ui/storage/user_datasets.py`

- Update `extract_zip_tables()` to separate data files (CSV/Parquet) from documentation files
- Add `extract_documentation_files(zip_file: zipfile.ZipFile) -> list[Path]` helper
- Call `doc_parser.extract_context_from_docs()` on identified documentation files immediately after extraction
- Store extracted context in metadata: `metadata["doc_context"] = extracted_text`

**Modified Component**: `src/clinical_analytics/core/schema_inference.py`

- Update `infer_schema()` signature to accept `doc_context: str | None = None`
- If `doc_context` provided, parse it using existing `parse_dictionary_pdf()` logic (or new text parsing)
- Merge dictionary metadata with data-driven inference (enhance existing `infer_schema_with_dictionary()`)
- Use dictionary descriptions to improve column mapping confidence

**Integration Point**: `UserDatasetStorage.save_zip_upload()`

- After extracting tables, extract documentation files
- Call `doc_parser.extract_context_from_docs()` on documentation files
- Store `doc_context` in metadata JSON
- Pass `doc_context` to `SchemaInferenceEngine.infer_schema_with_context()` when inferring schema

## Implementation

### Phase 1: Documentation Detection and Extraction (P0)

**Goal**: Identify and extract documentation files from ZIP uploads.

**Files to Modify**:
- `src/clinical_analytics/ui/storage/user_datasets.py`
  - Add `extract_documentation_files()` helper
  - Update `extract_zip_tables()` to return both tables and doc files
  - Store doc files in `{upload_id}/docs/` directory

**New File**: `src/clinical_analytics/core/doc_parser.py`

```python
"""
Documentation Parser for Schema Inference

Extracts text content from various documentation formats (PDF, Markdown, text)
to provide context for schema inference.
"""

from pathlib import Path
from typing import Any

def extract_context_from_docs(file_paths: list[Path]) -> str:
    """
    Extract text content from documentation files.
    
    Args:
        file_paths: List of paths to documentation files
        
    Returns:
        Concatenated text context (truncated to 50,000 chars if needed)
    """
    # Implementation:
    # 1. Iterate through file_paths
    # 2. Detect file type (PDF, MD, TXT)
    # 3. Extract text using appropriate library
    # 4. Concatenate with separators
    # 5. Truncate if >50k chars (with warning log)
    pass

def extract_pdf_text(pdf_path: Path) -> str:
    """Extract text from PDF using pymupdf."""
    # Use pymupdf (fitz) for extraction
    pass

def extract_markdown_text(md_path: Path) -> str:
    """Extract text from Markdown file."""
    # Direct read (preserve structure)
    pass
```

**Dependencies**: Add `pymupdf` to `pyproject.toml`:
```toml
[project]
dependencies = [
    # ... existing dependencies ...
    "pymupdf>=1.23.0",  # PDF extraction (fitz)
]
```

**Success Criteria**:
- [ ] Documentation files detected in ZIP uploads (`docs/` or root)
- [ ] PDF, Markdown, and text files extracted successfully
- [ ] Extracted text stored in metadata JSON
- [ ] Files stored in `{upload_id}/docs/` for audit trail

### Phase 2: Schema Inference Integration (P0)

**Goal**: Pass documentation context to schema inference engine.

**Files to Modify**:
- `src/clinical_analytics/core/schema_inference.py`
  - Update `infer_schema()` to accept `doc_context: str | None = None`
  - Enhance `parse_dictionary_pdf()` to also parse plain text (not just PDF)
  - Use `doc_context` to populate `DictionaryMetadata` when available
  - Merge dictionary metadata with data-driven inference

**Files to Modify**:
- `src/clinical_analytics/ui/storage/user_datasets.py`
  - In `save_zip_upload()`, extract documentation and pass to schema inference
  - Store `doc_context` in metadata JSON for future reference

**Success Criteria**:
- [ ] `SchemaInferenceEngine.infer_schema()` accepts `doc_context` parameter
- [ ] Documentation context improves column mapping confidence
- [ ] Dictionary metadata merged with data-driven inference
- [ ] Schema inference uses dictionary descriptions when available

### Phase 3: Testing (P1)

**Goal**: Verify documentation ingestion improves schema inference accuracy.

**New Test File**: `tests/core/test_doc_parser.py`

```python
def test_extract_pdf_text():
    """Test PDF text extraction."""
    # Create mock PDF with column definitions
    # Verify text extraction works

def test_extract_markdown_text():
    """Test Markdown text extraction."""
    # Create mock Markdown file
    # Verify text extraction preserves structure

def test_extract_context_truncation():
    """Test that large documentation is truncated appropriately."""
    # Create large text file (>50k chars)
    # Verify truncation with warning

def test_schema_inference_with_doc_context():
    """Test that documentation context improves schema inference."""
    # Create test DataFrame with ambiguous columns
    # Create documentation file defining columns
    # Verify schema inference uses documentation
```

**New Test File**: `tests/loader/test_doc_extraction.py`

```python
def test_zip_extraction_includes_docs():
    """Test that ZIP extraction identifies documentation files."""
    # Create ZIP with CSV + PDF in docs/
    # Verify docs are extracted and stored

def test_schema_inference_with_uploaded_docs():
    """End-to-end test: upload ZIP with docs, verify schema inference."""
    # Create ZIP with CSV + data dictionary PDF
    # Upload via UserDatasetStorage
    # Verify schema inference uses documentation
```

**Success Criteria**:
- [ ] Unit tests for doc parser (PDF, Markdown, text)
- [ ] Integration tests for ZIP extraction with docs
- [ ] End-to-end test: upload ZIP with docs, verify improved schema inference
- [ ] Test coverage >80% for new code

## Consequences

### Positive

- **High-Accuracy Mapping**: Resolves ambiguous columns (e.g., `val1` vs `val2`) by reading the actual definition
- **True Zero-Config**: MIMIC-style datasets can be mapped automatically without user intervention
- **Utilizes Existing User Behavior**: Users already have these files in their ZIP exports; we just need to read them
- **Improved Confidence Scores**: Dictionary metadata provides ground truth, increasing inference confidence
- **Better Column Descriptions**: Schema includes rich descriptions from documentation, improving downstream analysis

### Negative

- **New Dependencies**: Requires adding PDF processing libraries (`pymupdf`) to `pyproject.toml`
- **Token Usage**: Documentation can be verbose. We must implement truncation or summarization strategies to prevent overflowing the LLM context window (though modern windows of 128k+ tokens make this less risky for future LLM integration)
- **Processing Time**: Parsing PDFs adds a few seconds to the initial upload/processing step
- **Storage Overhead**: Extracted documentation text stored in metadata JSON increases file size
- **Parsing Complexity**: PDF structure varies widely; heuristics may miss some column definitions

### Mitigations

- **Truncation Strategy**: Limit extracted text to 50,000 characters with clear warning logs
- **Lazy Parsing**: Only parse documentation when schema inference confidence is low (future optimization)
- **Caching**: Store parsed dictionary metadata in metadata JSON to avoid re-parsing on schema re-inference
- **Graceful Degradation**: If documentation parsing fails, fall back to data-driven inference (no breaking changes)

## Alternatives Considered

### OCR for Scanned PDFs

**Rejected**: Most data dictionaries (MIMIC, hospital exports) are digital-native PDFs. OCR adds massive complexity (Tesseract) and slowness for edge cases we don't strictly need yet. Can be added in future if scanned PDFs become common.

### Separate "Upload Dictionary" Flow

**Rejected**: Increases friction. The goal is to handle the ZIP file the user *already has*, not ask them to separate files manually. Manual upload flow would violate "Zero-Config" principle.

### LLM-Based Extraction (Immediate)

**Deferred**: Current implementation uses pattern matching and heuristics (existing `parse_dictionary_pdf()`). LLM-based extraction can be added in future if heuristics prove insufficient. ADR003 Phase 2 (Enhanced LLM Parsing) may provide infrastructure for this.

### Structured Dictionary Format (JSON/YAML)

**Rejected**: Users don't have structured dictionaries - they have PDFs and Markdown. We must work with what users actually provide, not impose new format requirements.

## Code References

- `src/clinical_analytics/core/schema_inference.py` - Schema inference engine (target for doc context integration)
- `src/clinical_analytics/ui/storage/user_datasets.py` - ZIP upload processing (target for doc extraction)
- `src/clinical_analytics/core/schema_inference.py:DictionaryMetadata` - Existing dictionary metadata class (can be extended)
- `src/clinical_analytics/core/schema_inference.py:parse_dictionary_pdf()` - Existing PDF parsing (can be enhanced for text/Markdown)

## References

### Related ADRs

- **[ADR002: Persistent Storage Layer](../ADR/ADR002.md)**: Defines metadata JSON schema where `doc_context` will be stored
- **[ADR003: Clinical Trust Architecture & Adaptive Terminology](../ADR/ADR003.md)**: Schema inference feeds into semantic layer registration (ADR003's responsibility)

### Related Plans

- **Zero-Friction Onboarding** (ADR003 context): Documentation ingestion directly supports "Zero-Config" goal
- **Multi-Table Handler Refactor**: Documentation extraction should work for both single-file and multi-table uploads (feature parity)

