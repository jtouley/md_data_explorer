# ADR 004: Automated Schema Inference via Documentation Ingestion (RAG)

## Status

**Implemented** (All phases complete)

**Implementation Date**: 2026-01-04

**Completed Phases**:
- ✅ Phase 1: Documentation Detection and Extraction
- ✅ Phase 2: Schema Inference Integration
- ✅ Phase 3: Tier 3 AutoContext Packager
- ✅ Phase 4: Proactive Question Generation
- ✅ Phase 5: Comprehensive Testing

**Implementation Notes**:
- Documentation extraction from ZIP uploads implemented in `doc_parser.py`
- Schema inference enhanced with `doc_context` parameter in `SchemaInferenceEngine.infer_schema()`
- Question generation implemented with deterministic fallback and LLM support
- Integration tests verify end-to-end flow
- Performance tests confirm doc extraction <5s budget

## Related ADRs

- **[ADR002: Persistent Storage Layer](../ADR/ADR002.md)**: ADR002 owns metadata JSON schema and persistence. ADR004's extracted documentation context will be stored in ADR002's metadata schema (per `upload_id` + `dataset_version` scope).
- **[ADR003: Clinical Trust Architecture & Adaptive Terminology](../ADR/ADR003.md)**: ADR003 owns semantic layer execution and alias persistence. ADR004's documentation context will enhance schema inference, which feeds into ADR003's semantic layer registration. ADR004's AutoContext Packager provides bounded, privacy-safe context to ADR001's Tier 3 LLM fallback, enabling grounded query parsing.
- **[ADR001: Query Plan Producer, Filtering, and Chat-First Execution Rules](../ADR/ADR001.md)**: ADR001 owns Tier 3 LLM fallback implementation. ADR004 provides AutoContext Pack to Tier 3 so it can produce valid QueryPlans for negation, multi-part queries, and codebook-driven columns.

## Context

The "Zero-Friction" onboarding goal (ADR 003) is currently hindered by the limitations of pure data-based schema inference. When a user uploads a complex dataset (e.g., MIMIC-IV, custom hospital exports), the `SchemaInferenceEngine` attempts to map columns to semantic concepts (e.g., mapping `ITEMID: 220045` to `Heart Rate`) based solely on column names and data samples. This heuristic approach is error-prone and often requires manual user correction, violating the "Zero-Config" principle.

However, many of these datasets are uploaded as ZIP files that explicitly contain data dictionaries or definition files (PDFs, Markdown, text files) in a `docs/` subdirectory or root directory. These documents contain the "ground truth" needed to accurately map the schema, but they are currently ignored by the platform.

**Real-World Evidence**:

- MIMIC-IV datasets include comprehensive data dictionaries (PDF) defining all `ITEMID` codes and their clinical meanings
- Hospital export ZIPs often contain `README.md` or `data_dictionary.txt` files explaining column encodings
- Research datasets from PhysioNet include detailed documentation PDFs in `docs/` subdirectories
- Current system treats these files as "noise" and skips them during ZIP extraction

**Current Limitations**:

1. **Ambiguous Column Names**: `val1`, `val2`, `ITEMID: 220045` cannot be mapped without context
2. **Encoding Confusion**: Categorical variables encoded as `1: Yes 2: No` require documentation to understand meaning
3. **Missing Semantic Context**: System cannot distinguish between similar columns (e.g., `heart_rate_1` vs `heart_rate_2`) without descriptions
4. **Manual Correction Required**: Users must manually fix schema mappings after upload, breaking "Zero-Config" promise
5. **Tier 3 LLM Flying Blind**: ADR001's Tier 3 LLM fallback returns low-confidence plans with no variables (e.g., `confidence=0.3, matched_vars=[]` for "average BMI") because it lacks schema context. This produces "COUNT with no variables" and "group by patient_id" failures (see ADR003 Production Contract Violations).

## Decision

We will implement **Retrieval-Augmented Generation (RAG) for Schema Inference** by treating uploaded documentation as a first-class input signal.

Additionally, we will extend ADR004 to provide **Tier 3 AutoContext Packager** - a bounded, privacy-safe context pack for ADR001's Tier 3 LLM fallback. This addresses the "NLU without grounded context" problem where Tier 3 returns low-confidence plans with no variables because it lacks schema cues.

### 1. Documentation Ingestion Pipeline

We will modify the `UserDatasetStorage` and `normalize_upload_to_table_list()` to actively scan for and ingest documentation files during the upload/extraction process.

**Detection**: When processing a ZIP upload, the system will identify files in `docs/` or root directories with relevant extensions:
- `.pdf` - Data dictionary PDFs (most common)
- `.md`, `.markdown` - Markdown documentation
- `.txt` - Plain text data dictionaries
- `.docx` - Word documents (optional, lower priority)

**Extraction**: A new `DocParser` service will extract text content from these files:
- **PDF**: Use `pymupdf` (fitz) for fast, license-compatible extraction
- **Markdown/Text**: Direct text extraction
- **Word**: Use `python-docx` if needed (optional)

**Storage**: Extracted text will be stored alongside the dataset metadata:
- Location: `metadata/data_dictionary_context.txt` (or `metadata/doc_context_{filename}.txt` for multiple files)
- Format: Plain text concatenation of all extracted documentation
- Truncation: If total text exceeds 50,000 characters, truncate with warning (prevents LLM context overflow)

### 2. Context-Aware Schema Inference

We will refactor `SchemaInferenceEngine` to accept this extracted documentation context.

**Enhanced Method**: `infer_schema_with_context(df: pl.DataFrame, doc_context: str | None = None) -> InferredSchema`

**Prompt Engineering** (for LLM-based inference, if implemented in future):
- **Before**: "Here are columns `A`, `B`, `C` and some sample rows. Guess what they are."
- **After**: "Here are columns `A`, `B`, `C`. Here is the user's uploaded data dictionary which defines `A` as 'Heart Rate (beats per minute)'. Map these columns to the standard schema."

**Current Implementation** (non-LLM):
- Use `DictionaryMetadata` class (already exists in `schema_inference.py`) to parse structured information from documentation
- Enhance `parse_dictionary_pdf()` to extract column descriptions, types, and valid values
- Merge dictionary metadata with data-driven inference (already implemented in `infer_schema_with_dictionary()`)
- Use dictionary hints to improve confidence scores and resolve ambiguous columns

### 3. Architecture Changes

**New Component**: `src/clinical_analytics/core/doc_parser.py`

```python
def extract_context_from_docs(file_paths: list[Path]) -> str:
    """
    Iterates through provided paths, detects file types (PDF/TXT/MD),
    extracts text, and concatenates into a context string.
    Truncates to safe token limits if necessary.

    Args:
        file_paths: List of paths to documentation files

    Returns:
        Concatenated text context (truncated to 50,000 chars if needed)
    """
    # Implementation details:
    # - PDF: Use pymupdf (fitz) for extraction
    # - Markdown/Text: Direct read
    # - Concatenate with separators
    # - Truncate with warning if >50k chars
```

**Modified Component**: `src/clinical_analytics/ui/storage/user_datasets.py`

- Update `extract_zip_tables()` to separate data files (CSV/Parquet) from documentation files
- Add `extract_documentation_files(zip_file: zipfile.ZipFile) -> list[Path]` helper
- Call `doc_parser.extract_context_from_docs()` on identified documentation files immediately after extraction
- Store extracted context in metadata: `metadata["doc_context"] = extracted_text`

**Modified Component**: `src/clinical_analytics/core/schema_inference.py`

- Update `infer_schema()` signature to accept `doc_context: str | None = None`
- If `doc_context` provided, parse it using existing `parse_dictionary_pdf()` logic (or new text parsing)
- Merge dictionary metadata with data-driven inference (enhance existing `infer_schema_with_dictionary()`)
- Use dictionary descriptions to improve column mapping confidence

**Integration Point**: `UserDatasetStorage.save_zip_upload()`

- After extracting tables, extract documentation files
- Call `doc_parser.extract_context_from_docs()` on documentation files
- Store `doc_context` in metadata JSON
- Pass `doc_context` to `SchemaInferenceEngine.infer_schema_with_context()` when inferring schema

### 4. Tier 3 AutoContext Packager (P0)

**Goal**: Provide a bounded, privacy-safe, deterministic context pack to ADR001's Tier 3 LLM fallback, enabling grounded query parsing for negation, multi-part queries, and codebook-driven columns.

**Problem**: Production logs show Tier 3 returns `DESCRIBE, confidence=0.3, matched_vars=[]` for queries like "average BMI" / "average LDL" because the model has no schema cues. This produces "COUNT with no variables" and "group by patient_id" failures (see ADR003 Production Contract Violations). The root cause is "LLM without grounding" - Tier 3 is flying blind without authoritative schema context.

**Solution**: AutoContext Pack provides structured, privacy-safe, deterministic dataset context to Tier 3. This pack is **authoritative and closed** - it contains only what exists in the dataset and documentation, nothing more.

**Key Principle**: ADR004 owns **Context Assembly** (deterministic construction from existing data), not reasoning. The LLM's role is **context selection and mapping**, not generation or invention.

**AutoContext Pack Schema**:

```python
@dataclass
class AutoContext:
    """Bounded, privacy-safe context pack for Tier 3 LLM."""
    dataset: dict[str, str]  # {upload_id, dataset_version, display_name}
    entity_keys: list[str]  # Candidate columns ranked (e.g., ["patient_id", "encounter_id"])
    columns: list[ColumnContext]  # Column catalog with metadata
    glossary: dict[str, Any]  # Extracted terms, abbreviations, notes from documentation
    constraints: dict[str, Any]  # {no_row_level_data: true, max_tokens: N}

@dataclass
class ColumnContext:
    """Column metadata for AutoContext."""
    name: str  # Canonical column name
    normalized_name: str  # Normalized for matching
    system_aliases: list[str]  # System-generated aliases (from alias index)
    user_aliases: list[str]  # User-taught aliases (from ADR003 Phase 2)
    dtype: Literal["numeric", "categorical", "datetime", "id"]  # Inferred type
    units: str | None  # Detected units (mg/dL, %, etc.)
    codebook: dict[str, str] | None  # Categorical value dictionary (e.g., {"1": "Yes", "2": "No", "0": "n/a"})
    stats: dict[str, Any]  # Lightweight profiling:
        # numeric: {min, max, mean, missing_pct}
        # categorical: {top_values: [{value, count, pct}], missing_pct}
```

**Included (Safe + High Signal)**:
- Dataset identifiers: `upload_id`, `dataset_version`, `display_name`
- Primary key / entity key candidates: `patient_id`, etc. (ranked by confidence)
- Column catalog with:
  - Canonical column name
  - Normalized aliases (system + user from ADR003)
  - Inferred type: numeric / categorical / datetime / id-like
  - Units if detected (mg/dL, %, etc.)
  - Categorical value dictionary (top N + codebook if present: `1: Yes, 2: No`)
- Lightweight profiling:
  - Numeric: min/max/mean + % missing
  - Categorical: top N frequencies + % missing
- Extracted dictionary text from uploads: codebooks, definitions, abbreviations

**Explicitly Excluded (Privacy Guardrail)**:
- ❌ No patient-level rows or raw record samples
- ❌ No free-form "here are 100 random rows" garbage
- ❌ No PHI in context (only aggregated stats and dictionaries)

**Construction Rules** (Deterministic, No LLM):

1. **Build from multiple sources** (all deterministic):
   - Inferred schema + profiling (from `SchemaInferenceEngine`)
   - Extracted documentation dictionary (PDF/DOC/TXT from Phase 1)
   - Persisted alias mappings (from ADR003 Phase 2 - user aliases)
   - **No LLM inference or generation** - only what exists in dataset and documentation

2. **Token budget enforcement** (for LLM context window):
   - Keep only columns relevant to query terms (lexical + alias match)
   - Include top-K "similar" columns if needed (semantic similarity)
   - Cap categorical top values at N (e.g., top 10 frequencies)
   - Truncate glossary to most relevant terms

3. **Privacy rule**: Do not include row-level samples in AutoContext. Only include aggregated stats and dictionaries.

4. **Deterministic construction**: AutoContext Pack is produced without an LLM. Think: compiler metadata, not chat memory. Same inputs (schema, docs, aliases) always produce same AutoContext.

**Integration with ADR003 Trust UI**:

- Trust UI (ADR003 Phase 1) must display the exact AutoContext summary passed to Tier 3
- Show alongside raw plan and alias-resolved plan for auditability
- Enables users to verify what context the model saw (critical for trust)
- Format: "Tier 3 context: [N] columns, [M] entity keys, glossary: [terms]"
- **Note**: UI display of context belongs to ADR003 (trust surface), not ADR004

**Model Usage**:

- AutoContext is consumed by ADR001's Tier 3 LLM fallback (not ADR003)
- Tier 3 uses the same model as configured in ADR001 (e.g., `llama3.1:8b` via Ollama if local LLM enabled)
- AutoContext is model-agnostic - it's a structured context pack that works with any LLM provider

**Ownership Boundaries**:

- **ADR004 owns**: Context Assembly (deterministic construction from schema, docs, aliases)
- **ADR001 owns**: Tier 3 LLM implementation (consumes AutoContext, produces QueryPlan)
- **ADR003 owns**: Execution validation (refuses plans referencing columns not in AutoContext), confidence gating, run-key construction, UI display of context
- **ADR002 owns**: Metadata persistence (where AutoContext may be cached)

**ADR004 is supply. ADR003 is enforcement.**

**LLM Contract** (Critical - Prevents Hallucination):

Tier 3 LLM may:
- **Select** which columns from AutoContext are relevant to the query
- **Map** query phrases to known aliases in AutoContext
- **Express** negation and multi-part structure using QueryPlan schema
- **Emit** QueryPlan only using fields present in AutoContext

Tier 3 LLM may NOT:
- ❌ **Invent** columns not present in AutoContext
- ❌ **Invent** filters not derivable from AutoContext codebooks
- ❌ **Infer** entities not explicitly marked in AutoContext (e.g., cannot "assume" patient_id semantics unless AutoContext labels it as entity_key)
- ❌ **Enrich** or generate context beyond what's in AutoContext

**Enforcement**: ADR003 validation (Phase 3) refuses execution if QueryPlan references columns not present in AutoContext. This prevents hallucinated plans from executing.

**How Tier 3 Consumes AutoContext**:

Tier 3 prompt should be **structured parsing**, not chat:

```
System: "Return only JSON matching QueryPlan schema. You may only reference columns present in the AutoContext pack."

Provide:
- The query text
- The AutoContext pack (structured JSON) - AUTHORITATIVE, CLOSED
- Allowed operators / intents list
- QueryPlan schema with required fields

Require fields:
- intent
- primary_variable or grouping_variable when intent needs it (must exist in AutoContext)
- explicit filters list with negation encoded structurally (values must exist in AutoContext codebooks)
- requires_filters/entity_key/scope per ADR003 extensions
- requires_grouping per ADR003 extensions

Constraint: All column references must exist in AutoContext. If query mentions a column not in AutoContext, return QueryPlan with confidence=0.0 and failure_reason="Column not found in dataset."

Validate and refuse if it violates the contract. No "best effort execution" on invalid plans.
```

**Rationale**: Structured prompt with authoritative, closed context enables Tier 3 to:
- Map "BMI" → canonical column name using aliases (only if BMI column exists in AutoContext)
- Understand "1: Yes 2: No" → categorical codebook (only if codebook present in AutoContext)
- Identify `patient_id` as entity_key → avoid grouping on it (only if AutoContext labels it as entity_key)
- Extract "not on statin" → filter condition using codebook values (only if statin column + codebook exist in AutoContext)

This directly addresses production failures where Tier 3 returns plans with no variables or wrong grouping. The key constraint: **"may only reference columns present in AutoContext"** - this is the entire ballgame.

## Implementation

### Phase 1: Documentation Detection and Extraction (P0)

**Goal**: Identify and extract documentation files from ZIP uploads.

**Files to Modify**:
- `src/clinical_analytics/ui/storage/user_datasets.py`
  - Add `extract_documentation_files()` helper
  - Update `extract_zip_tables()` to return both tables and doc files
  - Store doc files in `{upload_id}/docs/` directory

**New File**: `src/clinical_analytics/core/doc_parser.py`

```python
"""
Documentation Parser for Schema Inference

Extracts text content from various documentation formats (PDF, Markdown, text)
to provide context for schema inference.
"""

from pathlib import Path
from typing import Any

def extract_context_from_docs(file_paths: list[Path]) -> str:
    """
    Extract text content from documentation files.

    Args:
        file_paths: List of paths to documentation files

    Returns:
        Concatenated text context (truncated to 50,000 chars if needed)
    """
    # Implementation:
    # 1. Iterate through file_paths
    # 2. Detect file type (PDF, MD, TXT)
    # 3. Extract text using appropriate library
    # 4. Concatenate with separators
    # 5. Truncate if >50k chars (with warning log)
    pass

def extract_pdf_text(pdf_path: Path) -> str:
    """Extract text from PDF using pymupdf."""
    # Use pymupdf (fitz) for extraction
    pass

def extract_markdown_text(md_path: Path) -> str:
    """Extract text from Markdown file."""
    # Direct read (preserve structure)
    pass
```

**Dependencies**: Add `pymupdf` to `pyproject.toml`:
```toml
[project]
dependencies = [
    # ... existing dependencies ...
    "pymupdf>=1.23.0",  # PDF extraction (fitz)
]
```

**Success Criteria**:
- [ ] Documentation files detected in ZIP uploads (`docs/` or root)
- [ ] PDF, Markdown, and text files extracted successfully
- [ ] Extracted text stored in metadata JSON
- [ ] Files stored in `{upload_id}/docs/` for audit trail

### Phase 2: Schema Inference Integration (P0)

**Goal**: Pass documentation context to schema inference engine.

**Files to Modify**:
- `src/clinical_analytics/core/schema_inference.py`
  - Update `infer_schema()` to accept `doc_context: str | None = None`
  - Enhance `parse_dictionary_pdf()` to also parse plain text (not just PDF)
  - Use `doc_context` to populate `DictionaryMetadata` when available
  - Merge dictionary metadata with data-driven inference

**Files to Modify**:
- `src/clinical_analytics/ui/storage/user_datasets.py`
  - In `save_zip_upload()`, extract documentation and pass to schema inference
  - Store `doc_context` in metadata JSON for future reference

**Success Criteria**:
- [ ] `SchemaInferenceEngine.infer_schema()` accepts `doc_context` parameter
- [ ] Documentation context improves column mapping confidence
- [ ] Dictionary metadata merged with data-driven inference
- [ ] Schema inference uses dictionary descriptions when available

### Phase 3: Tier 3 AutoContext Packager (P0)

**Goal**: Build bounded, privacy-safe context pack for ADR001's Tier 3 LLM fallback.

**New Component**: `src/clinical_analytics/core/autocontext.py`

```python
"""
AutoContext Packager for Tier 3 LLM Fallback

Builds bounded, privacy-safe context packs from schema inference,
documentation, and alias mappings to enable grounded query parsing.
"""

from dataclasses import dataclass
from typing import Literal

@dataclass
class ColumnContext:
    """Column metadata for AutoContext."""
    name: str
    normalized_name: str
    system_aliases: list[str]
    user_aliases: list[str]
    dtype: Literal["numeric", "categorical", "datetime", "id"]
    units: str | None = None
    codebook: dict[str, str] | None = None
    stats: dict[str, Any] = None

@dataclass
class AutoContext:
    """Bounded, privacy-safe context pack for Tier 3 LLM."""
    dataset: dict[str, str]  # {upload_id, dataset_version, display_name}
    entity_keys: list[str]  # Candidate columns ranked
    columns: list[ColumnContext]
    glossary: dict[str, Any]  # Extracted terms, abbreviations, notes
    constraints: dict[str, Any]  # {no_row_level_data: true, max_tokens: N}

def build_autocontext(
    semantic_layer,
    inferred_schema,
    doc_context: str | None = None,
    query_terms: list[str] | None = None,
    max_tokens: int = 4000,
) -> AutoContext:
    """
    Build AutoContext pack from schema inference, documentation, and aliases.

    **Deterministic construction** - no LLM inference. Same inputs always produce same output.
    Think: compiler metadata, not chat memory.

    Args:
        semantic_layer: SemanticLayer instance (for aliases, column metadata)
        inferred_schema: InferredSchema from SchemaInferenceEngine
        doc_context: Extracted documentation text (from Phase 1)
        query_terms: Optional query terms for relevance filtering
        max_tokens: Token budget for context (default: 4000)

    Returns:
        AutoContext pack (privacy-safe, bounded, authoritative, closed)

    Raises:
        ValueError: If no columns found in schema (empty dataset)
    """
    # Implementation (all deterministic, no LLM):
    # 1. Extract entity keys from inferred_schema (patient_id, etc.) - deterministic ranking
    # 2. Build column catalog from semantic layer:
    #    - Get canonical columns (from schema)
    #    - Get system aliases (from alias index - deterministic)
    #    - Get user aliases (from ADR003 persisted mappings - deterministic)
    #    - Get inferred types (from schema inference - deterministic)
    #    - Get codebooks (from column name patterns: "1: Yes 2: No" - deterministic parsing)
    #    - Get lightweight stats (min/max/mean for numeric, top N for categorical - deterministic aggregation)
    # 3. Extract glossary from doc_context (abbreviations, definitions - deterministic text extraction)
    # 4. Filter columns by relevance if query_terms provided (lexical + alias match - deterministic)
    # 5. Enforce token budget (truncate if needed - deterministic truncation strategy)
    # 6. Validate no row-level data included (assertion check)
    # 7. Return authoritative, closed AutoContext (only what exists, nothing invented)
    pass
```

**Files to Modify**:
- `src/clinical_analytics/core/nl_query_engine.py` (ADR001's Tier 3)
  - Update `_llm_parse()` to accept `AutoContext` parameter
  - Build structured prompt with AutoContext pack
  - Use AutoContext for variable matching and codebook understanding

**Files to Modify**:
- `src/clinical_analytics/core/semantic.py`
  - Add method to extract column metadata for AutoContext
  - Provide user aliases (from ADR003 Phase 2 persistence)

**Integration with ADR003 Trust UI**:
- Trust UI (Phase 1) must display AutoContext summary passed to Tier 3
- Show alongside raw plan and alias-resolved plan
- Format: "Tier 3 context: [N] columns, [M] entity keys, glossary: [terms]"

**Success Criteria**:
- [ ] AutoContext pack built deterministically from schema inference + documentation + aliases (no LLM in construction)
- [ ] No row-level data in AutoContext (privacy-safe)
- [ ] Token budget enforced (truncation if needed)
- [ ] AutoContext is authoritative and closed (only contains what exists in dataset/docs)
- [ ] Tier 3 receives AutoContext and uses it for variable matching (only references columns in AutoContext)
- [ ] ADR003 validation refuses QueryPlans referencing columns not in AutoContext
- [ ] Trust UI displays AutoContext summary for auditability
- [ ] Tier 3 produces valid QueryPlans with variables (not `matched_vars=[]`) when columns exist in AutoContext
- [ ] Tier 3 returns `confidence=0.0` with `failure_reason="Column not found in dataset"` when query mentions column not in AutoContext

### Phase 4: Testing (P1)

**Goal**: Verify documentation ingestion improves schema inference accuracy.

**New Test File**: `tests/core/test_doc_parser.py`

```python
def test_extract_pdf_text():
    """Test PDF text extraction."""
    # Create mock PDF with column definitions
    # Verify text extraction works

def test_extract_markdown_text():
    """Test Markdown text extraction."""
    # Create mock Markdown file
    # Verify text extraction preserves structure

def test_extract_context_truncation():
    """Test that large documentation is truncated appropriately."""
    # Create large text file (>50k chars)
    # Verify truncation with warning

def test_schema_inference_with_doc_context():
    """Test that documentation context improves schema inference."""
    # Create test DataFrame with ambiguous columns
    # Create documentation file defining columns
    # Verify schema inference uses documentation
```

**New Test File**: `tests/loader/test_doc_extraction.py`

```python
def test_zip_extraction_includes_docs():
    """Test that ZIP extraction identifies documentation files."""
    # Create ZIP with CSV + PDF in docs/
    # Verify docs are extracted and stored

def test_schema_inference_with_uploaded_docs():
    """End-to-end test: upload ZIP with docs, verify schema inference."""
    # Create ZIP with CSV + data dictionary PDF
    # Upload via UserDatasetStorage
    # Verify schema inference uses documentation
```

**New Test File**: `tests/core/test_autocontext.py`

```python
def test_build_autocontext_no_row_data():
    """Test that AutoContext excludes row-level data."""
    context = build_autocontext(...)
    # Verify no patient IDs, no raw rows in context
    assert "patient_id" not in str(context.columns)  # Only metadata, not values

def test_autocontext_includes_codebooks():
    """Test that categorical codebooks are included."""
    # Column: "Statin Used: 0: n/a 1: Atorvastatin ..."
    # Verify codebook extracted: {"0": "n/a", "1": "Atorvastatin", ...}

def test_autocontext_token_budget():
    """Test token budget enforcement."""
    # Create large schema (100+ columns)
    # Verify truncation to max_tokens

def test_autocontext_query_relevance_filtering():
    """Test that query terms filter relevant columns."""
    # Query mentions "BMI" and "LDL"
    # Verify AutoContext includes BMI and LDL columns + similar columns
    # Excludes irrelevant columns

def test_autocontext_includes_user_aliases():
    """Test that user aliases from ADR003 are included."""
    # User taught "VL" → "viral_load"
    # Verify AutoContext includes user_aliases: ["VL"] for viral_load column
```

**Success Criteria**:
- [ ] Unit tests for doc parser (PDF, Markdown, text)
- [ ] Integration tests for ZIP extraction with docs
- [ ] End-to-end test: upload ZIP with docs, verify improved schema inference
- [ ] Unit tests for AutoContext packager (privacy, codebooks, token budget)
- [ ] Integration test: Tier 3 receives AutoContext and produces valid plans
- [ ] Test coverage >80% for new code

## Consequences

### Positive

- **High-Accuracy Mapping**: Resolves ambiguous columns (e.g., `val1` vs `val2`) by reading the actual definition
- **True Zero-Config**: MIMIC-style datasets can be mapped automatically without user intervention
- **Utilizes Existing User Behavior**: Users already have these files in their ZIP exports; we just need to read them
- **Improved Confidence Scores**: Dictionary metadata provides ground truth, increasing inference confidence
- **Better Column Descriptions**: Schema includes rich descriptions from documentation, improving downstream analysis
- **Grounded Tier 3 Parsing**: AutoContext enables Tier 3 to produce valid QueryPlans with variables, addressing "COUNT with no variables" and "group by patient_id" failures
- **Privacy-Safe Context**: AutoContext provides schema cues without exposing patient-level data

### Negative

- **New Dependencies**: Requires adding PDF processing libraries (`pymupdf`) to `pyproject.toml`
- **Token Usage**: Documentation can be verbose. We must implement truncation or summarization strategies to prevent overflowing the LLM context window (though modern windows of 128k+ tokens make this less risky for future LLM integration)
- **Processing Time**: Parsing PDFs adds a few seconds to the initial upload/processing step
- **Storage Overhead**: Extracted documentation text stored in metadata JSON increases file size
- **Parsing Complexity**: PDF structure varies widely; heuristics may miss some column definitions
- **AutoContext Construction Overhead**: Building AutoContext pack adds processing time (schema profiling, alias resolution, codebook extraction)
- **Token Budget Management**: AutoContext must fit within LLM context window; truncation may drop less-relevant columns

### Mitigations

- **Truncation Strategy**: Limit extracted text to 50,000 characters with clear warning logs
- **Lazy Parsing**: Only parse documentation when schema inference confidence is low (future optimization)
- **Caching**: Store parsed dictionary metadata in metadata JSON to avoid re-parsing on schema re-inference
- **Graceful Degradation**: If documentation parsing fails, fall back to data-driven inference (no breaking changes)
- **AutoContext Token Budget**: Enforce max_tokens (default 4000) with relevance filtering (query terms) and truncation (least relevant columns first)
- **Privacy Validation**: AutoContext builder validates no row-level data included (assertion check before passing to Tier 3)
- **Incremental Construction**: Build AutoContext on-demand (when Tier 3 invoked) rather than pre-computing for all queries

## Alternatives Considered

### OCR for Scanned PDFs

**Rejected**: Most data dictionaries (MIMIC, hospital exports) are digital-native PDFs. OCR adds massive complexity (Tesseract) and slowness for edge cases we don't strictly need yet. Can be added in future if scanned PDFs become common.

### Separate "Upload Dictionary" Flow

**Rejected**: Increases friction. The goal is to handle the ZIP file the user *already has*, not ask them to separate files manually. Manual upload flow would violate "Zero-Config" principle.

### LLM-Based Extraction (Immediate)

**Deferred**: Current implementation uses pattern matching and heuristics (existing `parse_dictionary_pdf()`). LLM-based extraction can be added in future if heuristics prove insufficient.

**Note**: AutoContext Packager (Phase 3) provides context to Tier 3 LLM, but documentation extraction itself remains heuristic-based (not LLM-based) for MVP.

### Structured Dictionary Format (JSON/YAML)

**Rejected**: Users don't have structured dictionaries - they have PDFs and Markdown. We must work with what users actually provide, not impose new format requirements.

## Rollback Plan

ADR004 is designed with **surgical rollback** capabilities through phase-level feature flags. Each phase can be independently disabled without code deployment.

### Feature Flags

Configure in `config/nl_query.yaml`:

```yaml
# ADR004: Feature Flags for Surgical Rollback
adr004_enable_doc_extraction: true       # Phase 1: PDF/MD/TXT extraction from ZIP uploads
adr004_enable_schema_context: true       # Phase 2: Context-aware schema inference with docs
adr004_enable_autocontext: true          # Phase 3: Tier 3 AutoContext packager for LLM fallback
adr004_enable_question_generation: false # Phase 4: Proactive question generation (default: false for rollout)
```

### Rollback Procedure

If a phase causes issues in production:

1. **Identify the problematic phase**:
   - Phase 1 (Doc Extraction): Issues with PDF parsing, missing dependencies, performance degradation
   - Phase 2 (Schema Context): Incorrect schema inference, codebook mapping errors
   - Phase 3 (AutoContext): Token budget exceeded, Tier 3 LLM errors
   - Phase 4 (Question Generation): LLM timeout, inappropriate questions generated

2. **Disable the phase** in `config/nl_query.yaml`:
   ```yaml
   adr004_enable_[phase]: false  # Set to false
   ```

3. **Restart service** (config is loaded at startup):
   ```bash
   # Docker/Kubernetes
   kubectl rollout restart deployment/clinical-analytics

   # Streamlit
   pkill -f streamlit
   streamlit run app.py
   ```

4. **Phase is disabled** - no code deployment needed:
   - Phase 1 disabled: Documentation files ignored during upload (falls back to data-only inference)
   - Phase 2 disabled: Schema inference uses data-only approach (no doc_context)
   - Phase 3 disabled: Tier 3 LLM fallback works without AutoContext (lower confidence expected)
   - Phase 4 disabled: No proactive questions generated (users can still ask questions manually)

5. **Monitor and investigate** root cause while system remains operational

### Granularity Trade-Offs

**Feature Flag Rollback** (implemented):
- ✅ No code deployment needed
- ✅ Per-phase disable (surgical)
- ✅ Service restart only (~5-10s downtime)
- ❌ Cannot rollback individual commits within a phase

**Git Rollback** (not implemented):
- ❌ Requires code deployment
- ❌ All-or-nothing (entire PR must be reverted)
- ❌ Longer rollback time (~15-30 min)
- ✅ Can rollback individual commits

**Design Choice**: Feature flags chosen because production issues are typically **phase-specific** (e.g., "PDF parsing broke", not "commit abc123 broke"). Disabling an entire phase is the correct operational granularity.

### Monitoring Recommendations

Monitor these metrics post-deployment:

**Phase 1 (Doc Extraction)**:
- Doc extraction latency (<5s budget enforced in tests)
- PDF parsing errors
- Memory usage during extraction

**Phase 2 (Schema Context)**:
- Schema inference accuracy (codebook coverage, column mappings)
- User overrides (if users manually correct inferred schema, Phase 2 may be underperforming)

**Phase 3 (AutoContext)**:
- Token usage (should stay <4000 tokens)
- Tier 3 LLM confidence (should improve with AutoContext)
- AutoContext build time

**Phase 4 (Question Generation)**:
- LLM timeout rate (should be <5% of requests)
- Question quality (user feedback, question usage rate)
- Question generation latency

## Code References

- `src/clinical_analytics/core/schema_inference.py` - Schema inference engine (target for doc context integration)
- `src/clinical_analytics/ui/storage/user_datasets.py` - ZIP upload processing (target for doc extraction)
- `src/clinical_analytics/core/schema_inference.py:DictionaryMetadata` - Existing dictionary metadata class (can be extended)
- `src/clinical_analytics/core/schema_inference.py:parse_dictionary_pdf()` - Existing PDF parsing (can be enhanced for text/Markdown)

## References

### Related ADRs

- **[ADR002: Persistent Storage Layer](../ADR/ADR002.md)**: Defines metadata JSON schema where `doc_context` will be stored
- **[ADR003: Clinical Trust Architecture & Adaptive Terminology](../ADR/ADR003.md)**: Schema inference feeds into semantic layer registration (ADR003's responsibility). AutoContext Packager integrates with ADR003's trust UI to display context passed to Tier 3 for auditability.
- **[ADR001: Query Plan Producer, Filtering, and Chat-First Execution Rules](../ADR/ADR001.md)**: ADR001's Tier 3 LLM fallback consumes AutoContext Pack to produce valid QueryPlans. AutoContext addresses "NLU without grounded context" failures where Tier 3 returns low-confidence plans with no variables.

### Related Plans

- **Zero-Friction Onboarding** (ADR003 context): Documentation ingestion directly supports "Zero-Config" goal
- **Multi-Table Handler Refactor**: Documentation extraction should work for both single-file and multi-table uploads (feature parity)
