# ADR 003: Clinical Trust Protocol + Adaptive Alias Persistence

## Status
Proposed

## Related ADRs

- **[ADR001: Query Plan Producer, Filtering, and Chat-First Execution Rules](../ADR/ADR001.md)**: ADR001 owns QueryPlan production (NLU parsing, including Tier 3 LLM fallback). ADR003 provides trust UI and alias persistence that ADR001 depends on. ADR003's semantic layer execution contract (Phase 3) assumes ADR001 produces valid QueryPlans - see Phase 4 for hardening requirements. ADR001 does not implement trust UI - that's ADR003's responsibility.
- **[ADR002: Persistent Storage Layer](../ADR/ADR002.md)**: ADR002 owns metadata JSON schema and persistence. ADR003's alias mappings are stored in ADR002's metadata schema (per upload_id + dataset_version scope).

## Context
Clinical research tools fail on three axes:
1. **Trust Gap**: Physicians reject aggregate statistics without audit trails
2. **Onboarding Friction**: Configuration before use kills adoption
3. **Alias Resolution Failures**: System can't map user terminology ("VL", "LDL") to column names ("viral_load", "LDL mg/dL") without user teaching

**Note**: This ADR does NOT own NLU parsing (that's ADR001). This ADR owns:
- Trust/verification UI (audit trails, patient-level exports)
- Adaptive alias persistence (user teaches system terminology â†’ persists to metadata)

### Real-World Failure Evidence (2025-12-29)

**Four production queries from clinical dataset ("Statin use - deidentified") demonstrated parser limitations that motivate ADR003's trust UI and alias persistence features. These failures are documented as evidence of why trust/verification and adaptive learning are necessary, not as requirements for parser improvements (which are ADR001's responsibility).**

#### Failure 1: Multi-Part Query with Wrong Variable Matching

**User Query**:
```
"count number of current and former smokers, and give the breakdown how long since the former smokers stopped smoking"
```

**Expected Behavior**:
- **Primary variable**: `"Nicotine Use"` (column contains: "1. Current", "2. Former", "3. Never")
- **Secondary variable**: `"If Former Smoker Quit"` (column contains time since quitting: "0=N/A", "1. Unknown", "2. <6 mo ago", "3. 6mo-1.5yrs", etc.)
- **Intent**: DESCRIBE with grouping (count by smoking status, then breakdown by quit time for former smokers)
- **Confidence**: Should be high (>0.75) - clear intent with specific variables

**Actual Behavior** (from logs):
- **Matched variables**: `['If Former Smoker Quit ...', 'Statin ADE ...', 'Regimen (if 9 or other)']`
- **Primary variable inferred**: `'If Former Smoker Quit ...'` (wrong - this is secondary variable)
- **Intent**: DESCRIBE
- **Confidence**: `0.3` (fallback threshold - Tier 2 failed, fell back to default)
- **Result**: System analyzed wrong variable, producing incorrect breakdown

**Root Cause Analysis**:
1. **Tier 2 (Semantic Embeddings) failed**: Query didn't match any template with sufficient confidence (>0.7 threshold)
2. **Variable extraction failed**: N-gram matching didn't connect "current and former smokers" to "Nicotine Use" column
3. **Multi-part query not handled**: System couldn't parse compound question (smoking status + time breakdown)
4. **Fell back to default**: `QueryIntent(intent_type="DESCRIBE", confidence=0.3)` (line 654 in `nl_query_engine.py`)

**Why This Matters**: This is exactly the type of clinical query that requires structured understanding - the user wants a two-level breakdown (smoking status, then quit time), but the system treated it as a single variable description.

#### Failure 2: Filter Condition Not Extracted, Wrong Primary Variable

**User Query**:
```
"Average LDL for those patients not on statin?"
```

**Expected Behavior**:
- **Primary variable**: `"LDL mg/dL"` (the metric being averaged)
- **Filter condition**: `{"column": "Statin Prescribed?", "operator": "==", "value": "2: No"}`
- **Intent**: DESCRIBE with filter (compute mean of LDL, filtered to patients not on statin)
- **Confidence**: Should be high (>0.75) - clear intent with filter

**Actual Behavior** (from logs):
- **Matched variables**: `['Age', 'patient_id', 'Statin Used: ...']`
- **Primary variable inferred**: `'Age'` (completely wrong - should be LDL)
- **Filter condition**: Not extracted (empty)
- **Intent**: DESCRIBE
- **Confidence**: `0.3` (fallback threshold)
- **Result**: System analyzed Age instead of LDL, and didn't filter to non-statin patients

**Root Cause Analysis**:
1. **Variable matching failed**: "LDL" didn't match "LDL mg/dL" column (exact substring matching too strict)
2. **Filter extraction not implemented**: No code exists to parse "not on statin" â†’ filter condition
3. **Wrong variable selected**: System defaulted to first matched variable (Age) instead of correct one
4. **Negation not handled**: "not on statin" requires understanding negation + column mapping

**Why This Matters**: This query demonstrates why trust UI (Phase 1) is critical - users need to verify the parser selected the correct variable and applied filters. Alias persistence (Phase 2) helps by teaching "LDL" â†’ "LDL mg/dL" for future queries.

#### Failure 3: Categorical Variable Treated as Numeric - Wrong Statistics

**User Query**:
```
"how many patients on statins? which statin most prescribed?"
```

**Expected Behavior**:
- **Primary variable**: `"Statin Prescribed? 1: Yes 2: No"` (categorical - Yes/No)
- **Secondary variable**: `"Statin Used: 1: Atorvastatin 2: Rosuvastatin ..."` (categorical - statin type)
- **Intent**: DESCRIBE with frequency counts (not numeric statistics)
- **Expected output**: 
  - Count: "X patients on statins (Y%)"
  - Breakdown: "Most prescribed: Atorvastatin (Z patients, W%)"
- **Confidence**: Should be high (>0.75) - clear intent for counts/percentages

**Actual Behavior** (from logs and UI):
- **Matched variables**: `['patient_id', 'Statin Prescribed? 1: Yes 2: No', 'Statin Used: ...']`
- **Primary variable inferred**: `'Statin Prescribed? 1: Yes 2: No'` (correct variable)
- **Intent**: DESCRIBE
- **Confidence**: `0.3` (fallback threshold)
- **Result**: System treated categorical variable as numeric, computed:
  - Mean: 1.41 (meaningless - average of 1/2 encoding)
  - Median: 1.00
  - Std Dev: 0.49
  - Min: 1.00, Max: 2.00
- **User expectation**: Simple count/percentage (e.g., "500 patients on statins (62%)")

**Root Cause Analysis**:
1. **Data type detection failed**: System didn't recognize "1: Yes 2: No" as categorical encoding
2. **Query intent not understood**: "how many" clearly indicates count, not mean/median
3. **Numeric statistics computed for categorical**: Mean of 1/2 encoding is meaningless
4. **Multi-part query partially handled**: Matched both variables but only analyzed one, didn't answer "which statin most prescribed"

**Why This Matters**: 
- **Clinical usability**: Physicians want counts/percentages for categorical variables, not numeric statistics
- **Data type awareness**: System must detect categorical variables (even when encoded as 1/2) and provide frequency tables
- **Query intent**: "how many" should trigger count analysis, not mean computation
- **Multi-part queries**: System matched both variables but didn't answer the second part ("which statin most prescribed")

**Why This Matters**: This demonstrates why type-aware execution (Phase 3) is critical - the semantic layer must detect categorical encoding and return frequency tables, not numeric statistics, even when the parser misclassifies the variable.

#### Failure 4: Query Intent Misunderstood - "How Many" Should Return Count, Not Statistics

**User Query**:
```
"how many were on a statin?"
```

**Expected Behavior**:
- **Intent**: COUNT (not DESCRIBE)
- **Action**: Filter to patients on statin (`Statin Prescribed? == "Yes"` OR `Statin Used != 0`)
- **Output**: Simple number: "X patients were on a statin" (or "X patients (Y%)")
- **Confidence**: Should be high (>0.75) - clear count intent

**Actual Behavior** (from logs and UI):
- **Matched variables**: `['Statin Used: 0: n/a 1: Atorvastatin ...', 'ASCVD Risk % before statin']`
- **Primary variable inferred**: `'Statin Used: ...'` (wrong - should filter, not describe)
- **Intent**: DESCRIBE (wrong - should be COUNT)
- **Confidence**: `0.3` (fallback threshold)
- **Result**: System computed descriptive statistics (mean=0.93, median=1.00, std dev=0.98) for "Statin Used" variable
- **User expectation**: Simple count "X patients were on a statin"

**Root Cause Analysis**:
1. **Query intent not understood**: "how many" clearly indicates COUNT intent, but system defaulted to DESCRIBE
2. **No COUNT intent type**: System only has DESCRIBE, COMPARE_GROUPS, FIND_PREDICTORS - no COUNT intent for simple counting queries
3. **Filter not extracted**: Query implies filter ("were on a statin" = filter condition), but system treated it as variable selection
4. **Wrong analysis type**: System computed statistics instead of counting filtered rows

**Why This Matters**:
- **Basic functionality missing**: "How many" is a fundamental query type that should work out-of-the-box
- **User frustration**: Users ask simple questions, get complex statistics they didn't ask for
- **Query intent understanding**: System must distinguish between:
  - "how many X?" â†’ COUNT with filter
  - "describe X" â†’ DESCRIBE with statistics
  - "average X" â†’ DESCRIBE with mean
- **Missing intent type**: System needs COUNT intent type for simple counting queries

**Why This Matters**: This demonstrates why COUNT intent execution (Phase 3) is critical - the semantic layer must handle COUNT queries correctly even when the parser misclassifies intent. Trust UI (Phase 1) allows users to verify the parser's intent classification.

#### Common Failure Patterns

Both failures share these patterns:

1. **Tier 2 (Semantic Embeddings) insufficient**: 
   - Confidence 0.3 indicates Tier 2 didn't find good template match
   - Embedding similarity threshold (0.7) too high for clinical queries
   - Template library doesn't cover multi-part or filtered queries

2. **Variable extraction too brittle**:
   - N-gram matching fails on clinical terminology ("LDL" vs "LDL mg/dL")
   - Doesn't handle synonyms or abbreviations
   - No understanding of query context (selects wrong variable when multiple match)

3. **Tier 3 (LLM Fallback) is stub**:
   - Current implementation returns default `DESCRIBE` with 0.3 confidence
   - Complex queries fall back to low-confidence defaults
   - This is ADR001's responsibility (out of scope for ADR003)

4. **Filter extraction limitations**:
   - "not on statin" condition not parsed by current parser
   - No code path to parse embedded filter conditions
   - ADR003 mitigates via plan validation (Phase 3) - rejects plans with missing filters if user expects them

5. **Data type detection and query intent mismatch**:
   - Categorical variables (Yes/No encoded as 1/2) treated as numeric
   - "how many" queries trigger mean computation instead of counts
   - System computes meaningless statistics (mean of 1/2 encoding = 1.41)
   - User wants simple counts/percentages, not "cute" numeric statistics

6. **Query intent classification missing**:
   - "how many" queries should trigger COUNT intent, not DESCRIBE
   - System lacks COUNT intent type for simple counting queries
   - "how many X?" should filter and count, not compute statistics
   - Basic functionality missing - users can't ask simple count questions

#### Fixability Analysis: Pattern Matching vs LLM

**Observation**: 3 out of 4 failures could theoretically be addressed with improved pattern matching in the parser, but parser improvements are ADR001's responsibility, not ADR003's.

| Failure | Parser Fix Complexity | ADR003 Mitigation |
|---------|----------------------|------------------|
| **Failure 1** (Multi-part query) | Requires LLM or complex parsing | Trust UI shows parser output, user can verify |
| **Failure 2** (Filter extraction) | Pattern matching could work | Plan validation rejects plans with `requires_filters=True` and empty filters (explicit contract violation) |
| **Failure 3** (Categorical as numeric) | Pattern matching could detect encoding | Type-aware execution (Phase 3) executes correctly for specified variables (does not infer missing intent) |
| **Failure 4** (COUNT intent) | Pattern matching could classify | COUNT contract validation (Phase 3) enforces entity_key and scope semantics |

**ADR003 Mitigation Strategy**:
1. **Trust UI (Phase 1)**: Show parser output and effective execution so users can verify correctness
2. **Alias Persistence (Phase 2)**: Teach system terminology to improve future matches
3. **Type-Aware Execution (Phase 3)**: Execute safely and type-correctly for variables specified in plan (does not infer missing intent or filters)
4. **Plan Validation (Phase 3)**: Reject plans that violate explicit QueryPlan contract fields (e.g., `requires_filters=True` with empty filters). The executor does not infer missing filters from natural language.
5. **Confidence Gating (Phase 3)**: Hard gate - refuse execution when `confidence < threshold`, no side effects
6. **Intent Validation (Phase 3)**: Refuse COUNT queries with missing grouping when query pattern implies breakdown
7. **Run-Key Determinism (Phase 3)**: Generate deterministic run_key from complete canonical plan, UI verifies plan matches before displaying cached results
8. **Breakdown Validation (Phase 3)**: Refuse queries with `grouping_variable=entity_key` when query implies categorical breakdown
9. **Filter Deduplication (Phase 3)**: Detect and warn/deduplicate redundant filters (filtering and grouping on same field)

**Note**: Parser improvements (pattern matching, LLM hardening) are ADR001's responsibility, not ADR003's. This ADR focuses on execution-layer correctness and trust, not parser fixes.

#### Semantic Layer Responsibilities (Executor, Not Parser)

**Critical Architectural Clarification**: The semantic layer is the **executor and validator** of QueryPlans, not the producer. It cannot parse natural language - that's the NLU layer's job (ADR001).

**What Semantic Layer CAN Do** (Executor Role):
- **Validate QueryPlan**: Check columns exist, operators are valid, types match
- **Normalize aliases**: Resolve "VL" â†’ "viral_load" using persisted alias mappings (from ADR003)
- **Type-aware aggregation**: 
  - Categorical columns â†’ frequency tables (not mean/median)
  - Numeric columns â†’ descriptive statistics (mean, median, std dev)
  - Detects categorical encoding patterns ("1: Yes 2: No") and treats as categorical
- **Execute QueryPlan**: 
  - `QueryPlan(intent="COUNT", filters=[...])` â†’ `query(metrics=["count"], filters={...})`
  - `QueryPlan(intent="DESCRIBE", metric="X")` â†’ `query(metrics=["avg_X"])` or frequency table if categorical
  - `QueryPlan(intent="COMPARE_GROUPS", metric="X", group_by="Y")` â†’ Grouped aggregation
- **Safe query compilation**: DuckDB/Ibis SQL generation with validation

**What Semantic Layer CANNOT Do** (Not Parser):
- âŒ Parse natural language intent ("how many" â†’ COUNT)
- âŒ Infer multi-step plans from one sentence
- âŒ Handle negation logic reliably without structured input ("not on statin" requires NLU to map to column + value)
- âŒ Extract filters from query text (that's NLU's job)

**Architectural Split**:
- **NLU Layer (ADR001)**: Parses text â†’ produces `QueryPlan`
- **Semantic Layer (ADR003)**: Validates `QueryPlan` â†’ executes via SQL/Ibis â†’ returns results

**Impact on Failures**:
- Failures 2, 3, 4 are NLU parsing failures (ADR001's responsibility)
- Semantic layer can help with Failure 3 (categorical detection) and Failure 4 (COUNT execution) once NLU produces correct QueryPlan
- Failure 1 (multi-part queries) requires NLU enhancement (ADR001), not semantic layer changes

## Decision

**Scope Clarification**: This ADR owns trust UI and alias persistence. It does NOT own NLU parsing (that's ADR001). The semantic layer executes QueryPlans produced by ADR001's NLU layer.

### 1. Trust Protocol: Mandatory Verification UI
Every result includes patient-level audit:

```python
st.info(f"Average Viral Load: {mean_val}")

with st.expander("ðŸ”Ž Verify: Show source patients"):
    audit_cols = ["patient_id", primary_variable]
    if filters:
        audit_cols.extend(f.column for f in filters)
    
    st.dataframe(result_df.select(audit_cols).head(100))
    st.download_button("Download cohort CSV", result_df.write_csv())
```

**Rationale**: Clinicians must validate AI reasoning. Excel export enables external verification.

### 2. Adaptive Alias Persistence: Error-Driven Learning

**Note**: This extends the existing alias index system in `SemanticLayer` (`_build_alias_index()`, `_normalize_alias()`), not replaces it. The codebase already has:
- Column name normalization âœ…
- Fuzzy matching âœ…
- Collision detection âœ…

**Action**: Add persistence layer for user-added aliases (extend, don't replace).

**Alias Scope Rules** (per ADR002 metadata schema):
- Aliases scoped to `(upload_id, dataset_version)` - not global
- **Precedence**: User aliases always override system aliases for the same normalized key within `(upload_id, dataset_version)`. Collisions are surfaced in UI and never silently remapped.
- **Orphan Handling**: If schema changes and target column missing: mark alias orphaned and ignore. Do not silently remap to a different column.
- Prevents "poison" aliases from breaking other datasets

**Flow:**
```python
def handle_unknown_term(term: str, available_columns: list[str], semantic_layer: SemanticLayer, upload_id: str, dataset_version: str):
    st.error(f"Unknown term: '{term}'")
    
    with st.expander("ðŸ’¡ Add alias?"):
        col = st.selectbox("Map to column:", available_columns)
        if st.button("Save"):
            # Extend existing alias index with user mapping
            semantic_layer.add_user_alias(term, col)
            # Persist to dataset metadata JSON (per ADR002) - scoped to upload_id + dataset_version
            save_alias_mapping(upload_id, dataset_version, term, col)
            st.success("Retry your query")
```

**Persistence** (per ADR002):
- Store in `metadata["alias_mappings"]` dict in dataset metadata JSON
- Format: `{"user_aliases": {"VL": "viral_load", "LDL": "LDL mg/dL"}, "system_aliases": {...}}`
- Loaded on semantic layer initialization, merged with system aliases

### 3. Semantic Layer QueryPlan Execution Contract

**Note**: This ADR does NOT own QueryPlan production (that's ADR001). This section defines how the semantic layer executes QueryPlans produced by ADR001's NLU layer.

**QueryPlan Contract Extensions Required**:

ADR001 defines the base `QueryPlan` schema. ADR003 requires the following extensions for deterministic validation:

```python
@dataclass
class QueryPlan:
    """Structured query plan (base from ADR001, extensions for ADR003)."""
    # ... existing fields from ADR001 ...
    
    # ADR003 extensions for deterministic validation:
    requires_filters: bool = False  # True if query explicitly requires filters (e.g., "COUNT filtered patients")
    requires_grouping: bool = False  # True if query pattern implies breakdown ("most common", "by", "per", "breakdown")
    entity_key: str | None = None  # For COUNT: entity to count (default: dataset primary key if known)
    scope: Literal["all", "filtered"] = "all"  # For COUNT: count all rows vs filtered cohort
```

**Rationale**: The executor cannot infer user expectations from natural language. Explicit contract fields enable deterministic validation without NL interpretation.

**Semantic Layer Execution Contract**:

The semantic layer receives a `QueryPlan` (from ADR001) and must:

1. **Validate QueryPlan Contract**:
   - Check all columns exist (after alias resolution)
   - Validate operators are supported
   - Check type compatibility (e.g., can't use ">" on categorical)
   - **COUNT-specific validation**:
     - If `scope="all"` and `filters` present: Refuse with contract violation. `scope="all"` means "no cohort restriction." Use `scope="filtered"` to apply filters.
     - If `scope="filtered"` and `filters=[]` and `requires_filters=True`: Refuse with clear error
     - If `scope="filtered"` and `filters=[]`: Refuse (filters required for filtered scope)
     - If `entity_key` missing: Default to dataset primary key if known, else refuse
     - **Note**: `requires_filters` is only meaningful for `scope="filtered"`; it is ignored for `scope="all"`.

2. **Normalize Aliases** (including user-added aliases):
   - Resolve "VL" â†’ "viral_load" using persisted alias mappings
   - Handle collisions (multiple columns match same alias)
   - Use canonical column names for execution

3. **Type-Aware Execution** (for variables specified in plan):
   - **Categorical columns**: Detect encoding patterns ("1: Yes 2: No") â†’ return frequency tables, not mean/median
   - **Numeric columns**: Return descriptive statistics (mean, median, std dev)
   - **COUNT intent**: Use `query(metrics=["count"], filters={...}, entity_key=...)` for SQL aggregation
   - **DESCRIBE intent**: Use `query(metrics=["avg_X"])` for numeric, frequency table for categorical
   - **Limitation**: Executor executes safely and type-correctly for variables specified in the plan. It does not infer missing intent, filters, or variables. If "LDL" isn't selected, executor cannot "guess" it.

4. **Refuse Invalid Plans**:
   - Unknown columns â†’ raise `ValueError` with suggestions
   - Invalid operators â†’ raise `ValueError` with supported operators
   - Type mismatches â†’ raise `TypeError` with explanation
   - **Contract violations**: If `requires_filters=True` but `filters=[]` â†’ raise `ValueError` with explanation

**Example Execution**:
```python
# QueryPlan from ADR001 NLU layer:
plan = QueryPlan(
    intent="COUNT",
    filters=[FilterSpec(column="Statin Prescribed?", operator="==", value="Yes")],
    requires_filters=True,  # Explicit: query requires filters
    entity_key="patient_id",  # Count patients, not rows
    scope="filtered",  # Count filtered cohort, not all
    confidence=0.9
)

# Semantic layer validates:
# - requires_filters=True and filters not empty âœ“
# - entity_key specified âœ“
# - scope="filtered" with filters present âœ“

# Semantic layer executes:
result = semantic_layer.query(
    metrics=["count"],
    filters={"Statin Prescribed?": "Yes"},  # Converted from FilterSpec
    entity_key="patient_id"
)
# Returns: {"count": 500} (not mean/median/std dev)

# Trust UI shows:
# - Raw plan: COUNT with filter "Statin Prescribed? == Yes"
# - Effective execution: COUNT(patient_id) WHERE "Statin Prescribed?" = "Yes"
# - Cohort size: count_filtered=500, count_total=807 (62.0%)
```

## Implementation

### Existing Infrastructure

1. **Query Parsing**: Three-tier system already exists (`NLQueryEngine` in `src/clinical_analytics/core/nl_query_engine.py`):
   - Tier 1: Pattern matching (regex) âœ…
   - Tier 2: Semantic embeddings âœ…
   - Tier 3: LLM fallback (stub) âš ï¸
   
   **Note**: Tier 3 LLM fallback is ADR001's responsibility. ADR003 Phase 4 documents hardening requirements that ADR001 must implement for reliable QueryPlan production. ADR003's execution contract assumes valid QueryPlans from ADR001.

2. **Alias System**: Semantic layer already has alias index (`SemanticLayer._build_alias_index()` in `src/clinical_analytics/core/semantic.py`):
   - Column name normalization âœ…
   - Fuzzy matching âœ…
   - Collision detection âœ…
   
   **Action**: Add persistence layer for user-added aliases (extend, don't replace).

3. **QueryIntent Structure**: Currently uses `@dataclass` (not Pydantic) in `src/clinical_analytics/core/nl_query_engine.py`:
   - `filters: dict[str, Any]` (should be `list[dict[str, Any]]` per ADR001)
   - Consider keeping dataclass for consistency (or justify Pydantic migration if validation/JSON schema benefits needed)

### Implementation Plan

**Phase 1: Trust Layer (P0 - Mandatory Verification UI)**
- File: `src/clinical_analytics/ui/pages/3_ðŸ’¬_Ask_Questions.py`
- Add verification expander to result rendering functions (`_render_focused_descriptive()`, `render_comparison_analysis()`, etc.)
- **Show QueryPlan (raw)**: Intent, metric, filters as parsed
- **Show Alias-Resolved Plan**: Canonical column names (post-alias resolution)
- **Show Effective Execution**:
  - Dataset used
  - Entity key used for COUNT (if applicable)
  - Resolved canonical column names
  - **Effective filters** (post-normalization, with actual values):
    - Drop tautologies (non-restrictive filters) or label them "non-restrictive"
    - Show normalized filter values (e.g., "Current Regimen IN [1,2,3,4,5,6,7,8,9]" â†’ label as "non-restrictive" if all valid codes)
  - **Cohort size**:
    - `count_total` (same entity_key, no filters) - denominator for percentages
    - `count_filtered` (if filtered) - numerator
    - Percentage computed as `count_filtered / count_total`
  - **Run-key and audit trail**: Display run_key + "generated from: [query]" for audit
  - Compiled SQL (optional, behind expander for advanced users)
- **Privacy/Performance**: Patient-level export is capped (e.g., first 100 rows) or requires explicit confirmation for full export
- Add patient-level export with cohort size + run_id
- Coordinate with ADR001 Phase 2.4 (breakdown reporting)
- Reference: `src/clinical_analytics/ui/components/result_interpreter.py` for existing result rendering patterns

**Rationale**: Trust model collapses if UI shows one thing and executes another. Effective execution display enables users to verify what actually ran.

**Phase 2: Adaptive Alias Persistence (P0 - Error-Driven Learning)**
- File: `src/clinical_analytics/core/semantic.py` (extend `SemanticLayer`)
- Add `add_user_alias(term: str, column: str, upload_id: str, dataset_version: str)` method
- Persist to dataset metadata JSON (per ADR002 metadata schema)
- Load aliases on semantic layer initialization
- Handle orphaned aliases (column missing after schema change)
- UI: Add "Add alias?" expander in error handling (coordinate with ADR001 NLU layer)

**Phase 3: Semantic Layer QueryPlan Execution (P1 - Type-Aware Execution)**
- File: `src/clinical_analytics/core/semantic.py` (extend `SemanticLayer.query()`)
- Add `execute_query_plan(plan: QueryPlan, confidence_threshold: float = 0.75) -> dict[str, Any]` method
- **Confidence and Completeness Gating** (hard gate):
  - Gate: `confidence >= threshold AND is_complete AND validation_passes`
  - `is_complete` = required fields present for intent:
    - COUNT: requires `entity_key` OR `grouping_variable`
    - DESCRIBE: requires `primary_variable`
    - COMPARE_GROUPS: requires `primary_variable` AND `grouping_variable`
  - If gate fails: Refuse execution, return error dict with `requires_confirmation=True`, `failure_reason` explaining what's missing
  - No side effects: Do NOT cache, store, or execute when gate fails
  - UI must show plan with confidence score, completeness status, and missing fields, require explicit confirmation
- **Validate QueryPlan Contract**:
  - Check columns exist (after alias resolution)
  - Validate operators are supported
  - Check type compatibility
  - **COUNT-specific validation**:
    - Validate `entity_key`, `scope` semantics (refuse `scope="all"` with filters, refuse `scope="filtered"` with empty filters)
    - **Breakdown validation**: 
      - If `grouping_variable=entity_key` (e.g., `patient_id`), refuse with error "Cannot group by entity key. Did you mean to group by a categorical field?"
      - If `grouping_variable` is high-cardinality (near-unique, e.g., patient_id with ~N unique values where N â‰ˆ total rows) and query implies categorical grouping, refuse with error "Grouping by [entity_key] yields ~[N] groups. Choose a categorical column (e.g., Current Regimen)."
    - **Missing grouping**: If `requires_grouping=True` (from QueryPlan) and `grouping_variable=None`, refuse with error "This query requires grouping. Which field should I group by?"
    - **COUNT completeness**: If `intent=COUNT` and both `entity_key=None` and `grouping_variable=None`, refuse with error "COUNT plan missing entity_key/grouping_variable. Pick what to count: patient_id vs rows, and optionally group by a variable."
- **Filter Deduplication**:
  - Detect redundant filters: If grouping on field X and filtering on field X, warn and deduplicate (keep grouping, remove redundant filter)
- **Type-aware aggregation**: Execute safely for variables specified in plan (categorical â†’ frequency, numeric â†’ stats)
- **COUNT intent**: Use `query(metrics=["count"], filters={...}, entity_key=...)` with scope validation
- **Refuse invalid plans**: Contract violations â†’ clear error messages
- **Output formatting**: Always compute both `count_total` (denominator) and `count_filtered` (numerator) for percentage reporting
- **Run-key generation**: Generate deterministic `run_key = hash(dataset_version + canonical_plan_json + query_text_signature)` where:
  - `canonical_plan_json` includes all fields (intent, variables, filters, entity_key, scope) in normalized form
  - `query_text_signature` is normalized query text (lowercase, whitespace normalized) OR stable hash of original query
  - **Rationale**: Different queries must produce different run_keys even if plan structure is similar (prevents "average BMI" and "average ldl" from colliding)

**Note**: NLU parsing (QueryPlan production) is ADR001's responsibility. This ADR only handles execution and validation.

### Assumptions and Known Limitations

**Scope Clarification**: ADR003 assumes ADR001 produces a best-effort `QueryPlan` from its three-tier parsing system (pattern matching, semantic embeddings, LLM fallback). ADR003 does NOT own parser improvements - that's ADR001's responsibility and is explicitly out of scope for this ADR.

**Current State**: ADR001 is marked complete. The three-tier system exists:
- Tier 1: Pattern matching (regex) âœ…
- Tier 2: Semantic embeddings (sentence-transformers) âœ…
- Tier 3: LLM fallback (stub - returns low-confidence DESCRIBE intent) âš ï¸

**Known Failure Modes**: Real-world production queries (documented in Context section and validated in production logs) demonstrate that complex clinical queries fail with current parser:
- **Multi-part queries** ("count X and breakdown Y") â†’ Tier 3 stub returns low confidence
- **Filter extraction with negation** ("not on statin") â†’ Not parsed, filters missing
- **COUNT intent** ("how many") â†’ Misclassified as DESCRIBE, wrong statistics computed
- **Categorical variables** (encoded as 1/2) â†’ Treated as numeric, meaningless statistics

### Production Contract Violations (2025-12-30)

**Real-world production logs demonstrate core contract failures that ADR003 must address through execution-layer guardrails:**

#### 1. Confidence Gating Not Enforced

**Observed Behavior** (from logs):
- `confidence=0.3, tier=llm_fallback` â†’ `analysis_execution_triggered` â†’ `analysis_computation_complete`
- Low-confidence parses execute without user confirmation

**Contract Violation**: System promises "only auto-run when confidence â‰¥ threshold, otherwise require confirmation" but executes anyway.

**ADR003 Requirement**:
- **Hard gate**: `auto_execute := confidence >= threshold AND is_complete AND validation_passes`
  - `confidence >= threshold` (default 0.75)
  - `is_complete` = required fields present for intent (e.g., COUNT requires entity_key OR grouping_variable)
  - `validation_passes` = all contract validations pass (columns exist, operators valid, types compatible)
- **No side effects**: When gate fails, do NOT:
  - Execute analysis computation
  - Cache results
  - Store results in session state
- **UI behavior**: Show parsed plan with confidence score and completeness status, require explicit "Confirm and Run" button if gate fails
- **Validation**: Executor must check confidence AND completeness before execution, refuse if either fails
- **Rationale**: Confidence without plan completeness is theater. A 0.9 confidence COUNT with no variables is still wrong.

#### 2. "Most Common X" Without Grouping Produces Garbage

**Observed Behavior** (from logs):
- Query: `"what was the most common HIV regiment?"`
- Parse: `intent=COUNT, matched_vars=[], grouping_extraction_failed`
- Result: `Total Count = 807` (wrong - should be breakdown by regimen)

**Contract Violation**: System answers a different question than asked with high confidence (0.9). This is the worst failure mode.

**ADR003 Requirement**:
- **Query pattern detection**: Parser should set `requires_grouping: bool = True` in QueryPlan when query contains patterns like "most common", "breakdown", "by", "per"
- **Intent validation**: If `requires_grouping=True` and `grouping_variable=None`, then:
  - Force confidence low OR downgrade intent to "clarify"
  - Do NOT execute COUNT without grouping
- **UI behavior**: Show "Which field represents [X]?" with candidate variables
- **Validation**: Executor must refuse COUNT queries with `requires_grouping=True` and `grouping_variable=None`
- **COUNT completeness check**: If `intent=COUNT` and both `entity_key=None` and `grouping_variable=None`, refuse with error: "COUNT plan missing entity_key/grouping_variable. Pick what to count: patient_id vs rows, and optionally group by a variable."

#### 3. Run-Key / Caching Semantics Leaky

**Observed Behavior** (from logs):
- `"average BMI of patients"` and `"average ldl of all patients"` â†’ Same `run_key=...b7a5a863...`
- UI shows mismatched results (e.g., "Most Prescribed 1.0: 394" appears for BMI query)

**Contract Violation**: Different queries collapse to same execution pattern, causing stale/mismatched results in UI.

**ADR003 Requirement**:
- **Run-key contract**: `run_key = hash(dataset_version + canonical_plan_json + query_text_signature)` where:
  - `canonical_plan_json` includes: `intent_type`, `primary_variable` (canonical), `grouping_variable` (canonical), `filters` (normalized, sorted), `entity_key`, `scope`
  - `query_text_signature` is normalized query text (lowercase, whitespace normalized) OR stable hash of original query
  - **Rationale**: Different queries ("average BMI" vs "average ldl") must produce different run_keys even if plan structure is similar
- **UI binding**: Each result panel must be bound to its specific QueryPlan (not generic cached result)
- **Trust UI display**: Show run_key + "generated from: [query]" in verification expander for audit trail
- **Validation**: Executor must generate deterministic run_key from complete plan + query, UI must verify plan matches before displaying cached results

#### 4. "Broken Down By" Needs Semantic Model (1D vs 2D)

**Observed Behavior** (from logs):
- Query: `"what statins ... broken down by count of patients by their Current Regimen"`
- Parse: `grouping_variable=patient_id` (wrong - should be "Current Regimen" or 2D breakdown)
- Result: Giant list of patient IDs instead of statin Ã— regimen breakdown

**Contract Violation**: Multi-variable breakdowns ("X by Y") are mis-parsed into single-variable grouping on wrong field.

**ADR003 Requirement**:
- **Breakdown types**:
  - **1D**: Count by single categorical field (e.g., "count by statin")
  - **2D**: Pivot-style breakdown (group by A, B) (e.g., "statin by regimen")
- **MVP scope**: If 2D not supported, parser must detect and respond with clarification ("I can do A-by-B OR B-by-A, pick one"), not silently group by ID field
- **Validation**: Executor must refuse queries with `grouping_variable=entity_key` (e.g., `patient_id`) when query implies categorical breakdown
- **High-cardinality detection**: If `grouping_variable` is high-cardinality (near-unique, e.g., patient_id with ~807 unique values) and query implies categorical grouping, refuse with error: "Grouping by [entity_key] yields ~[N] groups. Choose a categorical column (e.g., Current Regimen)."
- **UI behavior**: Show detected breakdown pattern, ask user to confirm 1D or clarify 2D intent

#### 5. Compound-Coded Categorical Columns Need First-Class Handling

**Observed Behavior** (from logs):
- Column: `"Statin Used: 0: n/a 1: Atorvastatin 2: Rosuvastatin ..."`
- Correctly extracted for grouping, produced reasonable counts
- But for `"Current Regimen"`, parser injected redundant filter `IN [1..9]` while grouping on same field

**Contract Violation**: Compound labels not normalized, leading to redundant filters and inconsistent handling of "0: n/a" categories.

**ADR003 Requirement**:
- **Normalization**: Compound-coded columns must be normalized to:
  - `field_name = "current_regimen"` (canonical)
  - `codebook = {1: "Biktarvy", 2: "Symtuza", ..., 0: "n/a"}`
- **NA handling**: Default exclude "0: n/a" or missing categories, with explicit "include missing" toggle if needed
- **Filter deduplication**: If grouping on field X, do NOT also filter on X unless explicitly requested
- **Tautology detection**: If filter on field X is non-restrictive (e.g., `IN [1..9]` when 1..9 are all valid codes), label as "non-restrictive" in Trust UI or drop from effective filters display
- **Validation**: Executor must detect redundant filters (filtering and grouping on same field) and warn or deduplicate
- **Trust UI display**: Show "Effective filters" after normalization, drop tautologies or label them "non-restrictive"

**ADR003 Mitigations**: This ADR mitigates parser limitations and contract violations through:

1. **Confidence Gating** (Phase 3) - Addresses Violation #1:
   - Hard gate: Refuse execution when `confidence < threshold`
   - No side effects: Do not cache or store results below threshold
   - UI requires explicit confirmation for low-confidence plans

2. **Intent Validation** (Phase 3) - Addresses Violation #2:
   - Refuse COUNT queries with missing grouping when query pattern implies breakdown
   - Force confidence low or downgrade intent to "clarify"
   - UI shows candidate variables for user selection

3. **Run-Key Determinism** (Phase 3) - Addresses Violation #3:
   - Generate deterministic run_key from complete canonical plan
   - UI verifies plan matches before displaying cached results
   - Prevents stale/mismatched results in conversation flow

4. **Breakdown Validation** (Phase 3) - Addresses Violation #4:
   - Refuse queries with `grouping_variable=entity_key` when query implies categorical breakdown
   - Support 1D breakdowns (MVP), clarify 2D breakdowns if not supported
   - UI shows detected breakdown pattern for user confirmation

5. **Filter Deduplication** (Phase 3) - Addresses Violation #5:
   - Detect redundant filters (filtering and grouping on same field)
   - Warn and deduplicate automatically
   - Normalize compound-coded columns to canonical + codebook format

6. **Plan Validation and Refusal** (Phase 3):
   - Validate all `QueryPlan` objects before execution
   - Check columns exist (after alias resolution)
   - Validate operators and type compatibility
   - Refuse invalid plans with clear error messages
   - Log validation failures for debugging

7. **Trust UI with Explicit Plan Inspection** (Phase 1):
   - Show `QueryPlan` (intent, metric, filters) in verification UI
   - Show effective execution (canonical columns, applied filters, cohort size)
   - Allow users to inspect what the parser produced
   - Enable patient-level export for manual verification
   - Display breakdown reporting (filtered vs unfiltered counts)

8. **Alias Persistence to Improve Future Matches** (Phase 2):
   - User teaches system terminology ("VL" â†’ "viral_load")
   - Persisted aliases improve Tier 1/2 matching on subsequent queries
   - Reduces need for Tier 3 LLM fallback over time
   - Scoped per dataset to prevent cross-contamination

**Parser Improvements Out of Scope**: 
- Tier 3 LLM hardening (structured JSON extraction, validation, error handling)
- Enhanced filter extraction patterns
- Multi-part query parsing
- COUNT intent classification improvements

These are ADR001's responsibility. If parser improvements are needed, they should be addressed in a follow-on ADR (e.g., ADR008: NLU Hardening and Structured Parsing) or ADR001 revision.

**Failure Handling Contract**: 
- If ADR001 produces invalid `QueryPlan` (contract violation, missing columns, etc.), ADR003's execution will validate and reject with clear error message
- If ADR001 produces `None` (all tiers failed), ADR003 will return error to UI (don't silently fail)
- **Confidence gating**: If `confidence < threshold`, refuse execution with `requires_confirmation=True` (no side effects)
- **Intent validation**: Refuse COUNT queries with missing grouping when query pattern implies breakdown
- **Breakdown validation**: Refuse queries with `grouping_variable=entity_key` when query implies categorical breakdown
- ADR003 logs all validation failures for debugging and future parser improvements
- **Critical**: ADR003 does NOT infer user expectations from natural language. All validation is against explicit QueryPlan contract fields.

**Run-Key Contract**:
- **Deterministic generation**: `run_key = hash(dataset_version + canonical_plan_json)`
- **Canonical plan includes**: `intent_type`, `primary_variable` (canonical), `grouping_variable` (canonical), `filters` (normalized, sorted), `entity_key`, `scope`
- **UI binding**: Each result panel must be bound to its specific QueryPlan. Before displaying cached results, UI must verify plan matches current query's plan.
- **Validation**: Executor generates run_key from complete plan. UI verifies plan matches before displaying cached results to prevent stale/mismatched outputs.

**Semantic Predicates (Deferred - Not in MVP)**:

For domain concepts like "on statins" that have ambiguous mappings, consider adding a `SemanticPredicate` registry (not NLU, but ontology-lite execution support) in a future ADR. This is not required for MVP - ship only if telemetry shows it's necessary after Phase 1-3 implementation.


## Consequences

**Positive:**
- Clinicians trust results (audit trail with patient-level export)
- 10-second onboarding (no config wall, error-driven alias learning)
- User-taught aliases persist across sessions (per dataset)
- Type-aware execution prevents categorical-as-numeric errors
- COUNT intent uses SQL aggregation (faster, more correct)

**Negative:**
- First-query failures require user teaching (but teaching persists)
- Alias scope per dataset (can't share aliases across datasets - by design)
- Patient-level export requires explicit action (privacy guardrail)

## Alternatives Rejected

**Local LLM (Ollama) - Reconsidered:**
- **Status**: Now viable option for privacy-sensitive clinical data
- **Trade-offs**: Privacy improves (data stays on-device), but latency/quality concerns remain
- **Implementation**: Use LLMClient interface pattern - local vs remote is implementation detail (deferred to ADR001 or follow-on ADR)
- **Model recommendation** (if implementing):
  - **Default**: `llama3.1:8b` (good baseline, ~4.9GB model file, acceptable quality/latency for structured extraction)
  - **Lighter option**: `llama3.2:3b` (smaller, but expect higher failure/retry rates on multi-part + negation queries)
  - **Note**: Model file size â‰  RAM usage (depends on quantization, KV cache, context length, batch size)
  - **JSON compliance**: Not guaranteed by model - still requires schema-constrained decoding, defensive parsing, retries, confidence gating
  - **Evaluation**: Choose model that passes your evaluation harness, not the one with best marketing claims
- **Requirement**: Same hardening patterns apply (structured JSON, validation, timeouts, confidence gating) regardless of local vs remote
- **Decision**: Support local LLM via Ollama if implemented, but keep it optional and gated by feature flag. This is ADR001's responsibility (Tier 3 implementation), not ADR003's.

**Forced Column Mapping:**
- Creates adoption barrier
- Error-driven learning is superior for single-user tool

**Pure Regex Parsing (without alias persistence):**
- Fails on clinical terminology ("VL" vs "viral_load")
- Requires user to remember exact column names
- **Note**: Alias persistence enables user teaching, reducing need for exact terminology

**Pydantic Migration:**
- Current `@dataclass` approach is sufficient and consistent with codebase
- Pydantic would add dependency without clear benefit for MVP
- Can reconsider if validation/JSON schema generation becomes critical

## Code References

- `src/clinical_analytics/core/nl_query_engine.py` - Existing three-tier parsing system
- `src/clinical_analytics/core/semantic.py` - Existing alias index system
- `src/clinical_analytics/ui/pages/3_ðŸ’¬_Ask_Questions.py` - Result rendering (target for verification UI)
- `src/clinical_analytics/ui/components/result_interpreter.py` - Existing result rendering patterns
- `src/clinical_analytics/ui/storage/user_datasets.py` - Metadata persistence (for adaptive dictionary)