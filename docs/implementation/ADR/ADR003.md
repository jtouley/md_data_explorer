# ADR 003: Clinical Trust Protocol + Adaptive Alias Persistence

## Status
Proposed

## Related ADRs

- **[ADR001: Query Plan Producer, Filtering, and Chat-First Execution Rules](../ADR/ADR001.md)**: ADR001 owns QueryPlan production (NLU parsing). ADR003 provides trust UI and alias persistence that ADR001 depends on. ADR001 does not implement trust UI - that's ADR003's responsibility.
- **[ADR002: Persistent Storage Layer](../ADR/ADR002.md)**: ADR002 owns metadata JSON schema and persistence. ADR003's alias mappings are stored in ADR002's metadata schema (per upload_id + dataset_version scope).

## Context
Clinical research tools fail on three axes:
1. **Trust Gap**: Physicians reject aggregate statistics without audit trails
2. **Onboarding Friction**: Configuration before use kills adoption
3. **Alias Resolution Failures**: System can't map user terminology ("VL", "LDL") to column names ("viral_load", "LDL mg/dL") without user teaching

**Note**: This ADR does NOT own NLU parsing (that's ADR001). This ADR owns:
- Trust/verification UI (audit trails, patient-level exports)
- Adaptive alias persistence (user teaches system terminology â†’ persists to metadata)

### Real-World Failure Evidence (2025-12-29)

**Four production queries from clinical dataset ("Statin use - deidentified") demonstrated critical NL query engine failures that directly support the need for ADR003 Phase 2 (Enhanced LLM Parsing) and ADR001 Phase 1 (Data Type Detection).**

#### Failure 1: Multi-Part Query with Wrong Variable Matching

**User Query**:
```
"count number of current and former smokers, and give the breakdown how long since the former smokers stopped smoking"
```

**Expected Behavior**:
- **Primary variable**: `"Nicotine Use"` (column contains: "1. Current", "2. Former", "3. Never")
- **Secondary variable**: `"If Former Smoker Quit"` (column contains time since quitting: "0=N/A", "1. Unknown", "2. <6 mo ago", "3. 6mo-1.5yrs", etc.)
- **Intent**: DESCRIBE with grouping (count by smoking status, then breakdown by quit time for former smokers)
- **Confidence**: Should be high (>0.75) - clear intent with specific variables

**Actual Behavior** (from logs):
- **Matched variables**: `['If Former Smoker Quit ...', 'Statin ADE ...', 'Regimen (if 9 or other)']`
- **Primary variable inferred**: `'If Former Smoker Quit ...'` (wrong - this is secondary variable)
- **Intent**: DESCRIBE
- **Confidence**: `0.3` (fallback threshold - Tier 2 failed, fell back to default)
- **Result**: System analyzed wrong variable, producing incorrect breakdown

**Root Cause Analysis**:
1. **Tier 2 (Semantic Embeddings) failed**: Query didn't match any template with sufficient confidence (>0.7 threshold)
2. **Variable extraction failed**: N-gram matching didn't connect "current and former smokers" to "Nicotine Use" column
3. **Multi-part query not handled**: System couldn't parse compound question (smoking status + time breakdown)
4. **Fell back to default**: `QueryIntent(intent_type="DESCRIBE", confidence=0.3)` (line 654 in `nl_query_engine.py`)

**Why This Matters**: This is exactly the type of clinical query that requires structured understanding - the user wants a two-level breakdown (smoking status, then quit time), but the system treated it as a single variable description.

#### Failure 2: Filter Condition Not Extracted, Wrong Primary Variable

**User Query**:
```
"Average LDL for those patients not on statin?"
```

**Expected Behavior**:
- **Primary variable**: `"LDL mg/dL"` (the metric being averaged)
- **Filter condition**: `{"column": "Statin Prescribed?", "operator": "==", "value": "2: No"}`
- **Intent**: DESCRIBE with filter (compute mean of LDL, filtered to patients not on statin)
- **Confidence**: Should be high (>0.75) - clear intent with filter

**Actual Behavior** (from logs):
- **Matched variables**: `['Age', 'patient_id', 'Statin Used: ...']`
- **Primary variable inferred**: `'Age'` (completely wrong - should be LDL)
- **Filter condition**: Not extracted (empty)
- **Intent**: DESCRIBE
- **Confidence**: `0.3` (fallback threshold)
- **Result**: System analyzed Age instead of LDL, and didn't filter to non-statin patients

**Root Cause Analysis**:
1. **Variable matching failed**: "LDL" didn't match "LDL mg/dL" column (exact substring matching too strict)
2. **Filter extraction not implemented**: No code exists to parse "not on statin" â†’ filter condition
3. **Wrong variable selected**: System defaulted to first matched variable (Age) instead of correct one
4. **Negation not handled**: "not on statin" requires understanding negation + column mapping

**Why This Matters**: This query demonstrates two critical gaps:
- **Filter extraction** (ADR001 Phase 2): System must parse "those patients not on statin" as a filter condition
- **Enhanced parsing** (ADR003 Phase 2): System needs structured LLM extraction to handle negation and map "LDL" â†’ "LDL mg/dL"

#### Failure 3: Categorical Variable Treated as Numeric - Wrong Statistics

**User Query**:
```
"how many patients on statins? which statin most prescribed?"
```

**Expected Behavior**:
- **Primary variable**: `"Statin Prescribed? 1: Yes 2: No"` (categorical - Yes/No)
- **Secondary variable**: `"Statin Used: 1: Atorvastatin 2: Rosuvastatin ..."` (categorical - statin type)
- **Intent**: DESCRIBE with frequency counts (not numeric statistics)
- **Expected output**: 
  - Count: "X patients on statins (Y%)"
  - Breakdown: "Most prescribed: Atorvastatin (Z patients, W%)"
- **Confidence**: Should be high (>0.75) - clear intent for counts/percentages

**Actual Behavior** (from logs and UI):
- **Matched variables**: `['patient_id', 'Statin Prescribed? 1: Yes 2: No', 'Statin Used: ...']`
- **Primary variable inferred**: `'Statin Prescribed? 1: Yes 2: No'` (correct variable)
- **Intent**: DESCRIBE
- **Confidence**: `0.3` (fallback threshold)
- **Result**: System treated categorical variable as numeric, computed:
  - Mean: 1.41 (meaningless - average of 1/2 encoding)
  - Median: 1.00
  - Std Dev: 0.49
  - Min: 1.00, Max: 2.00
- **User expectation**: Simple count/percentage (e.g., "500 patients on statins (62%)")

**Root Cause Analysis**:
1. **Data type detection failed**: System didn't recognize "1: Yes 2: No" as categorical encoding
2. **Query intent not understood**: "how many" clearly indicates count, not mean/median
3. **Numeric statistics computed for categorical**: Mean of 1/2 encoding is meaningless
4. **Multi-part query partially handled**: Matched both variables but only analyzed one, didn't answer "which statin most prescribed"

**Why This Matters**: 
- **Clinical usability**: Physicians want counts/percentages for categorical variables, not numeric statistics
- **Data type awareness**: System must detect categorical variables (even when encoded as 1/2) and provide frequency tables
- **Query intent**: "how many" should trigger count analysis, not mean computation
- **Multi-part queries**: System matched both variables but didn't answer the second part ("which statin most prescribed")

**Related to ADR001 Phase 1**: This also relates to the comparison analysis bug where string numeric columns are incorrectly treated as categorical. Here, the inverse problem: categorical variables are incorrectly treated as numeric.

#### Failure 4: Query Intent Misunderstood - "How Many" Should Return Count, Not Statistics

**User Query**:
```
"how many were on a statin?"
```

**Expected Behavior**:
- **Intent**: COUNT (not DESCRIBE)
- **Action**: Filter to patients on statin (`Statin Prescribed? == "Yes"` OR `Statin Used != 0`)
- **Output**: Simple number: "X patients were on a statin" (or "X patients (Y%)")
- **Confidence**: Should be high (>0.75) - clear count intent

**Actual Behavior** (from logs and UI):
- **Matched variables**: `['Statin Used: 0: n/a 1: Atorvastatin ...', 'ASCVD Risk % before statin']`
- **Primary variable inferred**: `'Statin Used: ...'` (wrong - should filter, not describe)
- **Intent**: DESCRIBE (wrong - should be COUNT)
- **Confidence**: `0.3` (fallback threshold)
- **Result**: System computed descriptive statistics (mean=0.93, median=1.00, std dev=0.98) for "Statin Used" variable
- **User expectation**: Simple count "X patients were on a statin"

**Root Cause Analysis**:
1. **Query intent not understood**: "how many" clearly indicates COUNT intent, but system defaulted to DESCRIBE
2. **No COUNT intent type**: System only has DESCRIBE, COMPARE_GROUPS, FIND_PREDICTORS - no COUNT intent for simple counting queries
3. **Filter not extracted**: Query implies filter ("were on a statin" = filter condition), but system treated it as variable selection
4. **Wrong analysis type**: System computed statistics instead of counting filtered rows

**Why This Matters**:
- **Basic functionality missing**: "How many" is a fundamental query type that should work out-of-the-box
- **User frustration**: Users ask simple questions, get complex statistics they didn't ask for
- **Query intent understanding**: System must distinguish between:
  - "how many X?" â†’ COUNT with filter
  - "describe X" â†’ DESCRIBE with statistics
  - "average X" â†’ DESCRIBE with mean
- **Missing intent type**: System needs COUNT intent type for simple counting queries

**Related to Other Failures**:
- Similar to Failure 3 (categorical treated as numeric), but this is about intent type, not data type
- Demonstrates need for query intent classification beyond current DESCRIBE/COMPARE_GROUPS/FIND_PREDICTORS

#### Common Failure Patterns

Both failures share these patterns:

1. **Tier 2 (Semantic Embeddings) insufficient**: 
   - Confidence 0.3 indicates Tier 2 didn't find good template match
   - Embedding similarity threshold (0.7) too high for clinical queries
   - Template library doesn't cover multi-part or filtered queries

2. **Variable extraction too brittle**:
   - N-gram matching fails on clinical terminology ("LDL" vs "LDL mg/dL")
   - Doesn't handle synonyms or abbreviations
   - No understanding of query context (selects wrong variable when multiple match)

3. **Tier 3 (LLM Fallback) not implemented**:
   - Current stub returns default `DESCRIBE` with 0.3 confidence
   - No structured extraction for complex queries
   - No filter condition parsing

4. **No filter extraction**:
   - "not on statin" condition completely ignored
   - No code path to parse embedded filter conditions
   - ADR001 Phase 2 addresses this, but needs LLM support for negation

5. **Data type detection and query intent mismatch**:
   - Categorical variables (Yes/No encoded as 1/2) treated as numeric
   - "how many" queries trigger mean computation instead of counts
   - System computes meaningless statistics (mean of 1/2 encoding = 1.41)
   - User wants simple counts/percentages, not "cute" numeric statistics

6. **Query intent classification missing**:
   - "how many" queries should trigger COUNT intent, not DESCRIBE
   - System lacks COUNT intent type for simple counting queries
   - "how many X?" should filter and count, not compute statistics
   - Basic functionality missing - users can't ask simple count questions

#### Fixability Analysis: Pattern Matching vs LLM

**Critical Insight**: 3 out of 4 failures can be fixed with improved pattern matching and code logic - LLM is only needed for complex multi-part queries.

| Failure | Fixable Without LLM? | Solution Approach |
|---------|---------------------|-------------------|
| **Failure 1** (Multi-part query) | âŒ **No** - Needs LLM | Complex structure understanding ("count X and breakdown Y") requires semantic parsing |
| **Failure 2** (Filter extraction) | âœ… **Yes** - Pattern matching | Regex patterns: "not on X" â†’ filter, "those patients with X" â†’ filter, "for X" â†’ filter |
| **Failure 3** (Categorical as numeric) | âœ… **Yes** - Code logic | Detect categorical encoding patterns: column names containing "1: Yes 2: No", "0: n/a 1: X", etc. |
| **Failure 4** (Query intent "how many") | âœ… **Yes** - Pattern matching | Regex: `r"how many"` â†’ COUNT intent, `r"count"` â†’ COUNT intent |

**Implementation Strategy**:
1. **Fix Failures 2, 3, 4 first** (pattern matching + code logic) - These are quick wins that address 75% of failures
2. **Then implement LLM for Failure 1** (complex multi-part queries) - Only needed for edge cases

**Pattern Matching Solutions**:

**Failure 2 (Filter Extraction)**:
```python
# Pattern matching for filter extraction
FILTER_PATTERNS = [
    (r"not on (\w+)", lambda m: {"column": find_column(m.group(1)), "operator": "!=", "value": "Yes"}),
    (r"those patients with (\w+)", lambda m: {"column": find_column(m.group(1)), "operator": "==", "value": "Yes"}),
    (r"for those patients not on (\w+)", lambda m: {"column": find_column(m.group(1)), "operator": "==", "value": "No"}),
]
```

**Failure 3 (Categorical Detection)**:
```python
# Detect categorical encoding in column names
def is_categorical_encoded(column_name: str) -> bool:
    patterns = [
        r"\d+:\s*\w+",  # "1: Yes 2: No"
        r"0:\s*n/a",    # "0: n/a 1: X"
    ]
    return any(re.search(p, column_name) for p in patterns)
```

**Failure 4 (Query Intent)**:
```python
# Pattern matching for COUNT intent
COUNT_PATTERNS = [
    r"how many",
    r"count",
    r"number of",
]
if any(re.search(p, query.lower()) for p in COUNT_PATTERNS):
    intent = "COUNT"
```

**Rationale**: Don't over-engineer with LLM when pattern matching solves 75% of failures. LLM should be reserved for complex queries that pattern matching cannot handle.

#### Semantic Layer Responsibilities (Executor, Not Parser)

**Critical Architectural Clarification**: The semantic layer is the **executor and validator** of QueryPlans, not the producer. It cannot parse natural language - that's the NLU layer's job (ADR001).

**What Semantic Layer CAN Do** (Executor Role):
- **Validate QueryPlan**: Check columns exist, operators are valid, types match
- **Normalize aliases**: Resolve "VL" â†’ "viral_load" using persisted alias mappings (from ADR003)
- **Type-aware aggregation**: 
  - Categorical columns â†’ frequency tables (not mean/median)
  - Numeric columns â†’ descriptive statistics (mean, median, std dev)
  - Detects categorical encoding patterns ("1: Yes 2: No") and treats as categorical
- **Execute QueryPlan**: 
  - `QueryPlan(intent="COUNT", filters=[...])` â†’ `query(metrics=["count"], filters={...})`
  - `QueryPlan(intent="DESCRIBE", metric="X")` â†’ `query(metrics=["avg_X"])` or frequency table if categorical
  - `QueryPlan(intent="COMPARE_GROUPS", metric="X", group_by="Y")` â†’ Grouped aggregation
- **Safe query compilation**: DuckDB/Ibis SQL generation with validation

**What Semantic Layer CANNOT Do** (Not Parser):
- âŒ Parse natural language intent ("how many" â†’ COUNT)
- âŒ Infer multi-step plans from one sentence
- âŒ Handle negation logic reliably without structured input ("not on statin" requires NLU to map to column + value)
- âŒ Extract filters from query text (that's NLU's job)

**Architectural Split**:
- **NLU Layer (ADR001)**: Parses text â†’ produces `QueryPlan`
- **Semantic Layer (ADR003)**: Validates `QueryPlan` â†’ executes via SQL/Ibis â†’ returns results

**Impact on Failures**:
- Failures 2, 3, 4 are NLU parsing failures (ADR001's responsibility)
- Semantic layer can help with Failure 3 (categorical detection) and Failure 4 (COUNT execution) once NLU produces correct QueryPlan
- Failure 1 (multi-part queries) requires NLU enhancement (ADR001), not semantic layer changes

## Decision

**Scope Clarification**: This ADR owns trust UI and alias persistence. It does NOT own NLU parsing (that's ADR001). The semantic layer executes QueryPlans produced by ADR001's NLU layer.

### 1. Trust Protocol: Mandatory Verification UI
Every result includes patient-level audit:

```python
st.info(f"Average Viral Load: {mean_val}")

with st.expander("ðŸ”Ž Verify: Show source patients"):
    audit_cols = ["patient_id", primary_variable]
    if filters:
        audit_cols.extend(f.column for f in filters)
    
    st.dataframe(result_df.select(audit_cols).head(100))
    st.download_button("Download cohort CSV", result_df.write_csv())
```

**Rationale**: Clinicians must validate AI reasoning. Excel export enables external verification.

### 2. Adaptive Alias Persistence: Error-Driven Learning

**Note**: This extends the existing alias index system in `SemanticLayer` (`_build_alias_index()`, `_normalize_alias()`), not replaces it. The codebase already has:
- Column name normalization âœ…
- Fuzzy matching âœ…
- Collision detection âœ…

**Action**: Add persistence layer for user-added aliases (extend, don't replace).

**Alias Scope Rules** (per ADR002 metadata schema):
- Aliases scoped to `(upload_id, dataset_version)` - not global
- If schema changes and target column missing: mark alias orphaned and ignore
- Prevents "poison" aliases from breaking other datasets

**Flow:**
```python
def handle_unknown_term(term: str, available_columns: list[str], semantic_layer: SemanticLayer, upload_id: str, dataset_version: str):
    st.error(f"Unknown term: '{term}'")
    
    with st.expander("ðŸ’¡ Add alias?"):
        col = st.selectbox("Map to column:", available_columns)
        if st.button("Save"):
            # Extend existing alias index with user mapping
            semantic_layer.add_user_alias(term, col)
            # Persist to dataset metadata JSON (per ADR002) - scoped to upload_id + dataset_version
            save_alias_mapping(upload_id, dataset_version, term, col)
            st.success("Retry your query")
```

**Persistence** (per ADR002):
- Store in `metadata["alias_mappings"]` dict in dataset metadata JSON
- Format: `{"user_aliases": {"VL": "viral_load", "LDL": "LDL mg/dL"}, "system_aliases": {...}}`
- Loaded on semantic layer initialization, merged with system aliases

### 3. Semantic Layer QueryPlan Execution Contract

**Note**: This ADR does NOT own QueryPlan production (that's ADR001). This section defines how the semantic layer executes QueryPlans produced by ADR001's NLU layer.

**Semantic Layer Execution Contract**:

The semantic layer receives a `QueryPlan` (from ADR001) and must:

1. **Validate QueryPlan**:
   - Check all columns exist (after alias resolution)
   - Validate operators are supported
   - Check type compatibility (e.g., can't use ">" on categorical)

2. **Normalize Aliases** (including user-added aliases):
   - Resolve "VL" â†’ "viral_load" using persisted alias mappings
   - Handle collisions (multiple columns match same alias)
   - Use canonical column names for execution

3. **Type-Aware Execution**:
   - **Categorical columns**: Detect encoding patterns ("1: Yes 2: No") â†’ return frequency tables, not mean/median
   - **Numeric columns**: Return descriptive statistics (mean, median, std dev)
   - **COUNT intent**: Use `query(metrics=["count"], filters={...})` for SQL aggregation
   - **DESCRIBE intent**: Use `query(metrics=["avg_X"])` for numeric, frequency table for categorical

4. **Refuse Invalid Plans**:
   - Unknown columns â†’ raise `ValueError` with suggestions
   - Invalid operators â†’ raise `ValueError` with supported operators
   - Type mismatches â†’ raise `TypeError` with explanation

**Example Execution**:
```python
# QueryPlan from ADR001 NLU layer:
plan = QueryPlan(
    intent="COUNT",
    filters=[FilterSpec(column="Statin Prescribed?", operator="==", value="Yes")],
    confidence=0.9
)

# Semantic layer executes:
result = semantic_layer.query(
    metrics=["count"],
    filters={"Statin Prescribed?": "Yes"}  # Converted from FilterSpec
)
# Returns: {"count": 500} (not mean/median/std dev)
```

## Implementation

### Existing Infrastructure

1. **Query Parsing**: Three-tier system already exists (`NLQueryEngine` in `src/clinical_analytics/core/nl_query_engine.py`):
   - Tier 1: Pattern matching (regex) âœ…
   - Tier 2: Semantic embeddings âœ…
   - Tier 3: LLM fallback (stub) âš ï¸
   
   **Action**: Enhance Tier 3 LLM fallback rather than replacing entire system.

2. **Alias System**: Semantic layer already has alias index (`SemanticLayer._build_alias_index()` in `src/clinical_analytics/core/semantic.py`):
   - Column name normalization âœ…
   - Fuzzy matching âœ…
   - Collision detection âœ…
   
   **Action**: Add persistence layer for user-added aliases (extend, don't replace).

3. **QueryIntent Structure**: Currently uses `@dataclass` (not Pydantic) in `src/clinical_analytics/core/nl_query_engine.py`:
   - `filters: dict[str, Any]` (should be `list[dict[str, Any]]` per ADR001)
   - Consider keeping dataclass for consistency (or justify Pydantic migration if validation/JSON schema benefits needed)

### Implementation Plan

**Phase 1: Trust Layer (P0 - Mandatory Verification UI)**
- File: `src/clinical_analytics/ui/pages/3_ðŸ’¬_Ask_Questions.py`
- Add verification expander to result rendering functions (`_render_focused_descriptive()`, `render_comparison_analysis()`, etc.)
- Show QueryPlan (intent, metric, filters) in verification UI
- Add patient-level export with cohort size + run_id
- Coordinate with ADR001 Phase 2.4 (breakdown reporting)
- Reference: `src/clinical_analytics/ui/components/result_interpreter.py` for existing result rendering patterns

**Phase 2: Adaptive Alias Persistence (P0 - Error-Driven Learning)**
- File: `src/clinical_analytics/core/semantic.py` (extend `SemanticLayer`)
- Add `add_user_alias(term: str, column: str, upload_id: str, dataset_version: str)` method
- Persist to dataset metadata JSON (per ADR002 metadata schema)
- Load aliases on semantic layer initialization
- Handle orphaned aliases (column missing after schema change)
- UI: Add "Add alias?" expander in error handling (coordinate with ADR001 NLU layer)

**Phase 3: Semantic Layer QueryPlan Execution (P1 - Type-Aware Execution)**
- File: `src/clinical_analytics/core/semantic.py` (extend `SemanticLayer.query()`)
- Add `execute_query_plan(plan: QueryPlan) -> dict[str, Any]` method
- Validate QueryPlan (columns exist, operators valid, types match)
- Type-aware aggregation (categorical â†’ frequency, numeric â†’ stats)
- COUNT intent â†’ use `query(metrics=["count"], filters={...})`
- Refuse invalid plans with clear error messages

**Note**: NLU parsing (QueryPlan production) is ADR001's responsibility. This ADR only handles execution and validation.


## Consequences

**Positive:**
- Clinicians trust results (audit trail with patient-level export)
- 10-second onboarding (no config wall, error-driven alias learning)
- User-taught aliases persist across sessions (per dataset)
- Type-aware execution prevents categorical-as-numeric errors
- COUNT intent uses SQL aggregation (faster, more correct)

**Negative:**
- First-query failures require user teaching (but teaching persists)
- Alias scope per dataset (can't share aliases across datasets - by design)
- Patient-level export requires explicit action (privacy guardrail)

## Alternatives Rejected

**Local LLM (Ollama):**
- Deferred to V2 (deployment complexity, lower JSON accuracy)
- Reconsider if cloud parsing becomes regulatory blocker

**Forced Column Mapping:**
- Creates adoption barrier
- Error-driven learning is superior for single-user tool

**Pure Regex Parsing (without alias persistence):**
- Fails on clinical terminology ("VL" vs "viral_load")
- Requires user to remember exact column names
- **Note**: Alias persistence enables user teaching, reducing need for exact terminology

**Pydantic Migration:**
- Current `@dataclass` approach is sufficient and consistent with codebase
- Pydantic would add dependency without clear benefit for MVP
- Can reconsider if validation/JSON schema generation becomes critical

## Code References

- `src/clinical_analytics/core/nl_query_engine.py` - Existing three-tier parsing system
- `src/clinical_analytics/core/semantic.py` - Existing alias index system
- `src/clinical_analytics/ui/pages/3_ðŸ’¬_Ask_Questions.py` - Result rendering (target for verification UI)
- `src/clinical_analytics/ui/components/result_interpreter.py` - Existing result rendering patterns
- `src/clinical_analytics/ui/storage/user_datasets.py` - Metadata persistence (for adaptive dictionary)