# ADR 003: Clinical Trust Architecture & Adaptive Terminology (Revised)

## Status
Proposed

## Related ADRs

- **[ADR001: Fix Comparison Analysis, Implement Filtering, and Conversational UI](../ADR/ADR001.md)**: Phase 2 (Filter Parsing) overlaps with this ADR's structured parsing goals. Coordinate implementation to avoid duplication. Filter structure should use `list[dict[str, Any]]` for consistency.
- **[ADR002: Persistent Storage Layer](../ADR/ADR002.md)**: Persistent storage layer affects how verification data and adaptive dictionary mappings are persisted. Adaptive dictionary mappings should be stored in dataset metadata JSON.

## Context
Clinical research tools fail on three axes:
1. **Trust Gap**: Physicians reject aggregate statistics without audit trails
2. **Onboarding Friction**: Configuration before use kills adoption
3. **Parse Failures**: Regex breaks on negation ("without diabetes") and synonyms ("VL" vs "viral_load")

## Decision

### 1. Trust Protocol: Mandatory Verification UI
Every result includes patient-level audit:

```python
st.info(f"Average Viral Load: {mean_val}")

with st.expander("üîé Verify: Show source patients"):
    audit_cols = ["patient_id", primary_variable]
    if filters:
        audit_cols.extend(f.column for f in filters)
    
    st.dataframe(result_df.select(audit_cols).head(100))
    st.download_button("Download cohort CSV", result_df.write_csv())
```

**Rationale**: Clinicians must validate AI reasoning. Excel export enables external verification.

### 2. Adaptive Dictionary: Error-Driven Learning
**Note**: This extends the existing alias index system in `SemanticLayer` (`_build_alias_index()`, `_normalize_alias()`), not replaces it. The codebase already has:
- Column name normalization ‚úÖ
- Fuzzy matching ‚úÖ
- Collision detection ‚úÖ

**Action**: Add persistence layer for user-added aliases (extend, don't replace).

- Upload ‚Üí query immediately (no configuration gate)
- On parse failure: "Did you mean 'viral_load'?" ‚Üí user confirms ‚Üí persists to metadata JSON
- Optional settings panel for bulk editing

**Flow:**
```python
def handle_unknown_term(term: str, available_columns: list[str], semantic_layer: SemanticLayer):
    st.error(f"Unknown term: '{term}'")
    
    with st.expander("üí° Add alias?"):
        col = st.selectbox("Map to column:", available_columns)
        if st.button("Save"):
            # Extend existing alias index with user mapping
            semantic_layer.add_user_alias(term, col)
            # Persist to dataset metadata JSON (per ADR002)
            semantic_layer.save_alias_mapping(term, col)
            st.success("Retry your query")
```

### 3. Enhanced LLM Parsing: Structured Extraction Over Pure Regex
**Note**: This extends the existing three-tier parsing system (`NLQueryEngine`), not replaces it. The codebase already has:
- Tier 1: Pattern matching (regex) ‚úÖ
- Tier 2: Semantic embeddings ‚úÖ
- Tier 3: LLM fallback (stub) ‚ö†Ô∏è

**Action**: Enhance Tier 3 LLM fallback with structured extraction for complex queries.

**Filter Structure** (aligned with ADR001):
```python
# Use dict structure for consistency with existing QueryIntent and ADR001
filters: list[dict[str, Any]] = [
    {
        "column": "viral_load",
        "operator": "<",
        "value": 20,
        "negation": False  # "patients WITHOUT fever" ‚Üí negation=True
    }
]
```

**Enhanced LLM Call** (extends existing `_llm_parse()` method):
```python
# Only sends schema/column names, never patient data
# Uses existing QueryIntent dataclass structure
response = client.messages.create(
    model="claude-sonnet-4-20250514",
    messages=[{
        "role": "user",
        "content": f"""
        Available columns: {available_columns}
        Query: {user_query}
        
        Extract:
        1. Intent type (DESCRIBE, COMPARE_GROUPS, FIND_PREDICTORS, etc.)
        2. Primary variable
        3. Grouping variable (if applicable)
        4. Filter conditions (list of dicts with column, operator, value, negation)
        
        Return JSON matching QueryIntent structure.
        """
    }]
)
```

## Implementation

### Existing Infrastructure

1. **Query Parsing**: Three-tier system already exists (`NLQueryEngine` in `src/clinical_analytics/core/nl_query_engine.py`):
   - Tier 1: Pattern matching (regex) ‚úÖ
   - Tier 2: Semantic embeddings ‚úÖ
   - Tier 3: LLM fallback (stub) ‚ö†Ô∏è
   
   **Action**: Enhance Tier 3 LLM fallback rather than replacing entire system.

2. **Alias System**: Semantic layer already has alias index (`SemanticLayer._build_alias_index()` in `src/clinical_analytics/core/semantic.py`):
   - Column name normalization ‚úÖ
   - Fuzzy matching ‚úÖ
   - Collision detection ‚úÖ
   
   **Action**: Add persistence layer for user-added aliases (extend, don't replace).

3. **QueryIntent Structure**: Currently uses `@dataclass` (not Pydantic) in `src/clinical_analytics/core/nl_query_engine.py`:
   - `filters: dict[str, Any]` (should be `list[dict[str, Any]]` per ADR001)
   - Consider keeping dataclass for consistency (or justify Pydantic migration if validation/JSON schema benefits needed)

### Implementation Plan

**Phase 1: Trust Layer** (New - No conflicts)
- File: `src/clinical_analytics/ui/pages/3_üí¨_Ask_Questions.py`
- Add verification expander to result rendering functions (`_render_focused_descriptive()`, `render_comparison_analysis()`, etc.)
- Coordinate with ADR001 Phase 2.4 (breakdown reporting)
- Reference: `src/clinical_analytics/ui/components/result_interpreter.py` for existing result rendering patterns

**Phase 2: Enhanced LLM Parser** (Extends existing)
- File: `src/clinical_analytics/core/nl_query_engine.py`
- Enhance `_llm_parse()` method (currently stub at line ~636)
- Use structured output with existing `QueryIntent` dataclass
- Coordinate with ADR001 Phase 2.1 (filter extraction)
- Update `QueryIntent.filters` to `list[dict[str, Any]]` for consistency with ADR001

**Phase 3: Adaptive Dictionary Persistence** (Extends existing)
- File: `src/clinical_analytics/core/semantic.py`
- Add `add_user_alias()` method to extend existing alias index
- Add `save_alias_mapping()` method to persist user mappings
- Store in dataset metadata JSON (per ADR002 persistence layer)
- Add error recovery UI in `src/clinical_analytics/ui/pages/3_üí¨_Ask_Questions.py`

## Consequences

**Positive:**
- Clinicians trust results (audit trail)
- 10-second onboarding (no config wall)
- Handles clinical language ("no prior treatment", "VL <20")

**Negative:**
- API dependency (~$0.01/query, 800ms latency)
- First-query failures require user teaching
- Privacy constraint: only metadata to LLM, never patient rows

## Alternatives Rejected

**Local LLM (Ollama):**
- Deferred to V2 (deployment complexity, lower JSON accuracy)
- Reconsider if cloud parsing becomes regulatory blocker

**Forced Column Mapping:**
- Creates adoption barrier
- Error-driven learning is superior for single-user tool

**Pure Regex Parsing (without LLM enhancement):**
- Fails on negation, synonyms, compound conditions
- Clinical language too complex for pattern matching
- **Note**: We keep regex for Tier 1 (fast path) but enhance with LLM for complex cases

**Pydantic Migration:**
- Current `@dataclass` approach is sufficient and consistent with codebase
- Pydantic would add dependency without clear benefit for MVP
- Can reconsider if validation/JSON schema generation becomes critical

## Code References

- `src/clinical_analytics/core/nl_query_engine.py` - Existing three-tier parsing system
- `src/clinical_analytics/core/semantic.py` - Existing alias index system
- `src/clinical_analytics/ui/pages/3_üí¨_Ask_Questions.py` - Result rendering (target for verification UI)
- `src/clinical_analytics/ui/components/result_interpreter.py` - Existing result rendering patterns
- `src/clinical_analytics/ui/storage/user_datasets.py` - Metadata persistence (for adaptive dictionary)