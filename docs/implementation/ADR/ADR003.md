# ADR 003: Clinical Trust Architecture & Adaptive Terminology (Revised)

## Status
Proposed

## Related ADRs

- **[ADR001: Fix Comparison Analysis, Implement Filtering, and Conversational UI](../ADR/ADR001.md)**: Phase 2 (Filter Parsing) overlaps with this ADR's structured parsing goals. Coordinate implementation to avoid duplication. Filter structure should use `list[dict[str, Any]]` for consistency.
- **[ADR002: Persistent Storage Layer](../ADR/ADR002.md)**: Persistent storage layer affects how verification data and adaptive dictionary mappings are persisted. Adaptive dictionary mappings should be stored in dataset metadata JSON.

## Context
Clinical research tools fail on three axes:
1. **Trust Gap**: Physicians reject aggregate statistics without audit trails
2. **Onboarding Friction**: Configuration before use kills adoption
3. **Parse Failures**: Regex breaks on negation ("without diabetes") and synonyms ("VL" vs "viral_load")

### Real-World Failure Evidence (2025-12-29)

**Four production queries from clinical dataset ("Statin use - deidentified") demonstrated critical NL query engine failures that directly support the need for ADR003 Phase 2 (Enhanced LLM Parsing) and ADR001 Phase 1 (Data Type Detection).**

#### Failure 1: Multi-Part Query with Wrong Variable Matching

**User Query**:
```
"count number of current and former smokers, and give the breakdown how long since the former smokers stopped smoking"
```

**Expected Behavior**:
- **Primary variable**: `"Nicotine Use"` (column contains: "1. Current", "2. Former", "3. Never")
- **Secondary variable**: `"If Former Smoker Quit"` (column contains time since quitting: "0=N/A", "1. Unknown", "2. <6 mo ago", "3. 6mo-1.5yrs", etc.)
- **Intent**: DESCRIBE with grouping (count by smoking status, then breakdown by quit time for former smokers)
- **Confidence**: Should be high (>0.75) - clear intent with specific variables

**Actual Behavior** (from logs):
- **Matched variables**: `['If Former Smoker Quit ...', 'Statin ADE ...', 'Regimen (if 9 or other)']`
- **Primary variable inferred**: `'If Former Smoker Quit ...'` (wrong - this is secondary variable)
- **Intent**: DESCRIBE
- **Confidence**: `0.3` (fallback threshold - Tier 2 failed, fell back to default)
- **Result**: System analyzed wrong variable, producing incorrect breakdown

**Root Cause Analysis**:
1. **Tier 2 (Semantic Embeddings) failed**: Query didn't match any template with sufficient confidence (>0.7 threshold)
2. **Variable extraction failed**: N-gram matching didn't connect "current and former smokers" to "Nicotine Use" column
3. **Multi-part query not handled**: System couldn't parse compound question (smoking status + time breakdown)
4. **Fell back to default**: `QueryIntent(intent_type="DESCRIBE", confidence=0.3)` (line 654 in `nl_query_engine.py`)

**Why This Matters**: This is exactly the type of clinical query that requires structured understanding - the user wants a two-level breakdown (smoking status, then quit time), but the system treated it as a single variable description.

#### Failure 2: Filter Condition Not Extracted, Wrong Primary Variable

**User Query**:
```
"Average LDL for those patients not on statin?"
```

**Expected Behavior**:
- **Primary variable**: `"LDL mg/dL"` (the metric being averaged)
- **Filter condition**: `{"column": "Statin Prescribed?", "operator": "==", "value": "2: No"}`
- **Intent**: DESCRIBE with filter (compute mean of LDL, filtered to patients not on statin)
- **Confidence**: Should be high (>0.75) - clear intent with filter

**Actual Behavior** (from logs):
- **Matched variables**: `['Age', 'patient_id', 'Statin Used: ...']`
- **Primary variable inferred**: `'Age'` (completely wrong - should be LDL)
- **Filter condition**: Not extracted (empty)
- **Intent**: DESCRIBE
- **Confidence**: `0.3` (fallback threshold)
- **Result**: System analyzed Age instead of LDL, and didn't filter to non-statin patients

**Root Cause Analysis**:
1. **Variable matching failed**: "LDL" didn't match "LDL mg/dL" column (exact substring matching too strict)
2. **Filter extraction not implemented**: No code exists to parse "not on statin" ‚Üí filter condition
3. **Wrong variable selected**: System defaulted to first matched variable (Age) instead of correct one
4. **Negation not handled**: "not on statin" requires understanding negation + column mapping

**Why This Matters**: This query demonstrates two critical gaps:
- **Filter extraction** (ADR001 Phase 2): System must parse "those patients not on statin" as a filter condition
- **Enhanced parsing** (ADR003 Phase 2): System needs structured LLM extraction to handle negation and map "LDL" ‚Üí "LDL mg/dL"

#### Failure 3: Categorical Variable Treated as Numeric - Wrong Statistics

**User Query**:
```
"how many patients on statins? which statin most prescribed?"
```

**Expected Behavior**:
- **Primary variable**: `"Statin Prescribed? 1: Yes 2: No"` (categorical - Yes/No)
- **Secondary variable**: `"Statin Used: 1: Atorvastatin 2: Rosuvastatin ..."` (categorical - statin type)
- **Intent**: DESCRIBE with frequency counts (not numeric statistics)
- **Expected output**: 
  - Count: "X patients on statins (Y%)"
  - Breakdown: "Most prescribed: Atorvastatin (Z patients, W%)"
- **Confidence**: Should be high (>0.75) - clear intent for counts/percentages

**Actual Behavior** (from logs and UI):
- **Matched variables**: `['patient_id', 'Statin Prescribed? 1: Yes 2: No', 'Statin Used: ...']`
- **Primary variable inferred**: `'Statin Prescribed? 1: Yes 2: No'` (correct variable)
- **Intent**: DESCRIBE
- **Confidence**: `0.3` (fallback threshold)
- **Result**: System treated categorical variable as numeric, computed:
  - Mean: 1.41 (meaningless - average of 1/2 encoding)
  - Median: 1.00
  - Std Dev: 0.49
  - Min: 1.00, Max: 2.00
- **User expectation**: Simple count/percentage (e.g., "500 patients on statins (62%)")

**Root Cause Analysis**:
1. **Data type detection failed**: System didn't recognize "1: Yes 2: No" as categorical encoding
2. **Query intent not understood**: "how many" clearly indicates count, not mean/median
3. **Numeric statistics computed for categorical**: Mean of 1/2 encoding is meaningless
4. **Multi-part query partially handled**: Matched both variables but only analyzed one, didn't answer "which statin most prescribed"

**Why This Matters**: 
- **Clinical usability**: Physicians want counts/percentages for categorical variables, not numeric statistics
- **Data type awareness**: System must detect categorical variables (even when encoded as 1/2) and provide frequency tables
- **Query intent**: "how many" should trigger count analysis, not mean computation
- **Multi-part queries**: System matched both variables but didn't answer the second part ("which statin most prescribed")

**Related to ADR001 Phase 1**: This also relates to the comparison analysis bug where string numeric columns are incorrectly treated as categorical. Here, the inverse problem: categorical variables are incorrectly treated as numeric.

#### Failure 4: Query Intent Misunderstood - "How Many" Should Return Count, Not Statistics

**User Query**:
```
"how many were on a statin?"
```

**Expected Behavior**:
- **Intent**: COUNT (not DESCRIBE)
- **Action**: Filter to patients on statin (`Statin Prescribed? == "Yes"` OR `Statin Used != 0`)
- **Output**: Simple number: "X patients were on a statin" (or "X patients (Y%)")
- **Confidence**: Should be high (>0.75) - clear count intent

**Actual Behavior** (from logs and UI):
- **Matched variables**: `['Statin Used: 0: n/a 1: Atorvastatin ...', 'ASCVD Risk % before statin']`
- **Primary variable inferred**: `'Statin Used: ...'` (wrong - should filter, not describe)
- **Intent**: DESCRIBE (wrong - should be COUNT)
- **Confidence**: `0.3` (fallback threshold)
- **Result**: System computed descriptive statistics (mean=0.93, median=1.00, std dev=0.98) for "Statin Used" variable
- **User expectation**: Simple count "X patients were on a statin"

**Root Cause Analysis**:
1. **Query intent not understood**: "how many" clearly indicates COUNT intent, but system defaulted to DESCRIBE
2. **No COUNT intent type**: System only has DESCRIBE, COMPARE_GROUPS, FIND_PREDICTORS - no COUNT intent for simple counting queries
3. **Filter not extracted**: Query implies filter ("were on a statin" = filter condition), but system treated it as variable selection
4. **Wrong analysis type**: System computed statistics instead of counting filtered rows

**Why This Matters**:
- **Basic functionality missing**: "How many" is a fundamental query type that should work out-of-the-box
- **User frustration**: Users ask simple questions, get complex statistics they didn't ask for
- **Query intent understanding**: System must distinguish between:
  - "how many X?" ‚Üí COUNT with filter
  - "describe X" ‚Üí DESCRIBE with statistics
  - "average X" ‚Üí DESCRIBE with mean
- **Missing intent type**: System needs COUNT intent type for simple counting queries

**Related to Other Failures**:
- Similar to Failure 3 (categorical treated as numeric), but this is about intent type, not data type
- Demonstrates need for query intent classification beyond current DESCRIBE/COMPARE_GROUPS/FIND_PREDICTORS

#### Common Failure Patterns

Both failures share these patterns:

1. **Tier 2 (Semantic Embeddings) insufficient**: 
   - Confidence 0.3 indicates Tier 2 didn't find good template match
   - Embedding similarity threshold (0.7) too high for clinical queries
   - Template library doesn't cover multi-part or filtered queries

2. **Variable extraction too brittle**:
   - N-gram matching fails on clinical terminology ("LDL" vs "LDL mg/dL")
   - Doesn't handle synonyms or abbreviations
   - No understanding of query context (selects wrong variable when multiple match)

3. **Tier 3 (LLM Fallback) not implemented**:
   - Current stub returns default `DESCRIBE` with 0.3 confidence
   - No structured extraction for complex queries
   - No filter condition parsing

4. **No filter extraction**:
   - "not on statin" condition completely ignored
   - No code path to parse embedded filter conditions
   - ADR001 Phase 2 addresses this, but needs LLM support for negation

5. **Data type detection and query intent mismatch**:
   - Categorical variables (Yes/No encoded as 1/2) treated as numeric
   - "how many" queries trigger mean computation instead of counts
   - System computes meaningless statistics (mean of 1/2 encoding = 1.41)
   - User wants simple counts/percentages, not "cute" numeric statistics

6. **Query intent classification missing**:
   - "how many" queries should trigger COUNT intent, not DESCRIBE
   - System lacks COUNT intent type for simple counting queries
   - "how many X?" should filter and count, not compute statistics
   - Basic functionality missing - users can't ask simple count questions

#### Evidence for ADR003 Phase 2 (Enhanced LLM Parsing)

These failures directly demonstrate why Tier 3 LLM fallback with structured extraction is necessary:

- **Multi-part queries**: "count X and breakdown Y" requires understanding query structure
- **Filter conditions**: "for those patients not on statin" requires parsing negation + column mapping
- **Clinical terminology**: "LDL" ‚Üí "LDL mg/dL" requires semantic understanding, not just string matching
- **Context-aware variable selection**: When multiple variables match, system needs to understand query intent to select correct one
- **Query intent + data type awareness**: "how many" queries on categorical variables should trigger count analysis, not mean computation. System must detect categorical variables (even when encoded as 1/2) and provide appropriate statistics (frequencies, not means)
- **Query intent classification**: "how many X?" should trigger COUNT intent (filter + count), not DESCRIBE intent (statistics). System needs COUNT intent type for basic counting queries

**Implementation Priority**: These real-world failures show that Tier 3 LLM parsing is not optional - it's required for clinical usability. Tier 2 (embeddings) alone cannot handle these query patterns.

## Decision

### 1. Trust Protocol: Mandatory Verification UI
Every result includes patient-level audit:

```python
st.info(f"Average Viral Load: {mean_val}")

with st.expander("üîé Verify: Show source patients"):
    audit_cols = ["patient_id", primary_variable]
    if filters:
        audit_cols.extend(f.column for f in filters)
    
    st.dataframe(result_df.select(audit_cols).head(100))
    st.download_button("Download cohort CSV", result_df.write_csv())
```

**Rationale**: Clinicians must validate AI reasoning. Excel export enables external verification.

### 2. Adaptive Dictionary: Error-Driven Learning
**Note**: This extends the existing alias index system in `SemanticLayer` (`_build_alias_index()`, `_normalize_alias()`), not replaces it. The codebase already has:
- Column name normalization ‚úÖ
- Fuzzy matching ‚úÖ
- Collision detection ‚úÖ

**Action**: Add persistence layer for user-added aliases (extend, don't replace).

- Upload ‚Üí query immediately (no configuration gate)
- On parse failure: "Did you mean 'viral_load'?" ‚Üí user confirms ‚Üí persists to metadata JSON
- Optional settings panel for bulk editing

**Flow:**
```python
def handle_unknown_term(term: str, available_columns: list[str], semantic_layer: SemanticLayer):
    st.error(f"Unknown term: '{term}'")
    
    with st.expander("üí° Add alias?"):
        col = st.selectbox("Map to column:", available_columns)
        if st.button("Save"):
            # Extend existing alias index with user mapping
            semantic_layer.add_user_alias(term, col)
            # Persist to dataset metadata JSON (per ADR002)
            semantic_layer.save_alias_mapping(term, col)
            st.success("Retry your query")
```

### 3. Enhanced LLM Parsing: Structured Extraction Over Pure Regex
**Note**: This extends the existing three-tier parsing system (`NLQueryEngine`), not replaces it. The codebase already has:
- Tier 1: Pattern matching (regex) ‚úÖ
- Tier 2: Semantic embeddings ‚úÖ
- Tier 3: LLM fallback (stub) ‚ö†Ô∏è

**Action**: Enhance Tier 3 LLM fallback with structured extraction for complex queries.

**Filter Structure** (aligned with ADR001):
```python
# Use dict structure for consistency with existing QueryIntent and ADR001
filters: list[dict[str, Any]] = [
    {
        "column": "viral_load",
        "operator": "<",
        "value": 20,
        "negation": False  # "patients WITHOUT fever" ‚Üí negation=True
    }
]
```

**Enhanced LLM Call** (extends existing `_llm_parse()` method):
```python
# Only sends schema/column names, never patient data
# Uses existing QueryIntent dataclass structure
response = client.messages.create(
    model="claude-sonnet-4-20250514",
    messages=[{
        "role": "user",
        "content": f"""
        Available columns: {available_columns}
        Query: {user_query}
        
        Extract:
        1. Intent type (DESCRIBE, COMPARE_GROUPS, FIND_PREDICTORS, COUNT, etc.)
        2. Primary variable
        3. Grouping variable (if applicable)
        4. Filter conditions (list of dicts with column, operator, value, negation)
        
        Note: COUNT intent for "how many X?" queries - should filter and count, not compute statistics.
        
        Return JSON matching QueryIntent structure.
        """
    }]
)
```

## Implementation

### Existing Infrastructure

1. **Query Parsing**: Three-tier system already exists (`NLQueryEngine` in `src/clinical_analytics/core/nl_query_engine.py`):
   - Tier 1: Pattern matching (regex) ‚úÖ
   - Tier 2: Semantic embeddings ‚úÖ
   - Tier 3: LLM fallback (stub) ‚ö†Ô∏è
   
   **Action**: Enhance Tier 3 LLM fallback rather than replacing entire system.

2. **Alias System**: Semantic layer already has alias index (`SemanticLayer._build_alias_index()` in `src/clinical_analytics/core/semantic.py`):
   - Column name normalization ‚úÖ
   - Fuzzy matching ‚úÖ
   - Collision detection ‚úÖ
   
   **Action**: Add persistence layer for user-added aliases (extend, don't replace).

3. **QueryIntent Structure**: Currently uses `@dataclass` (not Pydantic) in `src/clinical_analytics/core/nl_query_engine.py`:
   - `filters: dict[str, Any]` (should be `list[dict[str, Any]]` per ADR001)
   - Consider keeping dataclass for consistency (or justify Pydantic migration if validation/JSON schema benefits needed)

### Implementation Plan

**Phase 1: Trust Layer** (New - No conflicts)
- File: `src/clinical_analytics/ui/pages/3_üí¨_Ask_Questions.py`
- Add verification expander to result rendering functions (`_render_focused_descriptive()`, `render_comparison_analysis()`, etc.)
- Coordinate with ADR001 Phase 2.4 (breakdown reporting)
- Reference: `src/clinical_analytics/ui/components/result_interpreter.py` for existing result rendering patterns

**Phase 2: Enhanced LLM Parser** (Extends existing)
- File: `src/clinical_analytics/core/nl_query_engine.py`
- Enhance `_llm_parse()` method (currently stub at line ~654)
- Use structured output with existing `QueryIntent` dataclass
- Coordinate with ADR001 Phase 2.1 (filter extraction)
- Update `QueryIntent.filters` to `list[dict[str, Any]]` for consistency with ADR001

**Priority Justification**: Real-world failures (2025-12-29) demonstrate that Tier 3 LLM parsing is not optional - current Tier 2 (embeddings) fails on:
- Multi-part queries (smoking status + quit time breakdown)
- Filter conditions with negation ("not on statin")
- Clinical terminology mapping ("LDL" ‚Üí "LDL mg/dL")
- Context-aware variable selection (selects wrong variable when multiple match)

Current fallback (0.3 confidence, default DESCRIBE) produces incorrect results. Tier 3 LLM parsing is required for clinical usability.

**Phase 3: Adaptive Dictionary Persistence** (Extends existing)
- File: `src/clinical_analytics/core/semantic.py`
- Add `add_user_alias()` method to extend existing alias index
- Add `save_alias_mapping()` method to persist user mappings
- Store in dataset metadata JSON (per ADR002 persistence layer)
- Add error recovery UI in `src/clinical_analytics/ui/pages/3_üí¨_Ask_Questions.py`

## Consequences

**Positive:**
- Clinicians trust results (audit trail)
- 10-second onboarding (no config wall)
- Handles clinical language ("no prior treatment", "VL <20")
- **Addresses real-world failures**: The two production query failures (2025-12-29) demonstrate that Tier 3 LLM parsing is required for clinical usability. Enhanced LLM parsing will handle:
  - Multi-part queries ("count X and breakdown Y")
  - Filter conditions with negation ("not on statin")
  - Clinical terminology mapping ("LDL" ‚Üí "LDL mg/dL")
  - Context-aware variable selection

**Negative:**
- API dependency (~$0.01/query, 800ms latency)
- First-query failures require user teaching
- Privacy constraint: only metadata to LLM, never patient rows
- **Current state**: Without Tier 3 LLM parsing, system falls back to default DESCRIBE intent with 0.3 confidence, producing incorrect results (as demonstrated in real-world failures)

## Alternatives Rejected

**Local LLM (Ollama):**
- Deferred to V2 (deployment complexity, lower JSON accuracy)
- Reconsider if cloud parsing becomes regulatory blocker

**Forced Column Mapping:**
- Creates adoption barrier
- Error-driven learning is superior for single-user tool

**Pure Regex Parsing (without LLM enhancement):**
- Fails on negation, synonyms, compound conditions
- Clinical language too complex for pattern matching
- **Note**: We keep regex for Tier 1 (fast path) but enhance with LLM for complex cases

**Pydantic Migration:**
- Current `@dataclass` approach is sufficient and consistent with codebase
- Pydantic would add dependency without clear benefit for MVP
- Can reconsider if validation/JSON schema generation becomes critical

## Code References

- `src/clinical_analytics/core/nl_query_engine.py` - Existing three-tier parsing system
- `src/clinical_analytics/core/semantic.py` - Existing alias index system
- `src/clinical_analytics/ui/pages/3_üí¨_Ask_Questions.py` - Result rendering (target for verification UI)
- `src/clinical_analytics/ui/components/result_interpreter.py` - Existing result rendering patterns
- `src/clinical_analytics/ui/storage/user_datasets.py` - Metadata persistence (for adaptive dictionary)