# ADR 008: Versioned Dataset Persistence with Rollback

## Status
**ACCEPTED** - 2025-12-31
**Implementation**: Pending

## Context

ADR002 established persistent storage with DuckDB and Parquet export, enabling data to survive app restarts. However, a critical gap remains: **dataset overwrite behavior**.

### Current Problem

When a user uploads a dataset with the same `dataset_name` as an existing upload, the system currently:
1. **Rejects the upload** with error: "Dataset 'X' already exists. Use a different name or delete the existing dataset."
2. **Forces manual deletion** before re-uploading
3. **Loses all history** - no way to track what changed between versions
4. **No rollback capability** - if new data has errors or incorrect outcomes, cannot revert

### Clinical Research Requirements

**Data Evolution Workflow:**
- Physicians receive updated datasets with new outcomes, corrected values, or additional patients
- Need to update existing dataset while preserving previous version for:
  - **Audit trail**: What changed and when
  - **Reproducibility**: Re-run analyses on previous version to verify results
  - **Rollback safety**: If new data has errors, revert to known-good version
  - **Outcome validation**: Compare results between versions to detect discrepancies

**Trust Requirements:**
- Never lose data (preserve all versions)
- Track which version is "active" (current production version)
- Enable time-travel queries (query any historical version)
- Support rollback without data loss

### Technical Constraints

- Single-user MVP (no multi-user version conflicts)
- File-based storage (HIPAA-friendly, no cloud vendor contracts)
- Must integrate with existing ADR002 architecture (DuckDB + Parquet)
- Content-based versioning already implemented (`dataset_version` = content hash)

## Decision

### Versioned Persistence Architecture

**Extend ADR002's versioning contract** to support:
1. **Version history tracking** - preserve all versions in metadata
2. **Active version flag** - mark which version is current
3. **Rollback mechanism** - switch active version without data loss
4. **Snapshot preservation** - never delete Parquet files (only add new ones)

### Version History Metadata Schema

**Extend ADR002's metadata JSON** to include version history:

```json
{
  "upload_id": "user_upload_20251231_120000_abc123",
  "dataset_name": "GDSI_OpenDataset_Final",
  "dataset_version": "x9y8z7w6v5u4t3s2",  // Current active version (content hash)
  "created_at": "2025-12-28T16:38:30Z",
  "updated_at": "2025-12-31T12:00:00Z",

  // NEW: Version history tracking
  "version_history": [
    {
      "version": "a1b2c3d4e5f6g7h8",  // dataset_version (content hash)
      "upload_id": "user_upload_20251228_163830_28450494",  // Original upload_id
      "created_at": "2025-12-28T16:38:30Z",
      "is_active": false,  // Legacy version (not current)
      "tables": {
        "GDSI_OpenDataset_Final": {
          "parquet_path": "data/parquet/user_upload_20251228_163830_28450494_GDSI_OpenDataset_Final_a1b2c3d4e5f6g7h8.parquet",
          "duckdb_table": "user_upload_20251228_163830_28450494_GDSI_OpenDataset_Final_a1b2c3d4e5f6g7h8",
          "row_count": 1141,
          "column_count": 47,
          "schema_fingerprint": "sha256_hash_of_canonical_schema_json"
        }
      },
      "schema": {
        "inferred_schema": {
          "GDSI_OpenDataset_Final": {
            "columns": {
              "patient_id": {"type": "Utf8", "is_identifier": true},
              "age": {"type": "Int64"},
              "ldl": {"type": "Float64"}
            }
          }
        }
      },
      "schema_changes": {
        "added_columns": [],
        "removed_columns": [],
        "type_changes": [],
        "is_backward_compatible": true
      },
      "note": "Initial upload"
    },
    {
      "version": "x9y8z7w6v5u4t3s2",  // New content hash (different data)
      "upload_id": "user_upload_20251231_120000_abc123",  // New upload_id
      "created_at": "2025-12-31T12:00:00Z",
      "is_active": true,  // Current active version
      "tables": {
        "GDSI_OpenDataset_Final": {
          "parquet_path": "data/parquet/user_upload_20251231_120000_abc123_GDSI_OpenDataset_Final_x9y8z7w6v5u4t3s2.parquet",
          "duckdb_table": "user_upload_20251231_120000_abc123_GDSI_OpenDataset_Final_x9y8z7w6v5u4t3s2",
          "row_count": 1250,
          "column_count": 48,
          "schema_fingerprint": "sha256_hash_of_canonical_schema_json"
        }
      },
      "schema": {
        "inferred_schema": {
          "GDSI_OpenDataset_Final": {
            "columns": {
              "patient_id": {"type": "Utf8", "is_identifier": true},
              "age": {"type": "Int64"},
              "ldl": {"type": "Float64"},
              "new_outcome_column": {"type": "Int64"}  // NEW COLUMN
            }
          }
        }
      },
      "schema_changes": {
        "added_columns": ["new_outcome_column"],
        "removed_columns": [],
        "type_changes": [],
        "is_backward_compatible": true  // Additive changes only
      },
      "note": "Updated with new outcomes"
    }
  ],

  // Existing ADR002 fields (preserved at top-level for backward compatibility)
  // NOTE: Active version's schema/aliases/config are authoritative
  "provenance": {
    "upload_type": "single",
    "tables": [...]
  },
  "schema": {
    "inferred_schema": {...}  // Mirrors active version's schema
  },
  "alias_mappings": {...},  // Mirrors active version's aliases (version-scoped)
  "semantic_config": {...},  // Mirrors active version's config (version-scoped)

  // NEW: Event log for audit trail
  "events": [
    {
      "timestamp": "2025-12-28T16:38:30Z",
      "event_type": "upload_created",
      "version": "a1b2c3d4e5f6g7h8",
      "upload_id": "user_upload_20251228_163830_28450494"
    },
    {
      "timestamp": "2025-12-31T12:00:00Z",
      "event_type": "version_activated",
      "version": "x9y8z7w6v5u4t3s2",
      "upload_id": "user_upload_20251231_120000_abc123",
      "previous_version": "a1b2c3d4e5f6g7h8"
    }
  ]
}
```

### Overwrite Behavior (Preserve All Versions)

**When `overwrite=True` and `dataset_name` exists:**

1. **Find existing metadata** by `dataset_name` (not `upload_id`)
2. **Compute `dataset_version`** from new data (content hash)
3. **Check for duplicate version**:
   - If `dataset_version` already exists in `version_history`:
     - Set that version as active (`is_active: true`)
     - Mark all other versions as inactive
     - Record `version_activated` event (not `upload_created`)
     - Update `dataset_version` at top-level
     - **Do NOT create new version entry** (dedupe: identical data = same version)
4. **If new version** (different content hash):
   - **Apply Schema Drift Policy** (see below) - may block or warn
   - **Mark all existing versions as inactive** (`is_active: false`)
   - **Create new version entry** with:
     - New `upload_id` (timestamp-based, immutable)
     - New `dataset_version` (content hash)
     - `is_active: true`
     - Canonical `tables` map (not `parquet_paths`/`duckdb_tables` lists)
     - Schema and schema changes
   - **Append to version history** (never delete old entries)
   - Record `upload_created` event
5. **Update metadata**:
   - `dataset_version` = active version (must match exactly one `is_active=True` entry)
   - `updated_at` = current timestamp
   - `schema`, `alias_mappings`, `semantic_config` = mirror active version's values
   - Append event to `events` list

**Invariants** (enforced in code):
- Exactly one version has `is_active: true` (assert in save/rollback)
- Top-level `dataset_version` equals active version's `version` field
- All Parquet files and DuckDB tables preserved (never deleted)

**Key Principle**: Never delete Parquet files or DuckDB tables. Old versions remain accessible for rollback and audit. Identical content hash = reuse existing version (no duplicate history).

### Cross-Dataset Content Deduplication

**Problem**: If a doctor uploads the same file content with a different `dataset_name`, the system currently creates duplicate storage and separate dataset entries. This wastes space and creates confusion (two datasets with identical data).

**Example Scenario**:
- Doctor uploads `patient_data.csv` as "Study A" â†’ creates dataset "Study A"
- Doctor uploads same `patient_data.csv` file as "Study B" â†’ creates duplicate dataset "Study B"
- Result: Same Parquet files stored twice, two separate dataset entries, no indication they're the same data

**Solution**: Check content hash before checking `dataset_name`. If content hash exists in any existing dataset, block the upload and suggest using existing dataset name with `overwrite=True`.

#### Deduplication Flow

**Before checking `dataset_name` uniqueness:**

1. **Compute `dataset_version`** (content hash) from uploaded file
2. **Check if content hash exists in ANY existing dataset**:
   ```python
   existing_with_same_content = find_datasets_by_content_hash(dataset_version)
   ```
3. **If content hash exists**:
   - Block upload with helpful error message
   - List existing dataset names that have this content
   - Suggest using existing dataset name with `overwrite=True`
   - **Do NOT create duplicate storage**

**Error Message Format**:
```
This file content already exists as dataset(s): "Study A", "Study B".
Use one of those names with overwrite=True, or rename the file if it's different data.
```

#### Implementation Details

**Function to find datasets by content hash:**
```python
def find_datasets_by_content_hash(content_hash: str) -> list[dict]:
    """Find all datasets (across all dataset_names) with matching content hash."""
    existing_datasets = []

    # Scan all metadata files
    for metadata_file in metadata_dir.glob("*.json"):
        meta = json.load(metadata_file)

        # Check top-level dataset_version (for datasets without version_history)
        if meta.get("dataset_version") == content_hash:
            existing_datasets.append({
                "dataset_name": meta.get("dataset_name"),
                "upload_id": meta.get("upload_id"),
                "version": meta.get("dataset_version")
            })

        # Check version_history (for datasets with version history)
        if "version_history" in meta:
            for version_entry in meta["version_history"]:
                if version_entry.get("version") == content_hash:
                    existing_datasets.append({
                        "dataset_name": meta.get("dataset_name"),
                        "upload_id": version_entry.get("upload_id"),
                        "version": version_entry.get("version")
                    })

    return existing_datasets
```

**Updated `save_upload()` flow:**
```python
def save_upload(file_bytes: bytes, dataset_name: str, overwrite: bool = False) -> tuple[bool, str, str | None]:
    # 1. Compute content hash FIRST (before any name checks)
    dataset_version = compute_dataset_version(file_bytes, metadata)

    # 2. Check for cross-dataset content deduplication
    existing_with_same_content = find_datasets_by_content_hash(dataset_version)
    if existing_with_same_content and not overwrite:
        existing_names = [m["dataset_name"] for m in existing_with_same_content]
        return (
            False,
            f"This file content already exists as dataset(s): {', '.join(existing_names)}. "
            f"Use one of those names with overwrite=True, or rename the file if it's different data.",
            None,
        )

    # 3. Check for same dataset_name (existing logic)
    if dataset_name_exists(dataset_name):
        if not overwrite:
            return (False, f"Dataset '{dataset_name}' already exists. Use overwrite=True to update.", None)
        # Proceed with overwrite (ADR008 overwrite logic)
        ...

    # 4. New dataset (different name AND different content)
    # Proceed with new upload
    ...
```

#### Edge Cases

**Same content, different name, user wants both:**
- **Blocked by default** (prevents accidental duplicates)
- **Workaround**: User must rename the file slightly (add a row, change a value) to get different content hash
- **Alternative**: Future enhancement could add "force_duplicate" flag (not recommended for MVP)

**Same content, same name, overwrite=False:**
- Handled by existing `dataset_name` check (rejects before content check)
- Error: "Dataset 'X' already exists. Use overwrite=True to update."

**Same content, same name, overwrite=True:**
- Handled by ADR008 overwrite logic (dedupe within same dataset_name)
- Reuses existing version entry (no duplicate storage)

#### Benefits

- **Prevents accidental duplicates**: Same file uploaded twice with different names is blocked
- **Saves storage**: No duplicate Parquet files for identical content
- **Clear user guidance**: Error message suggests correct action (use existing name with overwrite)
- **Maintains data integrity**: One content hash = one logical dataset (regardless of name)

#### Trade-offs

- **Stricter validation**: Users cannot create "duplicate" datasets with same content (by design)
- **Requires content hash computation**: Adds small overhead before name check (acceptable for MVP)
- **Metadata scanning**: Need to scan all metadata files to find content hash matches (acceptable for single-user MVP with limited datasets)

### Schema Drift Policy

**Problem**: When new columns are added (or removed) between versions, queries may reference columns that don't exist in rolled-back versions, causing failures. Additionally, column renames or type changes can break aliases and semantic configs.

**Solution**: Define explicit drift policy with classification, allow/block rules, and user override mechanism.

#### Drift Classification

**Schema fingerprint** (for fast comparison):
```python
def compute_schema_fingerprint(schema: dict) -> str:
    """Compute SHA256 hash of canonicalized schema JSON."""
    canonical = json.dumps(schema, sort_keys=True)
    return hashlib.sha256(canonical.encode()).hexdigest()[:16]
```

**Drift detector** (classifies changes):
```python
def classify_schema_drift(old_schema: dict, new_schema: dict) -> dict:
    """Classify schema changes: added, removed, type changes, rename candidates."""
    # Get table names (assume single table for MVP, or iterate over all tables)
    old_table_name = list(old_schema["inferred_schema"].keys())[0]
    new_table_name = list(new_schema["inferred_schema"].keys())[0]

    # For MVP: assume same table name (multi-table support deferred)
    if old_table_name != new_table_name:
        raise ValueError(f"Table name mismatch: {old_table_name} vs {new_table_name}")

    old_cols = set(old_schema["inferred_schema"][old_table_name]["columns"].keys())
    new_cols = set(new_schema["inferred_schema"][new_table_name]["columns"].keys())

    added = new_cols - old_cols
    removed = old_cols - new_cols

    # Type changes
    type_changes = []
    common_cols = old_cols & new_cols
    for col in common_cols:
        old_type = old_schema["inferred_schema"][old_table_name]["columns"][col]["type"]
        new_type = new_schema["inferred_schema"][new_table_name]["columns"][col]["type"]
        if old_type != new_type:
            type_changes.append({"column": col, "old_type": old_type, "new_type": new_type})

    # Rename candidates (heuristic: same type, similar name, one removed, one added)
    rename_candidates = []
    if removed and added:
        # Simple heuristic: if removed count == added count and types match, possible rename
        # (Full rename detection requires data comparison, deferred to future)
        pass

    return {
        "added_columns": list(added),
        "removed_columns": list(removed),
        "type_changes": type_changes,
        "rename_candidates": rename_candidates,
        "drift_type": "additive" if not removed and not type_changes else "breaking"
    }
```

#### Drift Policy Rules

**Allowed Drift (Default - No User Action Required):**
- âœ… **Add nullable columns only**: New columns added, all existing columns preserved
- âœ… **No type changes**: All existing columns keep same types
- âœ… **No column removals**: All existing columns remain

**Blocked Drift (Requires User Override):**
- âŒ **Column removals**: Any column removed from schema
- âŒ **Type changes**: Any column type changed (e.g., `Int64` â†’ `Float64`)
- âŒ **Rename without mapping**: Column removed and new column added (possible rename) without explicit rename mapping

**Override Mechanism:**
- User can acknowledge and proceed with blocked drift
- System records `overwrite_attempt_blocked_by_drift` event with user acknowledgment
- Writes drift note into version entry: `"drift_note": "User acknowledged: removed 'old_column', added 'new_column' (possible rename)"`
- **Warning**: Override may break aliases and semantic configs (see below)

**Drift Event Definition:**
- **Drift event occurs** when `schema_fingerprint_old != schema_fingerprint_new`
- **Drift is allowed** if policy permits (additive changes only)
- **Drift is blocked** if policy prohibits (removals, type changes, renames without mapping)
- **Drift override** requires explicit user acknowledgment (not automatic)

#### Impact on Aliases and Semantic Config

**When drift occurs:**

1. **Alias mappings** (ADR003):
   - Aliases are scoped to `(upload_id, dataset_version)`
   - If target column removed: alias becomes orphaned (marked in metadata, ignored in queries)
   - If target column renamed: alias breaks unless explicit rename mapping provided
   - **On rollback**: Active aliases switch to rolled-back version's aliases (version-scoped)

2. **Semantic config**:
   - If metric/dimension column removed: config becomes invalid
   - System should validate config against active version's schema on load
   - **On rollback**: Active semantic config switches to rolled-back version's config (version-scoped)

**Explicit Version Scoping:**
- Each version entry should include its own `alias_mappings` and `semantic_config` (or reference them)
- Top-level `alias_mappings` and `semantic_config` mirror active version
- Rollback updates top-level fields to match rolled-back version

### Schema Evolution and Compatibility

#### Schema Change Detection (Implementation)

**On overwrite, detect and record schema changes using drift classifier:**

```python
def detect_schema_changes(old_schema: dict, new_schema: dict) -> dict:
    """Detect and classify schema changes between versions."""
    drift = classify_schema_drift(old_schema, new_schema)

    return {
        "added_columns": drift["added_columns"],
        "removed_columns": drift["removed_columns"],
        "type_changes": [f"{tc['column']}: {tc['old_type']} â†’ {tc['new_type']}" for tc in drift["type_changes"]],
        "rename_candidates": drift["rename_candidates"],
        "is_backward_compatible": drift["drift_type"] == "additive",
        "schema_fingerprint_old": compute_schema_fingerprint(old_schema),
        "schema_fingerprint_new": compute_schema_fingerprint(new_schema)
    }
```

**Schema compatibility rules:**
- **Backward compatible**: Additive changes only (new columns added, no removals, no type changes)
- **Breaking changes**: Columns removed or types changed (block by default, require user override)

#### Schema Compatibility Checking

**Helper function to check compatibility between versions:**

```python
def check_schema_compatibility(current_schema: dict, target_schema: dict) -> dict:
    """Check schema compatibility between current and target versions."""
    if not current_schema or not target_schema:
        return {"is_backward_compatible": True, "removed_columns": [], "type_changes": []}

    # Get table names (assume single table for MVP)
    current_table_name = list(current_schema["inferred_schema"].keys())[0]
    target_table_name = list(target_schema["inferred_schema"].keys())[0]

    if current_table_name != target_table_name:
        raise ValueError(f"Table name mismatch: {current_table_name} vs {target_table_name}")

    current_cols = set(current_schema["inferred_schema"][current_table_name]["columns"].keys())
    target_cols = set(target_schema["inferred_schema"][target_table_name]["columns"].keys())

    removed = current_cols - target_cols
    added = target_cols - current_cols

    # Check type changes for common columns
    type_changes = []
    common_cols = current_cols & target_cols
    for col in common_cols:
        current_type = current_schema["inferred_schema"][current_table_name]["columns"][col]["type"]
        target_type = target_schema["inferred_schema"][target_table_name]["columns"][col]["type"]
        if current_type != target_type:
            type_changes.append(f"{col}: {current_type} â†’ {target_type}")

    return {
        "is_backward_compatible": len(removed) == 0 and len(type_changes) == 0,
        "removed_columns": list(removed),
        "added_columns": list(added),
        "type_changes": type_changes
    }
```

#### Query Validation Against Active Schema

**Before executing query, validate column references exist:**

```python
def validate_query_against_schema(query_plan: QueryPlan, active_version: dict) -> list[str]:
    """Validate query references exist in active version schema."""
    warnings = []
    schema = active_version.get("schema", {}).get("inferred_schema", {})
    if not schema:
        return warnings  # No schema available, skip validation

    # Get table name from schema (first table, or use table name from query context)
    table_name = list(schema.keys())[0] if schema else None
    if not table_name:
        return warnings

    available_columns = set(schema[table_name]["columns"].keys())

    # Check metric column
    if query_plan.metric and query_plan.metric not in available_columns:
        warnings.append(f"Metric column '{query_plan.metric}' not in active version schema")

    # Check group_by column
    if query_plan.group_by and query_plan.group_by not in available_columns:
        warnings.append(f"Group-by column '{query_plan.group_by}' not in active version schema")

    # Check filter columns
    for filter_spec in query_plan.filters:
        if filter_spec.column not in available_columns:
            warnings.append(f"Filter column '{filter_spec.column}' not in active version schema")

    return warnings
```

**Behavior:**
- Warnings displayed to user (non-blocking)
- Query execution proceeds (may fail at runtime if column missing)
- User can rollback to version with required columns

### Rollback Mechanism

**API:**
```python
def rollback_to_version(dataset_name: str, target_version: str) -> tuple[bool, str, list[str]]:
    """
    Rollback dataset to a previous version with schema compatibility check.

    Args:
        dataset_name: Dataset name (not upload_id)
        target_version: Content hash of target version

    Returns:
        (success, message, warnings)
    """
    # Load metadata by dataset_name (with file locking to prevent concurrent writes)
    import fcntl
    with open(metadata_path, "r+") as f:
        fcntl.flock(f, fcntl.LOCK_EX)  # Exclusive lock
        meta = json.load(f)

        # Find current and target versions
        current_version = next((v for v in meta["version_history"] if v["is_active"]), None)
        target = next((v for v in meta["version_history"] if v["version"] == target_version), None)

        if not target:
            return False, f"Version {target_version} not found in history", []

        warnings = []

        # Check schema compatibility if rolling back (not forward)
        if current_version and current_version["version"] != target_version:
            compatibility = check_schema_compatibility(
                current_version.get("schema", {}),
                target.get("schema", {})
            )

            if not compatibility["is_backward_compatible"]:
                if compatibility.get("removed_columns"):
                    warnings.append(f"âš ï¸ Columns removed in target version: {compatibility['removed_columns']}")
                if compatibility.get("type_changes"):
                    warnings.append(f"âš ï¸ Type changes in target version: {compatibility['type_changes']}")
                warnings.append("Queries referencing removed/changed columns may fail after rollback.")

        # Mark all versions as inactive
        for version in meta["version_history"]:
            version["is_active"] = False

        # Activate target version
        target["is_active"] = True
        meta["dataset_version"] = target["version"]  # Enforce invariant: top-level = active version

        # Update top-level schema/aliases/config to match active version (version-scoped)
        if "schema" in target:
            meta["schema"] = target["schema"]
        if "alias_mappings" in target:
            meta["alias_mappings"] = target["alias_mappings"]
        if "semantic_config" in target:
            meta["semantic_config"] = target["semantic_config"]

        meta["updated_at"] = datetime.now().isoformat()

        # Record rollback event
        meta.setdefault("events", []).append({
            "timestamp": datetime.now().isoformat(),
            "event_type": "rollback",
            "from_version": current_version["version"] if current_version else None,
            "to_version": target_version
        })

        # Assert invariant: exactly one active version
        active_count = sum(1 for v in meta["version_history"] if v["is_active"])
        assert active_count == 1, f"Invariant violation: {active_count} active versions (must be exactly 1)"

        # Save updated metadata
        f.seek(0)
        f.truncate()
        json.dump(meta, f, indent=2)
        fcntl.flock(f, fcntl.LOCK_UN)  # Release lock

    message = f"Rolled back to version {target_version} (created {target['created_at']})"
    if warnings:
        message += f"\nWarnings: {'; '.join(warnings)}"

    return True, message, warnings
```

**UI Integration:**
- Show version history in dataset selector
- Display active version indicator
- Add "Rollback to version" button in dataset management UI
- Show version metadata (row count, created date, notes, schema changes)
- Display schema compatibility warnings on rollback
- Show schema diff (added/removed columns) in version history
- Display event log (upload_created, version_activated, rollback events)
- Show drift warnings when overwriting with breaking changes

### Integration with ADR002

**Leverages existing ADR002 infrastructure:**

1. **Content-based versioning** (ADR002 Phase 1):
   - `compute_dataset_version()` already computes content hash
   - Same data = same version (storage reuse)
   - Different data = new version (new Parquet files)

2. **Parquet export** (ADR002 Phase 2):
   - Parquet files already versioned by filename: `{upload_id}_{table_name}_{dataset_version}.parquet`
   - Each version gets unique Parquet files (preserved on overwrite)

3. **DuckDB persistence** (ADR002 Phase 1):
   - DuckDB tables already versioned: `{upload_id}_{table_name}_{dataset_version}`
   - Old tables remain in DuckDB (can query historical versions)

4. **Metadata JSON** (ADR002):
   - Extends existing metadata schema (adds `version_history` field)
   - Preserves all existing fields (provenance, schema, aliases)

**No breaking changes** - existing uploads continue to work. New uploads with `overwrite=True` get version history.

### Active Version Resolution Algorithm

**Canonical rule** (enforced in all code paths):

```python
def get_active_version(metadata: dict) -> dict | None:
    """Get active version using canonical resolution algorithm."""
    if "version_history" in metadata and metadata["version_history"]:
        # Version history exists: find active version
        active = [v for v in metadata["version_history"] if v.get("is_active")]
        if len(active) == 0:
            # No active version: use latest (fallback)
            return sorted(metadata["version_history"], key=lambda v: v["created_at"], reverse=True)[0]
        elif len(active) == 1:
            # Exactly one active (invariant)
            return active[0]
        else:
            # Invariant violation: multiple active versions
            raise ValueError(f"Invariant violation: {len(active)} active versions (must be exactly 1)")
    else:
        # No version history: use top-level dataset_version (backward compatibility)
        if "dataset_version" in metadata:
            # Create synthetic version entry for backward compatibility
            return {
                "version": metadata["dataset_version"],
                "upload_id": metadata.get("upload_id"),
                "is_active": True,
                "tables": {...},  # Reconstruct from top-level metadata
                "schema": metadata.get("schema", {}),
            }
        return None
```

**Invariants** (asserted in save/rollback):
- If `version_history` exists: exactly one entry has `is_active: true`
- Top-level `dataset_version` equals active version's `version` field
- Top-level `schema`, `alias_mappings`, `semantic_config` mirror active version's values

## Implementation Plan

### Phase 1: Version History Metadata and Schema Drift (3 hours)

**Files to Modify:**
- `src/clinical_analytics/ui/storage/user_datasets.py`:
  - Add `version_history` field to metadata structure
  - Modify `save_upload()` to handle overwrite with version preservation
  - Add `rollback_to_version()` method

**Tasks:**
```
[ ] Extend metadata schema to include version_history array with canonical tables structure
    - Replace parquet_paths/duckdb_tables lists with tables map
    - Kill table_0 artifact (use actual table names)
    - Add schema_fingerprint to each table entry
[ ] Add cross-dataset content deduplication
    - Compute dataset_version (content hash) BEFORE dataset_name check
    - Implement find_datasets_by_content_hash() function
    - Block upload if content hash exists in any existing dataset
    - Provide helpful error message with existing dataset names
    - Suggest using existing name with overwrite=True
[ ] Implement schema drift detector
    - compute_schema_fingerprint() function
    - classify_schema_drift() function
    - Store schema_diff in version entry
[ ] Implement Schema Drift Policy
    - Allow: additive changes (new nullable columns)
    - Block: removals, type changes, renames without mapping
    - Override: user acknowledgment + drift_note in version entry
[ ] Modify save_upload() to preserve versions on overwrite
    - Find existing metadata by dataset_name
    - Compute dataset_version first (content hash)
    - Check for duplicate version (dedupe: if exists, activate it, don't create new)
    - Apply Schema Drift Policy (block or warn)
    - Mark old versions as inactive
    - Append new version to history (if not duplicate)
    - Never delete Parquet files or DuckDB tables
    - Enforce invariants (exactly one active, top-level matches active)
[ ] Add event log
    - Append-only events list in metadata
    - Event types: upload_created, version_activated, rollback, overwrite_attempt_blocked_by_drift
[ ] Add rollback_to_version() method
    - Load metadata by dataset_name (with file locking)
    - Find target version in history
    - Check schema compatibility (warn if breaking changes)
    - Switch active version flag
    - Update top-level schema/aliases/config to match active version (version-scoped)
    - Record rollback event
    - Enforce invariants (exactly one active)
    - Return warnings for UI display
[ ] Add active version resolution algorithm
    - get_active_version() function with canonical rules
    - Fallback to latest if no active version
    - Assert invariants
[ ] Add query validation against active schema
    - Validate QueryPlan column references exist in active version
    - Use actual table names (not table_0)
    - Return warnings for missing columns
    - Display warnings in UI (non-blocking)
[ ] Add file locking for metadata writes
    - Use fcntl (Unix) or msvcrt (Windows) for exclusive locks
    - Prevent concurrent Streamlit reruns from corrupting JSON
[ ] Add tests for version history preservation
[ ] Add tests for rollback mechanism
[ ] Add tests for schema change detection
[ ] Add tests for query validation against schema
[ ] Add tests for dedupe logic (identical content hash within same dataset_name)
[ ] Add tests for cross-dataset content deduplication
    - Same content, different name â†’ blocked with helpful error
    - Same content, same name, overwrite=True â†’ handled by overwrite logic
    - Different content, different name â†’ allowed (new dataset)
[ ] Add tests for active version resolution
[ ] Add tests for Schema Drift Policy (allow/block/override)
```

**Success Criteria:**
- Overwriting dataset preserves all previous versions
- Version history tracks all uploads with same dataset_name
- Identical content hash = reuse existing version (no duplicate history)
- Cross-dataset content deduplication blocks same content with different names
- Error messages guide users to use existing dataset name with overwrite=True
- Schema changes detected and recorded in version history
- Schema Drift Policy enforced (block breaking changes, allow additive)
- Rollback switches active version without data loss
- Rollback updates aliases/semantic_config to match rolled-back version (version-scoped)
- Schema compatibility warnings displayed on rollback
- Query validation warns about missing columns
- Active version resolution works (exactly one active, fallback to latest)
- Event log records all version operations
- File locking prevents metadata corruption
- Old Parquet files remain accessible

### Phase 2: UI Integration (1 hour)

**Files to Modify:**
- `src/clinical_analytics/ui/pages/1_ðŸ“¤_Add_Your_Data.py`:
  - Add "Overwrite existing dataset" checkbox
  - Show version history in dataset selector
  - Add rollback UI

**Tasks:**
```
[ ] Add overwrite checkbox to upload UI
    - Only shown when dataset_name matches existing upload
    - Passes overwrite=True to save_upload()
[ ] Display version history in dataset selector
    - Show active version indicator
    - Show version metadata (row count, date, notes)
[ ] Add rollback button/UI
    - Allow selecting target version from history
    - Confirm rollback action
    - Refresh dataset list after rollback
```

**Success Criteria:**
- Users can overwrite datasets via UI
- Version history visible in dataset selector
- Rollback accessible from UI

### Phase 3: Query Execution with Version Awareness (1 hour)

**Files to Modify:**
- `src/clinical_analytics/datasets/uploaded/definition.py`:
  - Load active version Parquet files (not just latest)
  - Support querying historical versions (optional)

**Tasks:**
```
[ ] Update UploadedDataset to use active version
    - Load metadata, find version with is_active=True
    - Use Parquet paths from active version
    - Fall back to latest if no active version (backward compatibility)
[ ] Add optional version parameter to query methods
    - Allow querying specific historical version
    - Default to active version
[ ] Update semantic layer registration
    - Register tables from active version
    - Keep old tables in DuckDB (for historical queries)
[ ] Integrate query validation with execution flow
    - Validate QueryPlan against active version schema before execution
    - Display warnings for missing columns in UI
    - Allow execution to proceed (may fail at runtime if column missing)
```

**Success Criteria:**
- Queries use active version by default (via active version resolution algorithm)
- Query validation warns about missing columns before execution
- Historical versions remain queryable (optional feature)
- Backward compatible (existing uploads without version_history work)
- Active version resolution enforces exactly-one-active invariant

## Future Consideration: Delta Lake

### Why Consider Delta Lake

**Delta Lake provides built-in versioning:**
- **Time-travel queries**: `SELECT * FROM delta_table VERSION AS OF 5`
- **ACID transactions**: Guaranteed consistency
- **Schema evolution**: Automatic schema versioning and merging
  - Additive changes (new columns) automatically merged
  - Breaking changes (removals, type changes) require explicit schema evolution mode
  - Schema history tracked in delta log
- **Audit log**: Delta log tracks all changes (data + schema)
- **Rollback**: Built-in version switching with schema awareness

**Example Delta Lake Workflow** (conceptual - actual API depends on deltalake library):
```python
# Write new version (creates new Delta version automatically)
# Schema evolution handled automatically (additive changes merged)
from deltalake import write_deltalake
write_deltalake("data/delta/my_dataset/", df, mode="overwrite")

# Query current version (uses current schema)
from deltalake import DeltaTable
dt = DeltaTable("data/delta/my_dataset/")
df = dt.to_polars()  # Uses current version

# Query historical version (uses schema from that version)
df_v5 = dt.to_polars(version=5)  # Uses schema from version 5

# Schema evolution: Add new column (automatic merge with schema evolution mode)
df_with_new_col = df.with_columns(pl.lit(0).alias("new_outcome"))
write_deltalake(
    "data/delta/my_dataset/",
    df_with_new_col,
    mode="append",
    schema_mode="merge"  # Automatically merges new columns
)
# Delta Lake automatically:
# - Merges new column into existing schema
# - Creates new version with updated schema
# - Tracks schema history in delta log

# Rollback (switch to previous version, schema reverts automatically)
dt.restore(version=5)  # Conceptual - actual API may differ
```

**Note**: Actual Delta Lake API may differ. This is conceptual to illustrate capabilities. Verify against [deltalake-py documentation](https://delta-io.github.io/delta-rs/python/) before implementation.

### Delta Lake Trade-offs

**Benefits:**
- Built-in versioning (no custom metadata management)
- Time-travel queries (query any version directly)
- ACID guarantees (transactional writes)
- Schema evolution (automatic handling)
- Audit trail (delta log provides complete history)

**Costs:**
- **Additional dependency**: `deltalake` library (Rust-based, adds complexity)
- **Different API**: Not drop-in replacement for vanilla Parquet
- **Learning curve**: Team needs to understand Delta Lake concepts
- **Migration effort**: Need to convert existing Parquet files to Delta format

### Recommendation: Defer to Phase 2

**Phase 1 (MVP)**: Vanilla Parquet with version history metadata
- Simpler implementation (no new dependencies)
- Full control over versioning logic
- Leverages existing ADR002 infrastructure
- Sufficient for single-user MVP needs

**Phase 2 (Enhancement)**: Evaluate Delta Lake migration
- **Trigger conditions**:
  - Time-travel queries become critical requirement
  - Schema evolution needs exceed manual handling
  - Audit trail requirements grow beyond metadata JSON
  - Multi-user support requires ACID transactions
- **Migration path**: Convert Parquet files to Delta format, update load paths
- **Decision point**: Create ADR009 when conditions met

### Delta Lake Decision Criteria

**Revisit Delta Lake when ANY of these conditions are met:**

1. **Time-Travel Queries Required**:
   - Users frequently query historical versions
   - Need to compare results across versions programmatically
   - Current metadata-based lookup is too slow

2. **Schema Evolution Complexity**:
   - Frequent schema changes (columns added/removed)
   - Manual schema versioning becomes error-prone
   - Need automatic schema migration
   - Current manual schema change detection insufficient for complex evolution

3. **Audit Trail Requirements**:
   - Regulatory requirements for complete change history
   - Need to track who changed what and when
   - Metadata JSON insufficient for audit needs

4. **Multi-User Support**:
   - Concurrent uploads require ACID transactions
   - Need conflict resolution for simultaneous updates
   - File-based locking insufficient

5. **Performance Issues**:
   - Metadata-based version lookup becomes bottleneck
   - Need faster version switching
   - Delta log provides better query optimization

**When triggered, create ADR009 with concrete Delta Lake migration plan.**

## Consequences

### Positive
- **Data Preservation**: Never lose historical versions (critical for clinical research)
- **Rollback Safety**: Revert to known-good version if new data has errors
- **Audit Trail**: Complete version history for reproducibility
- **Outcome Validation**: Compare results between versions to detect discrepancies
- **Trust**: Physicians can verify analyses on previous versions
- **No Breaking Changes**: Existing uploads continue to work (backward compatible)

### Negative
- **Disk Usage**: All versions stored (trade-off for safety) - see retention plan below
- **Metadata Complexity**: Version history adds to metadata JSON size
- **UI Complexity**: Need to expose version history and rollback in UI
- **Query Complexity**: Need to track which version is active
- **Schema Drift**: Queries may fail after rollback if columns removed (warnings help but don't prevent)
- **Manual Schema Tracking**: Schema changes detected manually (Delta Lake would automate this)
- **File Locking Overhead**: Exclusive locks on metadata writes (acceptable for single-user MVP)

### Risks & Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| Disk bloat from version history | Storage costs grow | **Retention plan**: MVP keeps all versions, warn at N versions or X GB, future: archive_versions(keep_last=N) |
| Metadata JSON becomes too large | Slow metadata loading | Paginate version history, lazy-load old versions |
| Confusion about active version | Wrong version queried | Clear UI indicators, explicit version selection, enforce exactly-one-active invariant |
| Rollback breaks dependent analyses | Results inconsistent | Add version dependency tracking, warn before rollback |
| Schema drift breaks queries | Queries fail after rollback | Schema compatibility warnings, query validation before execution, Schema Drift Policy blocks breaking changes |
| Manual schema tracking errors | Incorrect schema change detection | Comprehensive tests, schema fingerprint for fast comparison, consider Delta Lake for automation |
| Concurrent metadata writes | JSON corruption | File locking (fcntl/msvcrt) on metadata writes |
| Aliases/semantic config drift | Wrong aliases after rollback | Version-scoped aliases/config, rollback updates top-level to match active version |

### Version Retention and Garbage Collection

**MVP Default**: Keep all versions (no GC)
- Preserves complete audit trail
- Enables rollback to any historical version
- Acceptable for single-user MVP with limited dataset count

**Guardrails**:
- Warn user when version count exceeds threshold (e.g., 10 versions)
- Warn user when total storage exceeds threshold (e.g., 10 GB per dataset)
- Display storage usage in UI

**Future Enhancement**: `archive_versions(keep_last=N)` function
- Archives old versions to separate storage (does not break audit trail)
- Metadata remains (version history preserved)
- Parquet files moved to archive directory
- DuckDB tables can be dropped (Parquet is source of truth)
- **Does not break audit trail** because metadata JSON remains with full history

**Implementation** (deferred to future):
```python
def archive_versions(dataset_name: str, keep_last: int = 5) -> None:
    """Archive old versions, keeping only last N active."""
    meta = load_metadata_by_name(dataset_name)

    # Sort by created_at, keep last N
    sorted_versions = sorted(meta["version_history"], key=lambda v: v["created_at"], reverse=True)
    to_archive = sorted_versions[keep_last:]

    # Move Parquet files to archive
    for version in to_archive:
        for table_name, table_info in version["tables"].items():
            archive_path = archive_dir / table_info["parquet_path"].name
            shutil.move(table_info["parquet_path"], archive_path)
            table_info["parquet_path"] = str(archive_path)
            table_info["archived"] = True

    # Update metadata (version history remains, just marked archived)
    save_metadata(meta)
```

### NOT Addressed (Future ADRs)
- Version comparison UI (diff tool for seeing what changed)
- Multi-user version conflicts (concurrent overwrites)
- Advanced rename detection (requires data comparison, not just schema)

## Alternatives Considered

### 1. Simple Overwrite (Delete Old, Replace with New)
**Rejected**: Loses audit trail, no rollback capability, breaks trust requirements

### 2. Require Unique Dataset Names (No Overwrite)
**Rejected**: Forces manual deletion, breaks workflow when data updates arrive

### 3. Delta Lake from Start
**Rejected**: Adds complexity and dependency for MVP; can migrate later when needs justify it

### 4. Git-like Versioning (Full History in Metadata)
**Rejected**: Overkill for single-user MVP; Delta Lake provides this if needed later

## Success Metrics

### Must-Have (Go/No-Go)
- [ ] Overwrite dataset with same name preserves all previous versions
- [ ] Rollback to previous version works without data loss
- [ ] Version history visible in UI
- [ ] Queries use active version by default
- [ ] Old Parquet files remain accessible after overwrite
- [ ] Schema changes detected and recorded in version history
- [ ] Schema compatibility warnings displayed on rollback
- [ ] Query validation warns about missing columns before execution

### Nice-to-Have
- [ ] Version comparison UI (see what changed, including schema diff)
- [ ] Version retention policy (auto-archive old versions)
- [ ] Query historical versions directly (time-travel queries)
- [ ] Schema evolution visualization (show column additions/removals over time)

## References

### Related Decisions
- **[ADR002: Persistent Storage Layer with DuckDB and Deferred Ibis Integration](ADR002.md)**:
  - **PREREQUISITE**: ADR008 extends ADR002's versioning contract
  - ADR002 provides content-based versioning (`dataset_version` = content hash)
  - ADR002 provides Parquet export (versioned by filename)
  - ADR002 provides DuckDB persistence (versioned tables)
  - ADR008 adds version history tracking and rollback on top of ADR002 infrastructure

- **[ADR001: Query Plan Producer, Filtering, and Chat-First Execution Rules](ADR001.md)**:
  - ADR001 uses `dataset_version` for idempotent run keys
  - ADR008 ensures version history doesn't break run_key determinism
  - Rollback changes active version but preserves run_key for historical queries

- **[ADR003: Clinical Trust Protocol + Adaptive Alias Persistence](ADR003.md)**:
  - ADR003 stores alias mappings per `(upload_id, dataset_version)`
  - ADR008 preserves aliases in version history (each version has its own aliases)
  - Rollback may change active aliases (expected behavior)

### Technical Documentation
- [Delta Lake Documentation](https://delta.io/) - For future reference
- [DuckDB Time-Travel Queries](https://duckdb.org/docs/guides/performance/time_travel) - Alternative to Delta Lake
- [Polars Lazy API](https://pola-rs.github.io/polars/user-guide/lazy/using/) - For version-aware query execution

### Code References
- `src/clinical_analytics/ui/storage/user_datasets.py` - UserDatasetStorage (to be extended)
- `src/clinical_analytics/storage/versioning.py` - `compute_dataset_version()` (already implemented)
- `src/clinical_analytics/storage/datastore.py` - DataStore Parquet export (already versioned)
- `src/clinical_analytics/datasets/uploaded/definition.py` - UploadedDataset (to be made version-aware)

## Decision Makers
- **Jason** (Technical Lead): Approved architecture
- **Clinical Stakeholder** (Infectious Disease Physician): Validated workflow requirements

## Review Date
**2026-02-15** - After 6 weeks of production use (from implementation date), evaluate:
1. Is version history growing too large (disk usage)?
2. Are rollback operations frequent enough to justify Delta Lake?
3. Do users need time-travel queries (query historical versions directly)?
4. Should we implement version retention policies?

---

## Appendix A: Version History Metadata Example

**Complete example of version history in metadata JSON:**

```json
{
  "upload_id": "user_upload_20251231_120000_abc123",
  "dataset_name": "GDSI_OpenDataset_Final",
  "dataset_version": "x9y8z7w6v5u4t3s2",
  "created_at": "2025-12-28T16:38:30Z",
  "updated_at": "2025-12-31T12:00:00Z",

  "version_history": [
    {
      "version": "a1b2c3d4e5f6g7h8",
      "upload_id": "user_upload_20251228_163830_28450494",
      "created_at": "2025-12-28T16:38:30Z",
      "is_active": false,
      "tables": {
        "GDSI_OpenDataset_Final": {
          "parquet_path": "data/parquet/user_upload_20251228_163830_28450494_GDSI_OpenDataset_Final_a1b2c3d4e5f6g7h8.parquet",
          "duckdb_table": "user_upload_20251228_163830_28450494_GDSI_OpenDataset_Final_a1b2c3d4e5f6g7h8",
          "row_count": 1141,
          "column_count": 47,
          "schema_fingerprint": "a1b2c3d4e5f6g7h8"
        }
      },
      "schema": {
        "inferred_schema": {
          "GDSI_OpenDataset_Final": {
            "columns": {
              "patient_id": {"type": "Utf8", "is_identifier": true},
              "covid19_outcome_recovered": {"type": "Int64", "is_outcome": true},
              "age": {"type": "Int64"},
              "ldl": {"type": "Float64"}
            }
          }
        }
      },
      "schema_changes": {
        "added_columns": [],
        "removed_columns": [],
        "type_changes": [],
        "is_backward_compatible": true
      },
      "alias_mappings": {
        "user_aliases": {"VL": "viral_load"},
        "system_aliases": {"viral_load": ["VL", "viral load"]}
      },
      "semantic_config": {
        "base_view": "patient_level",
        "metrics": ["ldl", "age"]
      },
      "events": [
        {
          "timestamp": "2025-12-28T16:38:30Z",
          "event_type": "upload_created",
          "version": "a1b2c3d4e5f6g7h8"
        }
      ],
      "note": "Initial upload"
    },
    {
      "version": "x9y8z7w6v5u4t3s2",
      "upload_id": "user_upload_20251231_120000_abc123",
      "created_at": "2025-12-31T12:00:00Z",
      "is_active": true,
      "tables": {
        "GDSI_OpenDataset_Final": {
          "parquet_path": "data/parquet/user_upload_20251231_120000_abc123_GDSI_OpenDataset_Final_x9y8z7w6v5u4t3s2.parquet",
          "duckdb_table": "user_upload_20251231_120000_abc123_GDSI_OpenDataset_Final_x9y8z7w6v5u4t3s2",
          "row_count": 1250,
          "column_count": 48,
          "schema_fingerprint": "x9y8z7w6v5u4t3s2"
        }
      },
      "schema": {
        "inferred_schema": {
          "GDSI_OpenDataset_Final": {
            "columns": {
              "patient_id": {"type": "Utf8", "is_identifier": true},
              "covid19_outcome_recovered": {"type": "Int64", "is_outcome": true},
              "age": {"type": "Int64"},
              "ldl": {"type": "Float64"},
              "new_outcome_column": {"type": "Int64"}  // NEW COLUMN ADDED
            }
          }
        }
      },
      "schema_changes": {
        "added_columns": ["new_outcome_column"],
        "removed_columns": [],
        "type_changes": [],
        "is_backward_compatible": true,  // Additive change only
        "schema_fingerprint_old": "a1b2c3d4e5f6g7h8",
        "schema_fingerprint_new": "x9y8z7w6v5u4t3s2"
      },
      "alias_mappings": {
        "user_aliases": {"VL": "viral_load", "NO": "new_outcome_column"},
        "system_aliases": {"viral_load": ["VL", "viral load"], "new_outcome_column": ["NO", "new outcome"]}
      },
      "semantic_config": {
        "base_view": "patient_level",
        "metrics": ["ldl", "age", "new_outcome_column"]
      },
      "events": [
        {
          "timestamp": "2025-12-31T12:00:00Z",
          "event_type": "version_activated",
          "version": "x9y8z7w6v5u4t3s2",
          "previous_version": "a1b2c3d4e5f6g7h8"
        }
      ],
      "note": "Updated with new outcomes - added 109 patients and new_outcome_column"
    }
  ],

  "provenance": {
    "upload_type": "single",
    "tables": ["GDSI_OpenDataset_Final"],
    "source_files": ["GDSI_OpenDataset_Final.csv"]
  },
  "schema": {
    "inferred_schema": {
      "GDSI_OpenDataset_Final": {
        "columns": {
          "patient_id": {"type": "Utf8", "is_identifier": true},
          "covid19_outcome_recovered": {"type": "Int64", "is_outcome": true}
        }
      }
    }
  }
}
```

## Appendix B: Delta Lake Migration Path (Future)

**If Delta Lake is adopted in Phase 2:**

1. **Convert existing Parquet to Delta**:
   ```python
   # One-time migration script
   for version in version_history:
       for table_name, table_info in version["tables"].items():
           parquet_path = table_info["parquet_path"]
           delta_path = f"data/delta/{dataset_name}/{table_name}/"
           pl.read_parquet(parquet_path).write_delta(delta_path, mode="append")
   ```

2. **Update load paths**:
   ```python
   # Before: pl.scan_parquet(table_info["parquet_path"])
   # After:  pl.scan_delta(delta_path, version=target_version)
   ```

3. **Simplify metadata**:
   - Remove `tables` map from version_history (Delta handles storage paths)
   - Keep `version`, `is_active`, and `schema` (still needed for UI and validation)
   - Delta log provides complete audit trail

4. **Leverage Delta features**:
   - Time-travel queries: `pl.scan_delta(path, version=5)`
   - Schema evolution: Automatic handling
   - ACID transactions: Built-in guarantees

**Migration is non-breaking** - can run both formats in parallel during transition.
