# ADR 002: Persistent Storage Layer with DuckDB and Deferred Ibis Integration

## Status
**ACCEPTED** - 2024-12-29  
**IN PROGRESS** - Partial implementation complete; migration to full vision in progress

## Context

The MD Data Explorer currently stores all uploaded clinical datasets and analysis results in Streamlit session state (in-memory only). This creates critical usability and scalability issues:

### Problems with Current Architecture
1. **Session Volatility**: Data disappears on page refresh, browser crash, or Streamlit reload
2. **Memory Bloat**: Large datasets (e.g., MIMIC-III tables) consume RAM unnecessarily
3. **No Query Optimization**: Entire datasets loaded into memory for every query
4. **Broken Iterative Workflow**: Physicians cannot upload data once and return to it later
5. **Unreachable Optimizations**: Lazy Polars code paths never execute because data is eagerly loaded at upload

### Clinical Research Requirements
- **Reproducibility**: Analysis must be repeatable across sessions with identical results
- **Longitudinal Analysis**: Datasets span months/years; must support incremental loading
- **Trust**: Data loss mid-analysis destroys clinical confidence in the platform
- **HIPAA Compliance**: File-based storage avoids cloud vendor complications in MVP phase

### Technical Constraints
- Single-user MVP (no multi-tenancy yet)
- Development cycles are unpredictable (unknown when next work window occurs)
- Must enable lazy evaluation for large-than-memory datasets
- Need audit trail for clinical research validation

## Current State (As of 2025-01-XX)

### What's Implemented

**1. Basic Persistence (File-Based)**
- ✅ `UserDatasetStorage` class exists (`src/clinical_analytics/ui/storage/user_datasets.py`)
- ✅ CSV files stored in `data/uploads/raw/` (survives restarts)
- ✅ Metadata JSON in `data/uploads/metadata/` (schema, mappings, quality warnings)
- ✅ Upload tracking with unique `upload_id` (timestamp + hash)

**2. Semantic Layer Integration**
- ✅ `UploadedDataset` class integrates with registry system
- ✅ Semantic layer initializes DuckDB tables on-demand (in-memory, not persistent)
- ✅ Multi-table uploads: Individual tables saved to `{upload_id}_tables/` directory
- ✅ Multi-table uploads: All tables registered in DuckDB semantic layer

**3. Multi-Table Support**
- ✅ `save_zip_upload()` extracts and saves individual tables
- ✅ `MultiTableHandler` detects relationships and builds unified cohort
- ✅ Individual tables accessible via semantic layer

### What's Missing (ADR Vision)

**1. DuckDB-Backed Persistence**
- ❌ No persistent DuckDB database at `data/analytics.duckdb`
- ❌ No DataStore class managing centralized DuckDB connection
- ❌ Tables only exist in-memory during semantic layer initialization
- ❌ No ACID guarantees (file-based only)

**2. Parquet Export**
- ❌ No Parquet export for lazy Polars scanning
- ❌ All queries use CSV (slower, no columnar optimization)

**3. Lazy Evaluation**
- ❌ `get_upload_data()` returns pandas DataFrame (eager loading)
- ❌ No lazy Polars frames for query optimization
- ❌ Entire datasets loaded into memory for every query

**4. Conversation History**
- ❌ No JSONL conversation history
- ❌ No query/result persistence across sessions

**5. Session Recovery**
- ❌ No explicit recovery flow on app startup
- ❌ Datasets persist but not automatically restored to session

**6. Feature Parity Issue (Critical)**
- ⚠️ **Multi-table uploads**: Individual tables registered in DuckDB, saved to disk
- ⚠️ **Single-table uploads**: Only unified cohort CSV, no individual table persistence
- ⚠️ Both should have identical persistence and query capabilities

### Current Architecture

```
Single-Table Upload:
  CSV/Excel/SPSS → UserDatasetStorage.save_upload()
    → CSV saved to data/uploads/raw/{upload_id}.csv
    → Metadata JSON saved to data/uploads/metadata/{upload_id}.json
    → Semantic layer loads CSV on-demand (in-memory DuckDB)

Multi-Table Upload:
  ZIP → UserDatasetStorage.save_zip_upload()
    → Individual tables saved to data/uploads/raw/{upload_id}_tables/
    → Unified cohort CSV saved to data/uploads/raw/{upload_id}.csv
    → Metadata JSON with table list and relationships
    → Semantic layer registers all tables in DuckDB (in-memory)
```

### Gap Analysis

| Component | ADR Vision | Current State | Priority |
|-----------|------------|---------------|----------|
| **Feature Parity** | Both upload types identical | Multi-table richer than single | **P0 - Now** |
| **DuckDB Persistence** | Persistent tables with ACID | In-memory only | **P1 - Now** |
| **Lazy Evaluation** | Polars lazy frames | Pandas eager loading | **P1 - Now** |
| **Parquet Export** | Columnar format for scanning | CSV only | **P2 - Next** |
| **DataStore Class** | Centralized DuckDB manager | UserDatasetStorage (CSV-only) | **P1 - Now** |
| **Conversation History** | JSONL audit trail | Not implemented | **P2 - Next** |

## Decision

### Immediate Implementation (This Sprint)
Implement **DuckDB-backed persistent storage** with the following architecture:

```
CSV Upload → Polars Validation → DuckDB Table → Parquet Export → Lazy Polars Queries
                                       ↓
                                 Metadata JSON
                                       ↓
                              Conversation History (JSONL)
```

#### Core Components

**1. DataStore Class** (`semantic_layer/storage.py`)
- Manages DuckDB connection at `data/analytics.duckdb`
- Persists datasets as DuckDB tables (ACID guarantees)
- Exports to Parquet for Polars lazy scanning (columnar optimization)
- Stores metadata separately as JSON (schema versioning, audit trail)
- Appends conversation history as JSONL (lightweight, streaming-friendly)

**2. Modified Upload Flow**
```python
# Before: st.session_state.uploaded_data = df
# After:  store.save_dataset(df, dataset_name, metadata)
#         st.session_state.current_dataset = dataset_name  # Reference only
```

**3. Query Execution Pattern**
```python
# Load as lazy frame (deferred computation)
lazy_df = store.load_dataset(dataset_name)

# Apply filters (predicate pushdown in Polars)
filtered = lazy_df.filter(...)

# Materialize only final results
result = filtered.select(...).collect()
```

**4. Session Recovery**
- On app startup, detect existing datasets
- Offer to reload previous session data
- Restore conversation history from JSONL

### Deferred to Future (NOT in This Sprint)

**Ibis Integration** - Explicitly punting this decision for the following reasons:

#### Why We're NOT Using Ibis Now
1. **Single Backend**: We only use DuckDB; Ibis's multi-backend abstraction is overkill
2. **Polars Performance**: For in-memory ETL, Polars is faster than Ibis → DuckDB → Polars roundtrip
3. **Simple Semantic Layer**: Current needs (column aliasing, basic aggregations) don't require Ibis's expression DSL
4. **Learning Curve**: Adding Ibis now slows delivery without clear value
5. **Migration Path Exists**: Can add Ibis later without rewriting storage layer

#### When to Reconsider Ibis
- **Multi-table joins**: If semantic layer needs complex star schema queries
- **Pushdown Optimization**: When filtering large tables before Polars materialization
- **Cross-dialect needs**: If we add Postgres/BigQuery support
- **Metric definitions**: When we need reusable, composable analytical expressions

**Trigger for revisit:** When a single query requires >3 joins OR dataset size exceeds 10GB

## Migration Plan: Current State → ADR Vision

### Phase 0: Feature Parity (Priority 0 - Do First) ⚠️

**Goal**: Eliminate disparity between single-table and multi-table uploads. Both must be first-class citizens.

**Prerequisite**: This phase implements **[ADR 007: Feature Parity Architecture](ADR007.md)**, which establishes the core principle "Single-Table = Multi-Table with 1 Table" and defines the unified architecture.

**See ADR 007 for complete implementation details, code examples, and migration strategy.**

**High-Level Tasks** (detailed in ADR 007):
```
[ ] Normalize upload handling (ADR 007 Phase 1)
    - Create normalize_upload_to_table_list() function
    - Both save_upload() and save_zip_upload() use unified code path

[ ] Unify persistence (ADR 007 Phase 2)
    - Both upload types save individual tables to {upload_id}_tables/
    - Both use same metadata schema (inferred_schema format)

[ ] Unify semantic layer registration (ADR 007 Phase 3)
    - Both register all tables in DuckDB identically
    - Both support all granularity levels

[ ] Unify data access (ADR 007 Phase 4)
    - Both return Polars lazy frames (not pandas eager)
    - Both support predicate pushdown

[ ] Remove conditional logic (ADR 007 Phase 5)
    - Eliminate all upload-type conditionals
    - Single code path handles both upload types
```

**Success Criteria** (from ADR 007):
- Single-table and multi-table uploads have identical persistence structure
- Both upload types register all tables in DuckDB semantic layer
- Both upload types support lazy Polars evaluation
- Both upload types use same metadata schema (`inferred_schema` format)
- No conditional logic based on upload type exists in codebase
- Same query produces identical results for both upload types

### Phase 1: DuckDB-Backed Persistence (Priority 1 - Do Next)

**Goal**: Implement persistent DuckDB storage as specified in ADR vision.

**Tasks**:
```
[ ] Create DataStore class (or extend UserDatasetStorage)
    - Manage persistent DuckDB connection at data/analytics.duckdb
    - Provide save_dataset() and load_dataset() methods
    - Handle both single-table and multi-table uploads uniformly

[ ] Migrate from CSV-only to DuckDB + CSV hybrid
    - Save to DuckDB table (primary storage, ACID guarantees)
    - Keep CSV as export format (backward compatibility)
    - Update save_upload() and save_zip_upload() to use DataStore

[ ] Register all tables in persistent DuckDB
    - Single-table: Register unified cohort table
    - Multi-table: Register all individual tables
    - Ensure tables persist across app restarts

[ ] Add session recovery logic
    - On app startup, detect existing datasets in DuckDB
    - Restore dataset references to session state
    - Test that uploads survive app restart
```

**Success Criteria**:
- Datasets persist in DuckDB (not just CSV)
- Tables survive app restarts
- Both upload types use same persistence mechanism
- ACID guarantees for data integrity

### Phase 2: Lazy Evaluation & Parquet Export (Priority 1 - Do Next)

**Goal**: Enable lazy Polars evaluation with Parquet columnar storage.

**Tasks**:
```
[ ] Add Parquet export after DuckDB persistence
    - Export DuckDB tables to Parquet format
    - Store Parquet files alongside CSV (for lazy scanning)
    - Verify compression ratio (target: ≥40% smaller than CSV)

[ ] Implement lazy frame loading
    - load_dataset() returns Polars lazy frame (not pandas DataFrame)
    - Use Parquet for lazy scanning (columnar optimization)
    - Ensure predicate pushdown works correctly

[ ] Update query execution paths
    - Replace all pandas eager loading with Polars lazy frames
    - Update query_describe.py to use lazy frames
    - Update query_compare.py to use lazy frames
    - Verify lazy evaluation defers computation
```

**Success Criteria**:
- Queries use lazy Polars frames (not eager pandas)
- Parquet files are ≥40% smaller than CSV
- Predicate pushdown works correctly
- 1M+ row datasets query without OOM errors

### Phase 3: Conversation History (Priority 2 - Do After Core)

**Goal**: Add JSONL conversation history for reproducibility.

**Note**: This coordinates with ADR003 Phase 1 (Trust Layer) - conversation history provides audit trail for verification UI.

**Tasks**:
```
[ ] Append queries + results to JSONL on execution
    - Store query text, parameters, and results
    - Use JSONL format (streaming-friendly, append-only)
    - Store in data/uploads/conversations/{upload_id}.jsonl

[ ] Load history on app startup
    - Restore conversation history for active datasets
    - Display in UI for reproducibility

[ ] Add "Clear history" button for testing
    - Allow users to reset conversation history
    - Useful for testing and privacy
```

**Additional Consideration**: ADR003 Phase 3 (Adaptive Dictionary) will also need persistence for user-added alias mappings. These should be stored in dataset metadata JSON (not separate files) to align with this ADR's persistence architecture.

**Success Criteria**:
- Conversation history persists across restarts
- All queries and results are logged
- History is accessible in UI

## Implementation Plan

**Note**: This plan has been updated to reflect the migration path from current state to ADR vision. See "Migration Plan" section above for detailed phased approach.

### Phase 0: Feature Parity (2 hours) - Do First
```
[ ] Ensure single-table uploads register in DuckDB semantic layer
[ ] Unify storage format for both upload types
[ ] Replace eager pandas loading with lazy Polars
[ ] Test feature parity between single-table and multi-table
```

### Phase 1: DuckDB-Backed Persistence (4 hours)
```
[ ] Create DataStore class (or extend UserDatasetStorage)
[ ] Migrate from CSV-only to DuckDB + CSV hybrid
[ ] Register all tables in persistent DuckDB
[ ] Add session recovery logic to app.py
[ ] Write tests/test_storage.py for persistence verification
```

### Phase 2: Lazy Evaluation & Parquet Export (3 hours)
```
[ ] Add Parquet export after DuckDB persistence
[ ] Implement lazy frame loading (replace pandas eager)
[ ] Update query_describe.py to use lazy frames
[ ] Update query_compare.py to use lazy frames
[ ] Verify lazy frame execution with predicate pushdown
```

### Phase 3: Conversation History (1 hour)
```
[ ] Append queries + results to JSONL on execution
[ ] Load history on app startup
[ ] Add "Clear history" button for testing
```

### Phase 4: Validation (1 hour)
```
[ ] Test upload → refresh → data persists in DuckDB
[ ] Test conversation history survives restart
[ ] Verify Parquet file compression vs CSV size (≥40% reduction)
[ ] Confirm lazy evaluation with 1M row test dataset (no OOM)
[ ] Verify feature parity: single-table and multi-table identical
```

## Consequences

### Positive
- **Persistence**: Data survives browser crashes, Streamlit reloads, session timeouts
- **Performance**: Lazy evaluation enables processing datasets larger than RAM
- **Storage Efficiency**: Parquet compression reduces disk usage ~60% vs CSV
- **Reproducibility**: JSONL conversation history provides audit trail
- **Future-Proof**: Can add Ibis later without storage layer rewrite
- **HIPAA-Friendly**: Local file-based storage avoids cloud vendor contracts

### Negative
- **Disk Usage**: Stores data in both DuckDB and Parquet (trade-off for lazy scanning)
- **Complexity**: Two storage formats (DuckDB + Parquet) instead of one
- **Migration Path**: If we add Ibis, need to refactor query execution (not storage)
- **No Concurrent Users**: DuckDB file locking limits to single user (acceptable for MVP)

### Risks & Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| DuckDB file corruption | Data loss | Add backup exports to Parquet on dataset save |
| Parquet/DuckDB schema drift | Query failures | Store schema version in metadata JSON; validate on load |
| Conversation history bloat | Slow app startup | Implement history rotation (keep last 50 entries) |
| Lazy frame bugs | Incorrect results | Add integration tests comparing eager vs lazy execution |

### NOT Addressed (Future ADRs)
- Multi-user support (requires PostgreSQL or row-level locking)
- Cloud deployment (S3/GCS storage backends)
- Real-time collaboration (operational transforms)
- Dimensional modeling (star schemas, conformed dimensions)

## Alternatives Considered

### 1. Keep Session-Only State
**Rejected**: Breaks clinical workflow; data loss destroys trust

### 2. Direct Parquet-Only Storage
**Rejected**: No ACID guarantees; complex for metadata management; can't execute SQL queries

### 3. Ibis + DuckDB + Parquet Now
**Rejected**: Adds complexity without clear value; can add Ibis later when needs justify it

### 4. PostgreSQL Instead of DuckDB
**Rejected**: Requires server management; overkill for single-user MVP; HIPAA complications

### 5. SQLite Instead of DuckDB
**Rejected**: Lacks analytical SQL functions (percentiles, window functions); slower for OLAP queries

## Success Metrics

### Must-Have (Go/No-Go)
- [ ] Upload 100MB dataset, refresh page → data reloads in <2 seconds
- [ ] Run 5 queries, restart app → conversation history shows all 5
- [ ] Test with 1M rows → queries execute without OOM errors
- [ ] Parquet files are ≥40% smaller than source CSV

### Nice-to-Have
- [ ] Lazy evaluation visibly defers computation (add logging to verify)
- [ ] DuckDB analytical functions (percentile_cont) work correctly
- [ ] Metadata JSON enables schema versioning (not used yet, but structure exists)

## References

### Technical Documentation
- [DuckDB SQL Documentation](https://duckdb.org/docs/sql/introduction)
- [Polars Lazy API](https://pola-rs.github.io/polars/user-guide/lazy/using/)
- [Ibis Documentation](https://ibis-project.org/) - For future reference

### Related Decisions
- **ADR 007**: Feature Parity Architecture - **PREREQUISITE** for Phase 1+. ADR 007 must be completed before Phase 1+ can proceed, as it establishes unified architecture that Phase 1+ builds upon.
- **[ADR001: Fix Comparison Analysis, Implement Filtering, and Conversational UI](../ADR/ADR001.md)**: Requires feature parity (ADR 007) as prerequisite. Filter extraction and conversational UI must use persistent storage patterns defined here.
- **[ADR003: Clinical Trust Architecture & Adaptive Terminology](../ADR/ADR003.md)**: Adaptive dictionary mappings (Phase 3) should be persisted in dataset metadata JSON (per this ADR's persistence layer). Conversation history (Phase 3 of this ADR) provides audit trail for ADR003's verification UI.
- **ADR 004 (Future)**: When to adopt Ibis for semantic layer
- **ADR 005 (Future)**: Multi-table join strategy
- **ADR 006 (Future)**: Cloud deployment storage backends

### Code References
- `src/clinical_analytics/ui/storage/user_datasets.py` - Current UserDatasetStorage implementation
- `src/clinical_analytics/datasets/uploaded/definition.py` - UploadedDataset class (semantic layer integration)
- `semantic_layer/storage.py` - DataStore implementation (to be created)
- `tests/test_storage.py` - Persistence validation tests (to be created)

## Decision Makers
- **Jason** (Technical Lead): Approved architecture
- **Clinical Stakeholder** (Infectious Disease Physician): Validated workflow requirements

## Review Date
**2025-01-15** - After 2 weeks of production use, evaluate:
1. Are there performance issues with lazy evaluation?
2. Has conversation history proven useful for reproducibility?
3. Do we need Ibis for any emerging query patterns?
4. Should we migrate to cloud storage or stay file-based?

---

## Appendix A: Ibis Decision Criteria (Future Reference)

We will reconsider Ibis when **any** of these conditions are met:

### Performance Triggers
- [ ] Query execution time >5 seconds for common queries
- [ ] Need to filter 10GB+ datasets before loading into Polars
- [ ] Memory usage >80% during query execution

### Complexity Triggers
- [ ] Semantic layer has >5 reusable metric definitions
- [ ] Need to support 3+ table joins in single query
- [ ] Cross-dialect support required (Postgres, BigQuery, etc.)

### Maintenance Triggers
- [ ] Custom Polars query logic duplicated >3 times
- [ ] Schema evolution requires manual query rewrites
- [ ] Users request SQL export of queries (Ibis compiles to SQL)

**When triggered, create ADR 002 with concrete Ibis migration plan.**

## Appendix B: Storage Format Comparison

| Format | Read Speed | Write Speed | Compression | Query Pushdown | ACID |
|--------|-----------|-------------|-------------|----------------|------|
| **Session State** | Instant | Instant | None | N/A | No |
| **CSV** | Slow | Fast | None | No | No |
| **Parquet** | Fast | Moderate | High (~60%) | Yes (Polars) | No |
| **DuckDB** | Fast | Fast | Moderate (~40%) | Yes (SQL) | Yes |
| **Our Hybrid** | Fast | Moderate | High (Parquet) | Yes (both) | Yes (DuckDB) |

**Rationale for hybrid approach:** DuckDB provides ACID and SQL analytics; Parquet enables Polars lazy optimization. Disk usage trade-off is acceptable for clinical research workflow benefits.
```

---

## ADR Created

This ADR documents:

1. **Why we're adding persistence NOW** (session state is broken)
2. **Why we're using DuckDB + Parquet** (not alternatives)
3. **Why we're NOT using Ibis yet** (explicit deferral with criteria for revisit)
4. **Concrete implementation plan** (4 phases, 8 hours total)
5. **Success metrics** (testable go/no-go criteria)
6. **Review trigger** (2 weeks, reassess Ibis decision)

The ADR makes it clear this is **not** punting on architectural decisions—it's choosing the simplest thing that solves the immediate problem while leaving the door open for Ibis when complexity justifies it.
