# ADR 002: Persistent Storage Layer with DuckDB and Deferred Ibis Integration

## Status
**ACCEPTED** - 2024-12-29
**IN PROGRESS** - Partial implementation complete; migration to full vision in progress

## Context

The MD Data Explorer currently stores all uploaded clinical datasets and analysis results in Streamlit session state (in-memory only). This creates critical usability and scalability issues:

### Problems with Current Architecture
1. **Session Volatility**: Data disappears on page refresh, browser crash, or Streamlit reload
2. **Memory Bloat**: Large datasets (e.g., MIMIC-III tables) consume RAM unnecessarily
3. **No Query Optimization**: Entire datasets loaded into memory for every query
4. **Broken Iterative Workflow**: Physicians cannot upload data once and return to it later
5. **Unreachable Optimizations**: Lazy Polars code paths never execute because data is eagerly loaded at upload

### Clinical Research Requirements
- **Reproducibility**: Analysis must be repeatable across sessions with identical results
- **Longitudinal Analysis**: Datasets span months/years; must support incremental loading
- **Trust**: Data loss mid-analysis destroys clinical confidence in the platform
- **HIPAA Compliance**: File-based storage avoids cloud vendor complications in MVP phase

### Technical Constraints
- Single-user MVP (no multi-tenancy yet)
- Development cycles are unpredictable (unknown when next work window occurs)
- Must enable lazy evaluation for large-than-memory datasets
- Need audit trail for clinical research validation

## Current State (As of 2025-01-XX)

### What's Implemented

**1. Basic Persistence (File-Based)**
- ✅ `UserDatasetStorage` class exists (`src/clinical_analytics/ui/storage/user_datasets.py`)
- ✅ CSV files stored in `data/uploads/raw/` (survives restarts)
- ✅ Metadata JSON in `data/uploads/metadata/` (schema, mappings, quality warnings)
- ✅ Upload tracking with unique `upload_id` (timestamp + hash)

**2. Semantic Layer Integration**
- ✅ `UploadedDataset` class integrates with registry system
- ✅ Semantic layer initializes DuckDB tables on-demand (in-memory, not persistent)
- ✅ Multi-table uploads: Individual tables saved to `{upload_id}_tables/` directory
- ✅ Multi-table uploads: All tables registered in DuckDB semantic layer

**3. Multi-Table Support**
- ✅ `save_zip_upload()` extracts and saves individual tables
- ✅ `MultiTableHandler` detects relationships and builds unified cohort
- ✅ Individual tables accessible via semantic layer

### What's Missing (ADR Vision)

**1. DuckDB-Backed Persistence**
- ❌ No persistent DuckDB database at `data/analytics.duckdb`
- ❌ No DataStore class managing centralized DuckDB connection
- ❌ Tables only exist in-memory during semantic layer initialization
- ❌ No ACID guarantees (file-based only)

**2. Parquet Export**
- ❌ No Parquet export for lazy Polars scanning
- ❌ All queries use CSV (slower, no columnar optimization)

**3. Lazy Evaluation**
- ❌ `get_upload_data()` returns pandas DataFrame (eager loading)
- ❌ No lazy Polars frames for query optimization
- ❌ Entire datasets loaded into memory for every query

**4. Conversation History**
- ❌ No JSONL conversation history
- ❌ No query/result persistence across sessions

**5. Session Recovery**
- ❌ No explicit recovery flow on app startup
- ❌ Datasets persist but not automatically restored to session

**6. Feature Parity Issue (Critical)**
- ⚠️ **Multi-table uploads**: Individual tables registered in DuckDB, saved to disk
- ⚠️ **Single-table uploads (legacy)**: Only unified cohort CSV, no individual table persistence, no individual table registration in DuckDB
- ⚠️ **Single-table uploads (new, after ADR007)**: Should have individual table persistence, but may still have gaps in semantic layer registration
- ⚠️ **Both upload types**: Get semantic layer created, but individual table registrations depend on metadata and directory structure
- ⚠️ Both should have identical persistence and query capabilities

**Impact on Query Failures**: Legacy single-table uploads missing individual table registrations in DuckDB directly contributes to the NL query engine failures documented in ADR001. Without proper table registration, the semantic layer has limited metadata, causing variable matching to fail and leading to wrong variable selection and incorrect query results. This feature parity gap is not just a storage issue - it's a query capability issue that breaks basic functionality.

**7. Dataset Versioning (Missing)**
- ❌ No content-based versioning (re-upload same data creates duplicate storage)
- ❌ No immutable dataset identity (can't track query results to specific dataset version)
- ❌ No idempotent query execution (same query on same data runs multiple times)

### Current Architecture

```
Single-Table Upload (Legacy):
  CSV/Excel/SPSS → UserDatasetStorage.save_upload()
    → CSV saved to data/uploads/raw/{upload_id}.csv
    → Metadata JSON saved to data/uploads/metadata/{upload_id}.json
    → Semantic layer loads CSV on-demand (in-memory DuckDB)
    → ❌ Missing: Individual table registration in DuckDB (no tables in metadata)
    → ❌ Missing: {upload_id}_tables/ directory (legacy format)
    → ⚠️ Semantic layer exists but has limited metadata (cohort only, not individual tables)

Single-Table Upload (New, after ADR007):
  CSV/Excel/SPSS → normalize_upload_to_table_list() → save_table_list()
    → Table saved to data/uploads/raw/{upload_id}_tables/table_0.csv
    → Unified cohort CSV saved to data/uploads/raw/{upload_id}.csv
    → Metadata JSON with tables list and inferred_schema
    → Semantic layer registers table in DuckDB (if tables in metadata and directory exists)

Multi-Table Upload:
  ZIP → UserDatasetStorage.save_zip_upload()
    → Individual tables saved to data/uploads/raw/{upload_id}_tables/
    → Unified cohort CSV saved to data/uploads/raw/{upload_id}.csv
    → Metadata JSON with table list and relationships
    → Semantic layer registers all tables in DuckDB (in-memory)
```

### Gap Analysis

| Component | ADR Vision | Current State | Priority |
|-----------|------------|---------------|----------|
| **Feature Parity** | Both upload types identical | Multi-table richer than single | **P0 - Now** |
| **DuckDB Persistence** | Persistent tables with ACID | In-memory only | **P1 - Now** |
| **Lazy Evaluation** | Polars lazy frames | Pandas eager loading | **P1 - Now** |
| **Parquet Export** | Columnar format for scanning | CSV only | **P2 - Next** |
| **DataStore Class** | Centralized DuckDB manager | UserDatasetStorage (CSV-only) | **P1 - Now** |
| **Conversation History** | JSONL audit trail | Not implemented | **P2 - Next** |

## Decision

### Dataset Versioning Contract

**ADR002 owns dataset identity and versioning** - this is the foundation for idempotent query execution and result caching.

**Versioning Rules**:
- `upload_id` = immutable storage key (timestamp + hash, never changes)
- `dataset_version` = content hash of canonicalized tables (or table hashes aggregated)
- Re-upload behavior:
  - If same content hash: reuse `dataset_version`, don't duplicate tables/artifacts
  - If different: write new versioned directory
- Query execution uses `(upload_id, dataset_version)` for idempotent run keys

**Implementation**:
```python
def compute_dataset_version(tables: list[pl.DataFrame]) -> str:
    """Compute content hash of canonicalized tables."""
    hashes = []
    for table in tables:
        # Canonicalize: sort rows, normalize column order
        canonical = table.sort(table.columns[0]).select(sorted(table.columns))
        table_hash = hashlib.sha256(canonical.write_parquet()).hexdigest()[:16]
        hashes.append(table_hash)
    return hashlib.sha256("|".join(hashes).encode()).hexdigest()[:16]
```

### Metadata JSON Schema (Authoritative)

**ADR002 owns the metadata JSON schema** - this is where adaptive aliases (ADR003), semantic configs, and provenance live.

**Minimum Schema**:
```json
{
  "upload_id": "20251228_163830_28450494",
  "dataset_version": "a1b2c3d4e5f6g7h8",
  "created_at": "2025-12-28T16:38:30Z",
  "source_files": ["data.csv"],
  "provenance": {
    "upload_type": "single" | "multi",
    "tables": [
      {"name": "table_0", "row_count": 1000, "fingerprint": "abc123"}
    ]
  },
  "schema": {
    "inferred_schema": {
      "table_0": {
        "columns": {
          "patient_id": {"type": "Utf8", "is_identifier": true},
          "LDL mg/dL": {"type": "Float64", "units": "mg/dL"}
        }
      }
    }
  },
  "alias_mappings": {
    "user_aliases": {
      "VL": "viral_load",
      "LDL": "LDL mg/dL"
    },
    "system_aliases": {
      "viral_load": ["VL", "viral load", "vload"]
    }
  },
  "semantic_config": {
    "base_view": "patient_level",
    "metrics": [...],
    "dimensions": [...]
  },
  "query_history": []  // Optional: lightweight query plan + result metadata
}
```

**Alias Scope Rules** (prevents "poison" aliases):
- Aliases scoped to `(upload_id, dataset_version)` - not global
- If schema changes and target column missing: mark alias orphaned and ignore
- Prevents aliases from breaking other datasets

### Immediate Implementation (This Sprint)
Implement **DuckDB-backed persistent storage** with the following architecture:

```
CSV Upload → Polars Validation → DuckDB Table → Parquet Export → Lazy Polars Queries
                                       ↓
                                 Metadata JSON
                                       ↓
                              Conversation History (JSONL)
```

#### Core Components

**1. DataStore Class** (`semantic_layer/storage.py`)
- Manages DuckDB connection at `data/analytics.duckdb`
- Persists datasets as DuckDB tables (ACID guarantees)
- Exports to Parquet for Polars lazy scanning (columnar optimization)
- Stores metadata separately as JSON (schema versioning, audit trail)
- Appends conversation history as JSONL (lightweight, streaming-friendly)

**2. Modified Upload Flow**
```python
# Before: st.session_state.uploaded_data = df
# After:  store.save_dataset(df, dataset_name, metadata)
#         st.session_state.current_dataset = dataset_name  # Reference only
```

**3. Query Execution Pattern**
```python
# Load as lazy frame (deferred computation)
lazy_df = store.load_dataset(dataset_name)

# Apply filters (predicate pushdown in Polars)
filtered = lazy_df.filter(...)

# Materialize only final results
result = filtered.select(...).collect()
```

**4. Session Recovery**
- On app startup, detect existing datasets
- Offer to reload previous session data
- Restore conversation history from JSONL

**5. Artifacts and Result Persistence (Optional)**
- Store lightweight query metadata: `QueryPlan` JSON + generated SQL + row_count
- Do NOT store patient-level exports by default (only on explicit download)
- Supports trust + audit (ADR003) without inflating memory/session_state
- Format: `query_history` array in metadata JSON (optional field)

### Deferred to Future (NOT in This Sprint)

**Ibis Integration** - Explicitly punting this decision for the following reasons:

#### Why We're NOT Using Ibis Now
1. **Single Backend**: We only use DuckDB; Ibis's multi-backend abstraction is overkill
2. **Polars Performance**: For in-memory ETL, Polars is faster than Ibis → DuckDB → Polars roundtrip
3. **Simple Semantic Layer**: Current needs (column aliasing, basic aggregations) don't require Ibis's expression DSL
4. **Learning Curve**: Adding Ibis now slows delivery without clear value
5. **Migration Path Exists**: Can add Ibis later without rewriting storage layer

#### When to Reconsider Ibis
- **Multi-table joins**: If semantic layer needs complex star schema queries
- **Pushdown Optimization**: When filtering large tables before Polars materialization
- **Cross-dialect needs**: If we add Postgres/BigQuery support
- **Metric definitions**: When we need reusable, composable analytical expressions

**Trigger for revisit:** When a single query requires >3 joins OR dataset size exceeds 10GB

## Migration Plan: Current State → ADR Vision

### Phase 0: Feature Parity (Priority 0 - Do First) ⚠️

**Goal**: Eliminate disparity between single-table and multi-table uploads. Both must be first-class citizens.

**Prerequisite**: This phase implements **[ADR 007: Feature Parity Architecture](ADR007.md)**, which establishes the core principle "Single-Table = Multi-Table with 1 Table" and defines the unified architecture.

**See ADR 007 for complete implementation details, code examples, and migration strategy.**

**High-Level Tasks** (detailed in ADR 007):
```
[ ] Normalize upload handling (ADR 007 Phase 1)
    - Create normalize_upload_to_table_list() function
    - Both save_upload() and save_zip_upload() use unified code path

[ ] Unify persistence (ADR 007 Phase 2)
    - Both upload types save individual tables to {upload_id}_tables/
    - Both use same metadata schema (inferred_schema format)

[ ] Unify semantic layer registration (ADR 007 Phase 3)
    - Both register all tables in DuckDB identically
    - Both support all granularity levels

[ ] Unify data access (ADR 007 Phase 4)
    - Both return Polars lazy frames (not pandas eager)
    - Both support predicate pushdown

[ ] Remove conditional logic (ADR 007 Phase 5)
    - Eliminate all upload-type conditionals
    - Single code path handles both upload types
```

**Success Criteria** (from ADR 007):
- Single-table and multi-table uploads have identical persistence structure
- Both upload types register all tables in DuckDB semantic layer
- Both upload types support lazy Polars evaluation
- Both upload types use same metadata schema (`inferred_schema` format)
- No conditional logic based on upload type exists in codebase
- Same query produces identical results for both upload types

### Phase 1: DuckDB-Backed Persistence (Priority 1 - Do Next)

**Goal**: Implement persistent DuckDB storage as specified in ADR vision.

**Tasks**:
```
[ ] Create DataStore class (or extend UserDatasetStorage)
    - Manage persistent DuckDB connection at data/analytics.duckdb
    - Provide save_dataset() and load_dataset() methods
    - Handle both single-table and multi-table uploads uniformly

[ ] Migrate from CSV-only to DuckDB + CSV hybrid
    - Save to DuckDB table (primary storage, ACID guarantees)
    - Keep CSV as export format (backward compatibility)
    - Update save_upload() and save_zip_upload() to use DataStore

[ ] Register all tables in persistent DuckDB
    - Single-table: Register unified cohort table
    - Multi-table: Register all individual tables
    - Ensure tables persist across app restarts

[ ] Add session recovery logic
    - On app startup, detect existing datasets in DuckDB
    - Restore dataset references to session state
    - Test that uploads survive app restart
```

**Success Criteria**:
- Datasets persist in DuckDB (not just CSV)
- Tables survive app restarts
- Both upload types use same persistence mechanism
- ACID guarantees for data integrity

### Phase 2: Lazy Evaluation & Parquet Export (Priority 1 - Do Next)

**Goal**: Enable lazy Polars evaluation with Parquet columnar storage.

**Tasks**:
```
[ ] Add Parquet export after DuckDB persistence
    - Export DuckDB tables to Parquet format
    - Store Parquet files alongside CSV (for lazy scanning)
    - Verify compression ratio (target: ≥40% smaller than CSV)

[ ] Implement lazy frame loading
    - load_dataset() returns Polars lazy frame (not pandas DataFrame)
    - Use Parquet for lazy scanning (columnar optimization)
    - Ensure predicate pushdown works correctly

[ ] Update query execution paths
    - Replace all pandas eager loading with Polars lazy frames
    - Update query_describe.py to use lazy frames
    - Update query_compare.py to use lazy frames
    - Verify lazy evaluation defers computation
```

**Success Criteria**:
- Queries use lazy Polars frames (not eager pandas)
- Parquet files are ≥40% smaller than CSV
- Predicate pushdown works correctly
- 1M+ row datasets query without OOM errors

### Phase 3: Conversation History (Priority 2 - Do After Core)

**Goal**: Add JSONL conversation history for reproducibility.

**Note**: This coordinates with ADR003 Phase 1 (Trust Layer) - conversation history provides audit trail for verification UI.

**Findings from ADR001 Implementation (2025-12-30):**

During ADR001 implementation, we identified comprehensive logging requirements that should be captured in the JSONL logger:

**Current Logging State (via structlog):**
- ✅ Query parsing events logged (`query_parse_start`, `query_parse_success`, `query_parse_failure`)
- ✅ Filter extraction logged (`filters_extracted`, `filters_extracted_in_parse`)
- ✅ Execution events logged (`analysis_execution_start`, `analysis_computation_complete`)
- ✅ Result metadata logged (`analysis_result_stored`, `confidence_displayed`)
- ❌ **Missing**: Persistent JSONL file logging (only in-memory structlog)
- ❌ **Missing**: Query result metadata (row counts, execution time, SQL generated)
- ❌ **Missing**: Parsing logic details (which tier succeeded, why confidence is what it is)
- ❌ **Missing**: Cross-session persistence (logs lost on restart)
- ❌ **Missing**: Follow-up suggestion interactions

**Required Log Entry Structure:**
```json
{
  "timestamp": "2025-12-30T10:30:45.123456Z",
  "session_id": "streamlit_session_abc123",
  "query": {
    "text": "how many patients were on statins and which statin was most prescribed?",
    "intent": "COUNT",
    "confidence": 0.90,
    "parsing_tier": "pattern_match",
    "parsing_attempts": [{"tier": "pattern_match", "result": "success", "confidence": 0.90}],
    "query_plan": {
      "intent": "COUNT",
      "metric": null,
      "group_by": "statin_used",
      "filters": [{"column": "statin_prescribed", "operator": "==", "value": 1, "exclude_nulls": true}],
      "confidence": 0.90,
      "explanation": "Counting patients on statins, grouped by statin type",
      "run_key": "abc123def456..."
    }
  },
  "execution": {
    "run_key": "abc123def456...",
    "dataset_version": "user_upload_20251229_225650_45c58677",
    "execution_time_ms": 234,
    "cohort_shape": [807, 45],
    "filters_applied": true,
    "filtered_count": 807,
    "original_count": 807
  },
  "result": {
    "type": "count",
    "headline": "807 patients",
    "total_count": 807,
    "grouped_by": "statin_used",
    "grouped_counts": {"0": 327, "1": 278, "2": 152, "3": 41, "4": 2, "5": 7}
  },
  "follow_ups": {
    "suggestions_shown": ["Filter Statin Used... and count again", "Break down the count by a grouping variable"],
    "suggestion_clicked": null
  }
}
```

**What Should Be Logged:**
1. **Query Parsing**: Original query text, parsed intent, confidence, parsing tier, all parsing attempts, extracted variables, full QueryPlan JSON
2. **Execution**: Run key, dataset version, execution time, cohort shape, filters applied, filtered vs original counts
3. **Results**: Result type, headline/summary, key metrics, result metadata
4. **UI Interactions**: Follow-up suggestions shown, follow-up suggestion clicked, confidence display shown
5. **Errors**: Parsing failures, execution errors, variable selection fallbacks

**Implementation Notes:**
- Lightweight design: JSONL format (one JSON object per line, append-only)
- Stream-friendly: Can tail logs in real-time
- No full result data: Only metadata (results stored separately via run_key)
- Compress old logs (>90 days) to save space
- Async logging: Don't block UI on file I/O
- Batch writes: Flush every N entries or every T seconds
- Privacy: No PHI in logs (only metadata, counts, column names)

**Integration Points:**
- Hook into `execute_analysis_with_idempotency()` to log execution
- Hook into `NLQueryEngine.parse_query()` to log parsing
- Hook into `_suggest_follow_ups()` to log suggestions
- Hook into conversation history to log UI interactions

**Tasks**:
```
[ ] Append queries + results to JSONL on execution
    - Store query text, parameters, and results
    - Use JSONL format (streaming-friendly, append-only)
    - Store in data/uploads/conversations/{upload_id}.jsonl
    - Include full QueryPlan JSON, parsing attempts, execution metadata

[ ] Load history on app startup
    - Restore conversation history for active datasets
    - Display in UI for reproducibility

[ ] Add "Clear history" button for testing
    - Allow users to reset conversation history
    - Useful for testing and privacy

[ ] Implement QueryLogger class
    - New module: src/clinical_analytics/storage/query_logger.py
    - Methods: log_query(), log_execution(), log_result(), log_follow_up()
    - Handles file rotation, compression, async writes

[ ] Log follow-up suggestion interactions
    - Track which suggestions were shown
    - Track which suggestions were clicked
    - Store in follow_ups section of log entry
```

**Additional Consideration**: ADR003 Phase 3 (Adaptive Dictionary) will also need persistence for user-added alias mappings. These should be stored in dataset metadata JSON (not separate files) to align with this ADR's persistence architecture.

**Success Criteria**:
- Conversation history persists across restarts
- All queries and results are logged
- History is accessible in UI

## Implementation Plan

**Note**: This plan has been updated to reflect the migration path from current state to ADR vision. See "Migration Plan" section above for detailed phased approach.

### Phase 0: Feature Parity (2 hours) - Do First
```
[ ] Ensure single-table uploads register in DuckDB semantic layer
[ ] Unify storage format for both upload types
[ ] Replace eager pandas loading with lazy Polars
[ ] Test feature parity between single-table and multi-table
```

### Phase 1: DuckDB-Backed Persistence (4 hours)
```
[ ] Create DataStore class (or extend UserDatasetStorage)
[ ] Migrate from CSV-only to DuckDB + CSV hybrid
[ ] Register all tables in persistent DuckDB
[ ] Add session recovery logic to app.py
[ ] Write tests/test_storage.py for persistence verification
```

### Phase 2: Lazy Evaluation & Parquet Export (3 hours)
```
[ ] Add Parquet export after DuckDB persistence
[ ] Implement lazy frame loading (replace pandas eager)
[ ] Update query_describe.py to use lazy frames
[ ] Update query_compare.py to use lazy frames
[ ] Verify lazy frame execution with predicate pushdown
```

### Phase 3: Conversation History (1 hour)
```
[ ] Append queries + results to JSONL on execution
[ ] Load history on app startup
[ ] Add "Clear history" button for testing
```

### Phase 4: Validation (1 hour)
```
[ ] Test upload → refresh → data persists in DuckDB
[ ] Test conversation history survives restart
[ ] Verify Parquet file compression vs CSV size (≥40% reduction)
[ ] Confirm lazy evaluation with 1M row test dataset (no OOM)
[ ] Verify feature parity: single-table and multi-table identical
```

## Consequences

### Positive
- **Persistence**: Data survives browser crashes, Streamlit reloads, session timeouts
- **Performance**: Lazy evaluation enables processing datasets larger than RAM
- **Storage Efficiency**: Parquet compression reduces disk usage ~60% vs CSV
- **Reproducibility**: JSONL conversation history provides audit trail
- **Future-Proof**: Can add Ibis later without storage layer rewrite
- **HIPAA-Friendly**: Local file-based storage avoids cloud vendor contracts

### Negative
- **Disk Usage**: Stores data in both DuckDB and Parquet (trade-off for lazy scanning)
- **Complexity**: Two storage formats (DuckDB + Parquet) instead of one
- **Migration Path**: If we add Ibis, need to refactor query execution (not storage)
- **No Concurrent Users**: DuckDB file locking limits to single user (acceptable for MVP)

### Risks & Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| DuckDB file corruption | Data loss | Add backup exports to Parquet on dataset save |
| Parquet/DuckDB schema drift | Query failures | Store schema version in metadata JSON; validate on load |
| Conversation history bloat | Slow app startup | Implement history rotation (keep last 50 entries) |
| Lazy frame bugs | Incorrect results | Add integration tests comparing eager vs lazy execution |

### NOT Addressed (Future ADRs)
- Multi-user support (requires PostgreSQL or row-level locking)
- Cloud deployment (S3/GCS storage backends)
- Real-time collaboration (operational transforms)
- Dimensional modeling (star schemas, conformed dimensions)

## Alternatives Considered

### 1. Keep Session-Only State
**Rejected**: Breaks clinical workflow; data loss destroys trust

### 2. Direct Parquet-Only Storage
**Rejected**: No ACID guarantees; complex for metadata management; can't execute SQL queries

### 3. Ibis + DuckDB + Parquet Now
**Rejected**: Adds complexity without clear value; can add Ibis later when needs justify it

### 4. PostgreSQL Instead of DuckDB
**Rejected**: Requires server management; overkill for single-user MVP; HIPAA complications

### 5. SQLite Instead of DuckDB
**Rejected**: Lacks analytical SQL functions (percentiles, window functions); slower for OLAP queries

## Success Metrics

### Must-Have (Go/No-Go)
- [ ] Upload 100MB dataset, refresh page → data reloads in <2 seconds
- [ ] Run 5 queries, restart app → conversation history shows all 5
- [ ] Test with 1M rows → queries execute without OOM errors
- [ ] Parquet files are ≥40% smaller than source CSV

### Nice-to-Have
- [ ] Lazy evaluation visibly defers computation (add logging to verify)
- [ ] DuckDB analytical functions (percentile_cont) work correctly
- [ ] Metadata JSON enables schema versioning (not used yet, but structure exists)

## References

### Technical Documentation
- [DuckDB SQL Documentation](https://duckdb.org/docs/sql/introduction)
- [Polars Lazy API](https://pola-rs.github.io/polars/user-guide/lazy/using/)
- [Ibis Documentation](https://ibis-project.org/) - For future reference

### Related Decisions
- **ADR 007**: Feature Parity Architecture - **PREREQUISITE** for Phase 1+. ADR 007 must be completed before Phase 1+ can proceed, as it establishes unified architecture that Phase 1+ builds upon.
- **[ADR001: Query Plan Producer, Filtering, and Chat-First Execution Rules](../ADR/ADR001.md)**:
  - ADR001 depends on ADR002 for `dataset_version` (for idempotent run keys)
  - ADR001 depends on ADR002 for metadata read/write (QueryPlan persistence)
  - ADR001 does NOT depend on ADR003 (trust UI is separate)
- **[ADR003: Clinical Trust Protocol + Adaptive Alias Persistence](../ADR/ADR003.md)**:
  - ADR003 depends on ADR002 for storing alias mappings in metadata JSON (per `(upload_id, dataset_version)` scope)
  - ADR003 depends on ADR002 for audit metadata (query history, provenance)
  - ADR003 does NOT own NLU parsing (that's ADR001)
- **ADR 004 (Future)**: When to adopt Ibis for semantic layer
- **ADR 005 (Future)**: Multi-table join strategy
- **ADR 006 (Future)**: Cloud deployment storage backends

### Code References
- `src/clinical_analytics/ui/storage/user_datasets.py` - Current UserDatasetStorage implementation
- `src/clinical_analytics/datasets/uploaded/definition.py` - UploadedDataset class (semantic layer integration)
- `semantic_layer/storage.py` - DataStore implementation (to be created)
- `tests/test_storage.py` - Persistence validation tests (to be created)

## Decision Makers
- **Jason** (Technical Lead): Approved architecture
- **Clinical Stakeholder** (Infectious Disease Physician): Validated workflow requirements

## Review Date
**2025-01-15** - After 2 weeks of production use, evaluate:
1. Are there performance issues with lazy evaluation?
2. Has conversation history proven useful for reproducibility?
3. Do we need Ibis for any emerging query patterns?
4. Should we migrate to cloud storage or stay file-based?

---

## Appendix A: Ibis Decision Criteria (Future Reference)

We will reconsider Ibis when **any** of these conditions are met:

### Performance Triggers
- [ ] Query execution time >5 seconds for common queries
- [ ] Need to filter 10GB+ datasets before loading into Polars
- [ ] Memory usage >80% during query execution

### Complexity Triggers
- [ ] Semantic layer has >5 reusable metric definitions
- [ ] Need to support 3+ table joins in single query
- [ ] Cross-dialect support required (Postgres, BigQuery, etc.)

### Maintenance Triggers
- [ ] Custom Polars query logic duplicated >3 times
- [ ] Schema evolution requires manual query rewrites
- [ ] Users request SQL export of queries (Ibis compiles to SQL)

**When triggered, create ADR 002 with concrete Ibis migration plan.**

## Appendix B: Storage Format Comparison

| Format | Read Speed | Write Speed | Compression | Query Pushdown | ACID |
|--------|-----------|-------------|-------------|----------------|------|
| **Session State** | Instant | Instant | None | N/A | No |
| **CSV** | Slow | Fast | None | No | No |
| **Parquet** | Fast | Moderate | High (~60%) | Yes (Polars) | No |
| **DuckDB** | Fast | Fast | Moderate (~40%) | Yes (SQL) | Yes |
| **Our Hybrid** | Fast | Moderate | High (Parquet) | Yes (both) | Yes (DuckDB) |

**Rationale for hybrid approach:** DuckDB provides ACID and SQL analytics; Parquet enables Polars lazy optimization. Disk usage trade-off is acceptable for clinical research workflow benefits.
```

---

## ADR Created

This ADR documents:

1. **Why we're adding persistence NOW** (session state is broken)
2. **Why we're using DuckDB + Parquet** (not alternatives)
3. **Why we're NOT using Ibis yet** (explicit deferral with criteria for revisit)
4. **Concrete implementation plan** (4 phases, 8 hours total)
5. **Success metrics** (testable go/no-go criteria)
6. **Review trigger** (2 weeks, reassess Ibis decision)

The ADR makes it clear this is **not** punting on architectural decisions—it's choosing the simplest thing that solves the immediate problem while leaving the door open for Ibis when complexity justifies it.
