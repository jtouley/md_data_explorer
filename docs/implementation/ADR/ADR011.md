# ADR 011: LLM-Assisted Dataset Context and Metadata Enrichment

## Status

Proposed

## Date

2026-01-12

## Owners

- Staff Data Engineer (primary)
- Platform Team

## Decision Summary

- **Metadata Patch Model**: Define `MetadataPatch` as immutable, schema-validated JSON patches with atomic operations (`set_label`, `add_alias`, `set_description`, `set_semantic_type`, `mark_phi`, `mark_entity_key`, `set_unit`, `set_codebook_entry`).
- **LLM as Suggestion Engine Only**: LLM produces suggestions as strict JSON payloads validated against `MetadataPatch` schema. LLM never writes state directly; all suggestions require explicit user acceptance.
- **Overlay Architecture**: Store patches as overlays keyed by `(upload_id, dataset_version)`. Merge precedence: `base < inferred < accepted_patches`. Rejected/pending patches excluded from AutoContext.
- **Privacy-First Local LLM**: Default to schema + profiling + dictionary context only (no raw rows). Optional gated mode for richer samples requires explicit admin toggle, redaction/masking, and retention controls.
- **Human-in-the-Loop Workflow**: UI presents diff view for propose → accept/reject → batch apply. All patch operations logged with actor, timestamp, model_id, and prompt hash.
- **Deterministic Runtime Merge**: Resolved metadata computed at runtime via deterministic merge resolver. No LLM outputs in production paths until accepted.

## Related ADRs

- **[ADR002: Persistent Storage Layer](./ADR002.md)**: Owns metadata JSON schema and persistence. This ADR extends ADR002's metadata schema with overlay storage for patches.
- **[ADR003: Clinical Trust Protocol + Adaptive Alias Persistence](./ADR003.md)**: Owns alias persistence and trust UI. This ADR builds on ADR003's alias system to add LLM-suggested aliases via patch model.
- **[ADR004: Proactive Question Generation](./ADR004.md)**: Related feature using LLM for question suggestions. Shares LLM client infrastructure, AutoContext Packager, and privacy constraints.

---

## Context / Problem Statement

### Cold Start Problem

When users upload datasets, the system faces a **cold start problem**:

1. **Opaque Schemas**: Column names like `var_001`, `q3_resp`, or `hba1c_pct` convey no meaning to the semantic layer or NLU engine.
2. **Missing Context**: No descriptions, units, semantic types, or codebook mappings exist. The AutoContext packager has nothing to inject into Tier 3 LLM prompts.
3. **Manual Configuration Burden**: Requiring users to manually annotate 50+ columns before querying creates adoption friction (violates ADR003's "10-second onboarding" principle).

### Gap Between Config-Driven Engine and UX

The platform uses a config-driven architecture (`datasets.yaml`, `InferredSchema`, `SemanticLayer`) but lacks a **metadata enrichment layer** that:

1. Bridges raw schema inference with human-readable descriptions
2. Enables LLM to suggest context without directly modifying state
3. Allows progressive enrichment without blocking initial use
4. Maintains provenance and rollback capability

### Current State

| Component | Status | Gap |
|-----------|--------|-----|
| `InferredSchema` | ✅ Exists | Detects types but no descriptions/semantic types |
| `DictionaryMetadata` | ✅ Exists | Extracts from PDFs but not editable post-upload |
| `ColumnContext` (AutoContext) | ✅ Exists | Has `codebook`, `units`, `dtype` but populated only from inference |
| `alias_mappings` | ✅ Exists (ADR003) | User/system aliases, but no LLM suggestions |
| Column descriptions | ❌ Missing | No storage or UI for column-level descriptions |
| Semantic types | ❌ Missing | No tagging (PHI, entity_key, measurement, etc.) |
| Codebook editing | ❌ Missing | Extracted from docs but not user-editable |
| Enrichment audit trail | ❌ Missing | No provenance for metadata changes |

---

## Goals

1. **Machine-Actionable Metadata**: Store enriched metadata (descriptions, semantic types, units, codebooks) in a structured format usable by AutoContext, NLU, and semantic layer.

2. **Human-in-the-Loop Acceptance**: LLM suggestions require explicit user acceptance. Rejected suggestions are logged but never applied. No "auto-apply" of LLM outputs.

3. **Deterministic Runtime Merge**: Resolved metadata computed via deterministic precedence rules at runtime. Same inputs always produce same outputs (testable, reproducible).

4. **Provenance and Audit Trail**: Every patch operation records actor (user/llm), timestamp, model_id (if LLM), prompt_hash, and before/after diff.

5. **Rollback Capability**: Users can revert individual patches or restore to any previous state via version history.

6. **Progressive Enrichment**: Users can query immediately with inferred schema. Enrichment is additive and non-blocking.

## Non-Goals

1. **LLM Directly Edits YAML/State**: LLM never writes to `datasets.yaml`, metadata JSON, or any persisted state. It produces suggestions only.

2. **Remote LLM Calls**: This ADR assumes local LLM (Ollama). Remote LLM support is out of scope and would require separate privacy review.

3. **Perfect Automatic PII Detection**: LLM-suggested `mark_phi` is a hint, not a guarantee. Manual review required. No claim of HIPAA-compliant auto-detection.

4. **Real-Time LLM in Query Path**: LLM enrichment is an offline/batch operation. Query execution uses only accepted metadata.

5. **Cross-Dataset Metadata Sharing**: Enrichments are scoped to `(upload_id, dataset_version)`. No global metadata inheritance.

---

## Decision

### 1. MetadataPatch Model

Define a structured patch model for atomic metadata operations:

```python
from dataclasses import dataclass, field
from datetime import datetime
from typing import Literal, Any
from enum import Enum

class PatchOperation(str, Enum):
    SET_LABEL = "set_label"                     # Human-readable column label
    ADD_ALIAS = "add_alias"                     # Add column alias for NLU
    REMOVE_ALIAS = "remove_alias"               # Remove alias
    SET_DESCRIPTION = "set_description"         # Column or dataset description
    SET_SEMANTIC_TYPE = "set_semantic_type"     # identifier, demographic, clinical, etc.
    MARK_PHI = "mark_phi"                       # Flag as protected health information
    UNMARK_PHI = "unmark_phi"                   # Remove PHI flag
    MARK_ENTITY_KEY = "mark_entity_key"         # Flag as entity key (patient_id, encounter_id)
    SET_UNIT = "set_unit"                       # Measurement unit (mg/dL, mmHg, etc.)
    SET_CODEBOOK_ENTRY = "set_codebook_entry"   # Add/update codebook mapping
    REMOVE_CODEBOOK_ENTRY = "remove_codebook_entry"
    SET_DATASET_DESCRIPTION = "set_dataset_description"  # Dataset-level description

class SemanticType(str, Enum):
    IDENTIFIER = "identifier"       # Patient ID, encounter ID, etc.
    DEMOGRAPHIC = "demographic"     # Age, sex, race, etc.
    CLINICAL = "clinical"           # Diagnoses, vitals, labs
    TEMPORAL = "temporal"           # Dates, durations
    OUTCOME = "outcome"             # Binary endpoints
    MEASUREMENT = "measurement"     # Numeric with units
    CODED = "coded"                 # Categorical with codebook
    FREE_TEXT = "free_text"         # Unstructured text
    PHI = "phi"                     # Protected health information

class PatchSource(str, Enum):
    USER = "user"                   # Manual user edit via UI
    LLM_ACCEPTED = "llm_accepted"   # LLM suggestion accepted by user
    SYSTEM = "system"               # System-generated (e.g., from inference)

class PatchStatus(str, Enum):
    PENDING = "pending"             # LLM suggestion awaiting review
    ACCEPTED = "accepted"           # Applied to resolved metadata
    REJECTED = "rejected"           # User rejected, logged but not applied

@dataclass(frozen=True)
class MetadataPatch:
    """Immutable, atomic metadata patch operation."""

    patch_id: str                           # UUID v4
    upload_id: str                          # Scoped to upload
    dataset_version: str                    # Scoped to version (content hash)
    operation: PatchOperation
    target_column: str | None               # None for dataset-level ops

    # Operation-specific payload
    value: Any                              # New value (type depends on operation)
    previous_value: Any | None = None       # For rollback

    # Provenance
    source: PatchSource = PatchSource.USER
    actor: str = "anonymous"                # User ID or "llm"
    model_id: str | None = None             # e.g., "llama3.1:8b"
    prompt_hash: str | None = None          # SHA256 of prompt (for LLM)

    # Timestamps
    created_at: datetime = field(default_factory=datetime.utcnow)
    reviewed_at: datetime | None = None     # When accepted/rejected

    # State
    status: PatchStatus = PatchStatus.PENDING
```

### 2. Example JSON Patch Payloads

**LLM Suggestion Batch** (raw output from LLM, pre-validation):

```json
{
  "suggestions": [
    {
      "patch_id": "550e8400-e29b-41d4-a716-446655440001",
      "operation": "set_description",
      "target_column": "hba1c_pct",
      "value": "Glycated hemoglobin percentage, measure of average blood glucose over 2-3 months",
      "confidence": 0.92
    },
    {
      "patch_id": "550e8400-e29b-41d4-a716-446655440002",
      "operation": "set_unit",
      "target_column": "hba1c_pct",
      "value": "%",
      "confidence": 0.95
    },
    {
      "patch_id": "550e8400-e29b-41d4-a716-446655440003",
      "operation": "set_semantic_type",
      "target_column": "hba1c_pct",
      "value": "measurement",
      "confidence": 0.88
    },
    {
      "patch_id": "550e8400-e29b-41d4-a716-446655440004",
      "operation": "add_alias",
      "target_column": "hba1c_pct",
      "value": "A1C",
      "confidence": 0.90
    },
    {
      "patch_id": "550e8400-e29b-41d4-a716-446655440005",
      "operation": "mark_phi",
      "target_column": "mrn",
      "value": true,
      "confidence": 0.85
    },
    {
      "patch_id": "550e8400-e29b-41d4-a716-446655440006",
      "operation": "set_codebook_entry",
      "target_column": "smoking_status",
      "value": {"code": "1", "label": "Current smoker"},
      "confidence": 0.78
    }
  ],
  "model_id": "llama3.1:8b",
  "prompt_hash": "sha256:a1b2c3d4e5f6..."
}
```

**Accepted Patch** (after user acceptance, persisted to overlay):

```json
{
  "patch_id": "550e8400-e29b-41d4-a716-446655440001",
  "upload_id": "upload_abc123",
  "dataset_version": "v_7f8a9b0c",
  "operation": "set_description",
  "target_column": "hba1c_pct",
  "value": "Glycated hemoglobin percentage, measure of average blood glucose over 2-3 months",
  "previous_value": null,
  "source": "llm_accepted",
  "actor": "user_jane_doe",
  "model_id": "llama3.1:8b",
  "prompt_hash": "sha256:a1b2c3d4e5f6...",
  "created_at": "2026-01-12T14:30:00Z",
  "reviewed_at": "2026-01-12T14:32:15Z",
  "status": "accepted"
}
```

### 3. Resolved Metadata Object

The merge resolver produces a `ResolvedColumnMetadata` object by applying patches in precedence order:

```python
@dataclass
class ResolvedColumnMetadata:
    """Fully resolved metadata for a single column."""

    # Identity
    canonical_name: str                     # Original column name
    label: str | None = None                # Human-readable label
    description: str | None = None          # Full description

    # Type information
    inferred_dtype: str = "unknown"         # From schema inference (numeric, categorical, etc.)
    semantic_type: SemanticType | None = None

    # Aliases (for NLU matching)
    system_aliases: list[str] = field(default_factory=list)  # From inference
    user_aliases: list[str] = field(default_factory=list)    # From accepted patches

    # Measurement metadata
    unit: str | None = None                 # e.g., "mg/dL", "mmHg"

    # Codebook (for coded/categorical columns)
    codebook: dict[str, str] = field(default_factory=dict)  # {code: label}

    # Flags
    is_phi: bool = False                    # Protected health information
    is_entity_key: bool = False             # Primary/foreign key

    # Provenance
    enrichment_source: Literal["base", "inferred", "user", "llm_accepted"] = "base"
    last_updated: datetime | None = None

@dataclass
class ResolvedDatasetMetadata:
    """Fully resolved metadata for entire dataset."""

    upload_id: str
    dataset_version: str

    # Dataset-level
    display_name: str | None = None
    description: str | None = None

    # Column metadata
    columns: dict[str, ResolvedColumnMetadata] = field(default_factory=dict)

    # Entity keys (ordered by priority)
    entity_keys: list[str] = field(default_factory=list)

    # Merge provenance
    base_source: str = "inferred_schema"
    patch_count: int = 0
    last_merged: datetime | None = None
```

**Example Resolved Metadata** (after merge):

```json
{
  "upload_id": "upload_abc123",
  "dataset_version": "v_7f8a9b0c",
  "display_name": "Diabetes Registry Q4 2025",
  "description": "Patient registry for type 2 diabetes management program",
  "columns": {
    "patient_id": {
      "canonical_name": "patient_id",
      "label": "Patient ID",
      "description": "Unique patient identifier (synthetic)",
      "inferred_dtype": "id",
      "semantic_type": "identifier",
      "system_aliases": ["patientid", "patient", "id"],
      "user_aliases": ["pt_id", "subject"],
      "is_phi": false,
      "is_entity_key": true,
      "enrichment_source": "inferred",
      "last_updated": "2026-01-10T10:00:00Z"
    },
    "hba1c_pct": {
      "canonical_name": "hba1c_pct",
      "label": "HbA1c",
      "description": "Glycated hemoglobin percentage, measure of average blood glucose over 2-3 months",
      "inferred_dtype": "numeric",
      "semantic_type": "measurement",
      "system_aliases": ["hba1c", "hemoglobin_a1c"],
      "user_aliases": ["A1C", "glycated hemoglobin"],
      "unit": "%",
      "is_phi": false,
      "is_entity_key": false,
      "enrichment_source": "llm_accepted",
      "last_updated": "2026-01-12T14:32:15Z"
    },
    "smoking_status": {
      "canonical_name": "smoking_status",
      "label": "Smoking Status",
      "description": "Current smoking behavior",
      "inferred_dtype": "categorical",
      "semantic_type": "coded",
      "system_aliases": ["smoking", "tobacco_use"],
      "user_aliases": [],
      "codebook": {
        "1": "Current smoker",
        "2": "Former smoker",
        "3": "Never smoked",
        "9": "Unknown"
      },
      "is_phi": false,
      "is_entity_key": false,
      "enrichment_source": "llm_accepted",
      "last_updated": "2026-01-12T14:35:00Z"
    },
    "mrn": {
      "canonical_name": "mrn",
      "label": "Medical Record Number",
      "description": "Hospital medical record number (PHI - masked in exports)",
      "inferred_dtype": "id",
      "semantic_type": "identifier",
      "system_aliases": ["medical_record_number"],
      "user_aliases": [],
      "is_phi": true,
      "is_entity_key": false,
      "enrichment_source": "llm_accepted",
      "last_updated": "2026-01-12T14:33:00Z"
    }
  },
  "entity_keys": ["patient_id"],
  "base_source": "inferred_schema",
  "patch_count": 8,
  "last_merged": "2026-01-12T14:35:00Z"
}
```

### 4. Overlay Storage and Merge Precedence

**Storage Structure**:

```
data/uploads/
├── metadata/
│   ├── {upload_id}.json              # Base metadata (ADR002)
│   └── overlays/
│       └── {upload_id}/
│           ├── {dataset_version}/
│           │   ├── patches.jsonl     # Append-only patch log
│           │   ├── pending.json      # Current pending suggestions
│           │   └── resolved.json     # Cached resolved metadata
│           └── ...
```

**Merge Precedence** (deterministic, lowest to highest priority):

1. **Base** (`base`): Empty defaults, column names only
2. **Inferred** (`inferred`): From `SchemaInferenceEngine` and `DictionaryMetadata` (ADR004)
3. **Accepted Patches** (`accepted_patches`): User-accepted patches in chronological order

**Merge Rules**:

```python
def resolve_metadata(
    upload_id: str,
    dataset_version: str,
    base_schema: InferredSchema,
    patches: list[MetadataPatch],
) -> ResolvedDatasetMetadata:
    """
    Deterministic merge of base schema with accepted patches.

    Rules:
    1. Start with base schema (column names, inferred types)
    2. Apply DictionaryMetadata if available (descriptions, codebooks from docs)
    3. Apply accepted patches in chronological order (created_at)
    4. Later patches override earlier patches for same (column, operation)
    5. Rejected/pending patches are excluded
    6. Result is deterministic: same inputs → same output
    """
    resolved = ResolvedDatasetMetadata(
        upload_id=upload_id,
        dataset_version=dataset_version,
    )

    # Step 1: Initialize from base schema
    for col in base_schema.all_columns:
        resolved.columns[col] = ResolvedColumnMetadata(
            canonical_name=col,
            inferred_dtype=_infer_dtype(col, base_schema),
            enrichment_source="base",
        )

    # Step 2: Apply inferred metadata (DictionaryMetadata)
    if base_schema.dictionary_metadata:
        for col, desc in base_schema.dictionary_metadata.column_descriptions.items():
            if col in resolved.columns:
                resolved.columns[col].description = desc
                resolved.columns[col].enrichment_source = "inferred"

        for col, codebook in base_schema.dictionary_metadata.codebooks.items():
            if col in resolved.columns:
                resolved.columns[col].codebook = codebook
                resolved.columns[col].enrichment_source = "inferred"

    # Step 3: Apply accepted patches in order
    accepted = [p for p in patches if p.status == PatchStatus.ACCEPTED]
    accepted.sort(key=lambda p: p.created_at)

    for patch in accepted:
        _apply_patch(resolved, patch)
        resolved.patch_count += 1

    resolved.last_merged = datetime.utcnow()
    return resolved
```

**State Transitions**:

```
                    ┌─────────────────┐
                    │  LLM Generates  │
                    │   Suggestions   │
                    └────────┬────────┘
                             │
                             ▼
                    ┌─────────────────┐
                    │     PENDING     │◄──────────────────┐
                    │  (not applied)  │                   │
                    └────────┬────────┘                   │
                             │                            │
               ┌─────────────┼─────────────┐              │
               │             │             │              │
               ▼             ▼             ▼              │
      ┌─────────────┐  ┌───────────┐  ┌─────────────┐    │
      │  ACCEPTED   │  │ REJECTED  │  │   EXPIRED   │    │
      │  (applied)  │  │ (logged)  │  │  (30 days)  │    │
      └──────┬──────┘  └───────────┘  └─────────────┘    │
             │                                            │
             │ User can revert                            │
             └────────────────────────────────────────────┘
```

### 5. LLM Suggestion Workflow

**LLM produces suggestions ONLY as strict JSON patches**:

1. LLM receives schema + profiling + optional dictionary context
2. LLM outputs JSON array of `MetadataPatch` suggestions
3. System validates JSON against `MetadataPatch` schema
4. Invalid suggestions are logged and discarded (never surfaced to user)
5. Valid suggestions stored as `pending` patches
6. User reviews in UI diff view
7. User accepts, rejects, or modifies suggestions
8. Only `accepted` patches are applied to resolved metadata

**LLM Prompt Structure** (schema only, no raw rows by default):

```
You are a clinical data analyst. Given the following dataset schema and statistics, suggest metadata enrichments.

## Dataset
- Upload ID: upload_abc123
- Version: v_7f8a9b0c
- Row count: 1,247
- Column count: 42

## Columns (with profiling)
| Column | Type | Unique | Null% | Sample Values (masked) |
|--------|------|--------|-------|------------------------|
| hba1c_pct | float64 | 89 | 2.3% | 5.4, 7.2, 9.1, ... |
| smoking_status | int64 | 4 | 0.8% | 1, 2, 3, 9 |
| mrn | string | 1247 | 0% | [REDACTED] |

## Available Documentation
[Extracted text from data_dictionary.pdf - see ADR004]

## Task
Output a JSON array of metadata patches. Each patch must have:
- operation: one of [set_label, add_alias, set_description, set_semantic_type, mark_phi, set_unit, set_codebook_entry]
- target_column: column name
- value: the enrichment value
- confidence: your confidence 0.0-1.0

Only suggest enrichments you are confident about (>0.7). Output valid JSON only.
```

**Validation Requirements**:

```python
def validate_llm_suggestions(raw_json: str) -> list[MetadataPatch]:
    """
    Validate LLM output against MetadataPatch schema.

    Requirements:
    1. Must be valid JSON
    2. Must be array of objects
    3. Each object must have required fields
    4. operation must be valid PatchOperation
    5. target_column must exist in dataset schema
    6. value must be correct type for operation
    7. confidence must be 0.0-1.0

    Invalid suggestions are logged and excluded.
    Returns only valid patches with status=PENDING.
    """
```

### 6. UI Workflow

**Enrichment Panel** (integrated into dataset detail view):

```
┌─────────────────────────────────────────────────────────────────┐
│  Dataset: Diabetes Registry Q4 2025                              │
│  Version: v_7f8a9b0c | Columns: 42 | Rows: 1,247                │
├─────────────────────────────────────────────────────────────────┤
│  ⚡ 6 AI Suggestions Available              [Review All] [Dismiss]│
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  Column: hba1c_pct                                               │
│  ┌─────────────────────────────────────────────────────────────┐│
│  │ Current         │ Suggested (AI)                            ││
│  │─────────────────│────────────────────────────────────────────││
│  │ Label: (none)   │ Label: HbA1c                               ││
│  │ Desc: (none)    │ Desc: Glycated hemoglobin percentage...    ││
│  │ Unit: (none)    │ Unit: %                                    ││
│  │ Type: numeric   │ Type: measurement                          ││
│  │ Aliases: []     │ Aliases: ["A1C", "glycated hemoglobin"]   ││
│  └─────────────────────────────────────────────────────────────┘│
│                                                                  │
│  [✓ Accept All] [✓ Accept Selected] [✗ Reject] [✎ Edit]          │
│                                                                  │
├─────────────────────────────────────────────────────────────────┤
│  Column: mrn                                                     │
│  ┌─────────────────────────────────────────────────────────────┐│
│  │ ⚠️ PHI Flag Suggested                                        ││
│  │                                                              ││
│  │ AI suggests marking 'mrn' as Protected Health Information   ││
│  │ Confidence: 85%                                              ││
│  │                                                              ││
│  │ [✓ Confirm PHI] [✗ Not PHI]                                  ││
│  └─────────────────────────────────────────────────────────────┘│
└─────────────────────────────────────────────────────────────────┘
```

**Batch Operations**:

- Accept All: Apply all pending suggestions
- Accept Selected: Apply checked suggestions only
- Reject: Mark as rejected (logged, not applied)
- Edit: Modify suggestion before accepting

**History View** (audit trail):

```
┌─────────────────────────────────────────────────────────────────┐
│  Enrichment History                           [Export] [Revert]  │
├─────────────────────────────────────────────────────────────────┤
│  2026-01-12 14:35:00 | user_jane_doe | set_codebook_entry       │
│    Column: smoking_status                                        │
│    Added: {"1": "Current smoker", "2": "Former smoker", ...}    │
│    Source: llm_accepted (llama3.1:8b)                           │
│                                                         [Revert] │
├─────────────────────────────────────────────────────────────────┤
│  2026-01-12 14:33:00 | user_jane_doe | mark_phi                  │
│    Column: mrn                                                   │
│    Changed: false → true                                         │
│    Source: llm_accepted (llama3.1:8b)                           │
│                                                         [Revert] │
├─────────────────────────────────────────────────────────────────┤
│  2026-01-12 14:32:15 | user_jane_doe | set_description          │
│    Column: hba1c_pct                                             │
│    Added: "Glycated hemoglobin percentage..."                   │
│    Source: llm_accepted (llama3.1:8b)                           │
│                                                         [Revert] │
└─────────────────────────────────────────────────────────────────┘
```

---

## Privacy / Security Posture

Even with a local LLM (Ollama), enforce privacy controls:

### Default Mode: Schema + Profiling Only

**What LLM sees**:
- Column names and inferred types
- Aggregate statistics (count, null%, unique count, min/max for numerics)
- Value distributions (histogram bins, not raw values)
- Extracted documentation text (ADR004)

**What LLM does NOT see**:
- Raw row data
- Individual patient values
- Identifiable patterns in data
- Full text of free-text columns

### Optional Gated Mode: Sample Rows

For richer context (e.g., inferring codebooks from actual values), an admin-gated mode allows sample rows:

**Requirements**:
1. **Explicit Admin Toggle**: `ENABLE_LLM_SAMPLE_ROWS=true` environment variable
2. **Redaction/Masking**: PHI-flagged columns replaced with `[REDACTED]`
3. **No Persistence**: Sample rows used transiently, never logged or cached
4. **Row Limit**: Maximum 10 sample rows
5. **Audit Log**: Records that sample mode was used (but not the samples themselves)

```python
@dataclass
class LLMContext:
    """Context provided to LLM for enrichment suggestions."""

    # Always included
    upload_id: str
    dataset_version: str
    schema_summary: dict[str, Any]      # Column names, types, stats
    doc_context: str | None             # Extracted documentation

    # Gated mode only
    sample_rows: list[dict] | None = None  # Max 10, PHI redacted
    sample_mode_enabled: bool = False

    def to_prompt(self) -> str:
        """Generate LLM prompt from context."""
```

### Audit Log

Every LLM interaction is logged:

```json
{
  "event_id": "evt_123456",
  "timestamp": "2026-01-12T14:30:00Z",
  "event_type": "llm_enrichment_request",
  "upload_id": "upload_abc123",
  "dataset_version": "v_7f8a9b0c",
  "actor": "user_jane_doe",
  "model_id": "llama3.1:8b",
  "prompt_hash": "sha256:a1b2c3d4e5f6...",
  "sample_mode_enabled": false,
  "suggestion_count": 6,
  "validation_failures": 0,
  "latency_ms": 2340
}
```

---

## Alternatives Considered

### 1. LLM Edits datasets.yaml Directly

**Rejected because**:
- No audit trail or rollback
- Risk of corrupting config file
- No human review before apply
- Violates "LLM as suggestion engine" principle

### 2. Freeform Notes Only (No Structured Patches)

**Rejected because**:
- Not machine-actionable (can't use in AutoContext)
- No merge semantics
- No conflict resolution
- Loses type safety

### 3. Always Auto-Apply LLM Outputs

**Rejected because**:
- LLM confidence != correctness
- Clinical domain requires human review
- PHI detection cannot be fully automated
- Violates ADR003's trust principles

### 4. Store Enrichments in InferredSchema

**Rejected because**:
- `InferredSchema` is compute-derived, not user-editable
- No separation between inferred and user-accepted
- No patch history or rollback
- Breaks single-responsibility principle

### 5. Global Metadata Inheritance Across Datasets

**Rejected because**:
- Column names are dataset-specific
- "age" in dataset A may differ from "age" in dataset B
- Increases complexity without clear benefit
- Scope isolation prevents cross-contamination

---

## Consequences

### Positive

1. **Machine-Actionable Context**: AutoContext can include rich metadata (descriptions, aliases, semantic types) for better Tier 3 LLM query parsing.

2. **Progressive Enrichment**: Users start querying immediately; enrichment is additive and non-blocking.

3. **Auditability**: Full provenance trail for compliance and debugging.

4. **Rollback Safety**: Any enrichment can be reverted without data loss.

5. **LLM Guardrails**: Strict JSON validation prevents malformed suggestions from reaching users.

6. **Privacy by Default**: Schema-only mode protects sensitive data while still enabling useful suggestions.

### Negative

1. **Operational Complexity**: New storage layer (overlays), merge resolver, and UI components to maintain.

2. **UX Overhead**: Users must review and accept suggestions (not fully automatic).

3. **LLM Latency**: Enrichment suggestions add 2-5 seconds per batch (acceptable for offline operation).

4. **Storage Growth**: Patch logs grow over time (mitigated by periodic compaction).

### Neutral

1. **Local LLM Dependency**: Requires Ollama running locally. Not a new constraint (ADR004 already requires it).

2. **Learning Curve**: Users must understand diff/accept workflow. Mitigated by clear UI.

---

## Implementation Plan

### Phase 0: Data Model + Storage + Merge Resolver (P0)

**Files**:
- `src/clinical_analytics/core/metadata_patch.py` - Patch model and operations
- `src/clinical_analytics/core/metadata_resolver.py` - Merge resolver
- `src/clinical_analytics/storage/overlay_store.py` - Overlay persistence

**Deliverables**:
1. `MetadataPatch` dataclass with validation
2. `PatchLog` for append-only storage (JSONL)
3. `resolve_metadata()` function with precedence rules
4. Unit tests for merge semantics (golden tests)

**TDD Workflow**:
```bash
# RED: Write failing tests for merge precedence
uv run pytest tests/core/test_metadata_resolver.py -xvs

# GREEN: Implement resolver
# ... implement ...

# VERIFY: All tests pass
make test-core
make check && git commit -m "feat: Phase 0 - Metadata patch model and merge resolver"
```

### Phase 1: UI Enrichment + Patch Log (P1)

**Files**:
- `src/clinical_analytics/ui/components/enrichment_panel.py` - Diff view component
- `src/clinical_analytics/ui/components/history_viewer.py` - Audit trail UI
- `src/clinical_analytics/api/routes/metadata.py` - API endpoints (if FastAPI)

**Deliverables**:
1. Column enrichment panel with diff view
2. Accept/reject/edit workflow
3. History viewer with revert capability
4. Integration with existing dataset detail page

**TDD Workflow**:
```bash
make test-ui PYTEST_ARGS="tests/ui/test_enrichment_panel.py -xvs"
make check && git commit -m "feat: Phase 1 - UI enrichment panel and patch log"
```

### Phase 2: LLM Suggestion Endpoint + Validation (P1)

**Files**:
- `src/clinical_analytics/core/llm_enrichment.py` - LLM integration
- `src/clinical_analytics/core/llm_json.py` - Extend with patch validation

**Deliverables**:
1. `generate_enrichment_suggestions()` function
2. JSON schema validation for LLM output
3. Confidence filtering (>0.7 threshold)
4. Integration with existing LLM client (ADR004)

**TDD Workflow**:
```bash
uv run pytest tests/core/test_llm_enrichment.py -xvs
make check && git commit -m "feat: Phase 2 - LLM suggestion generation with validation"
```

### Phase 3: Optional Gated Sample Mode (P2)

**Files**:
- `src/clinical_analytics/core/llm_context.py` - Context builder with masking
- `config/privacy.yaml` - Privacy settings

**Deliverables**:
1. Environment variable gate (`ENABLE_LLM_SAMPLE_ROWS`)
2. PHI column redaction
3. Audit logging for sample mode usage
4. Admin documentation

**TDD Workflow**:
```bash
uv run pytest tests/core/test_llm_context.py -xvs
make check && git commit -m "feat: Phase 3 - Gated sample rows mode with redaction"
```

---

## Test Plan

### Golden Tests for Merge Precedence

```python
class TestMergePrecedence:
    """Golden tests verifying deterministic merge behavior."""

    def test_base_only_returns_column_names(self):
        """Base schema provides column names, no enrichments."""

    def test_inferred_overrides_base(self):
        """DictionaryMetadata descriptions override base."""

    def test_accepted_patches_override_inferred(self):
        """User-accepted patches override inferred metadata."""

    def test_later_patches_override_earlier(self):
        """Chronologically later patches win for same (column, op)."""

    def test_rejected_patches_excluded(self):
        """Rejected patches do not affect resolved metadata."""

    def test_pending_patches_excluded(self):
        """Pending patches do not affect resolved metadata."""

    def test_deterministic_output(self):
        """Same inputs always produce same resolved metadata."""
```

### Property Tests for Patch Validation

```python
from hypothesis import given, strategies as st

class TestPatchValidation:
    """Property-based tests for patch validation."""

    @given(st.text())
    def test_invalid_json_rejected(self, raw):
        """Malformed JSON never produces valid patches."""

    @given(st.lists(st.fixed_dictionaries({...})))
    def test_invalid_operations_rejected(self, patches):
        """Unknown operations are rejected."""

    @given(st.lists(st.fixed_dictionaries({...})))
    def test_missing_columns_rejected(self, patches):
        """Patches targeting non-existent columns are rejected."""
```

### Regression: AutoContext Determinism

```python
class TestAutoContextDeterminism:
    """Ensure AutoContext excludes unaccepted suggestions."""

    def test_pending_suggestions_excluded_from_autocontext(self):
        """Pending LLM suggestions do not appear in AutoContext."""

    def test_rejected_suggestions_excluded_from_autocontext(self):
        """Rejected suggestions do not appear in AutoContext."""

    def test_accepted_enrichments_included_in_autocontext(self):
        """Accepted patches appear in AutoContext column metadata."""

    def test_autocontext_deterministic_across_runs(self):
        """Same patches produce identical AutoContext."""
```

---

## Open Questions / Follow-ups

### 1. Where to Persist Overlays?

**Options**:
- **JSONL files** (current proposal): Simple, human-readable, append-only
- **SQLite**: Better querying, but adds dependency
- **DuckDB**: Already in stack, could extend `analytics.duckdb`

**Recommendation**: Start with JSONL for MVP. Migrate to SQLite/DuckDB if query patterns require it.

### 2. Confidence Thresholds for Auto-Apply?

**Current decision**: No auto-apply. All suggestions require human acceptance.

**Future consideration**: Could add opt-in "auto-accept if confidence > 0.95 AND not PHI-related" mode. Requires user preference storage and additional trust analysis.

### 3. How to Handle Renamed Columns Across Versions?

**Scenario**: User uploads v1 with column "hba1c", enriches it. Later uploads v2 with column renamed to "hba1c_pct".

**Options**:
- **Orphan detection**: Mark patches as orphaned if target_column missing in new version
- **Column mapping**: Allow user to map old column → new column
- **Automatic matching**: Use fuzzy matching to suggest column mappings

**Recommendation**: Implement orphan detection for MVP. Column mapping wizard for Phase 2+.

### 4. Codebook Conflict Resolution?

**Scenario**: LLM suggests `{"1": "Yes", "2": "No"}` but user knows it's `{"1": "Male", "2": "Female"}`.

**Current handling**: User rejects LLM suggestion and manually enters correct codebook.

**Future enhancement**: Allow partial accept (keep structure, change labels).

### 5. Multi-User Collaboration?

**Current scope**: Single-user tool. No concurrent edit handling.

**Future consideration**: If multi-user, need optimistic locking or CRDT-style merge for patches.

---

## Code References

- `src/clinical_analytics/core/schema_inference.py` - `InferredSchema`, `DictionaryMetadata`
- `src/clinical_analytics/core/autocontext.py` - `AutoContext`, `ColumnContext`
- `src/clinical_analytics/core/semantic.py` - `SemanticLayer.add_user_alias()`
- `src/clinical_analytics/ui/storage/user_datasets.py` - Metadata persistence
- `src/clinical_analytics/core/llm_json.py` - LLM JSON validation

---

## Approval

- [ ] Staff Data Engineer review
- [ ] Security/Privacy review (PHI handling)
- [ ] UX review (enrichment workflow)

## Revision History

- **2026-01-12**: Initial proposal (ADR011 created)
