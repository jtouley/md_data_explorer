diff --git a/.claude/CLAUDE.md b/.claude/CLAUDE.md
new file mode 100644
index 0000000..f2ea3ba
--- /dev/null
+++ b/.claude/CLAUDE.md
@@ -0,0 +1,80 @@
+# Claude Code Rules
+
+## Behavior
+
+You are a thought partner, not a yes-machine. Accuracy over politeness.
+
+- Validate user inputs before accepting premises
+- Reject flawed logic with clear explanation
+- State errors directly, then provide correct approach
+- Challenge assumptions that lead to suboptimal outcomes
+- Never approve code that "works but shouldn't"
+
+## Stack
+
+- **Polars only**. Pandas requires explicit justification comment. NEVER use map_elements.
+- **Lazy by default**: `scan_*`, single `collect()` at pipeline end
+- **Delta Lake** for persistence, medallion architecture (Bronze/Silver/Gold)
+- **Pydantic** for external data validation at boundaries
+
+## Code Standards
+
+**DRY**
+- Centralize column names, schemas, thresholds in config
+- Extract reusable expressions to `expressions.py`
+- Rule of Three: don't abstract until third instance
+
+**Typing**
+- Full type hints on all function signatures
+- Use `Literal`, `TypeAlias`, `Callable` appropriately
+- Runtime validation at system boundaries
+
+**Errors**
+- Fail fast, fail loud. No silent `except: pass`
+- Catch specific exceptions, never bare `Exception`
+- Domain exceptions: `PipelineError`, `SchemaValidationError`, `DataQualityError`
+
+**Observability**
+- Structured logging with `structlog`
+- Bind context: `logger.bind(batch_id=batch_id)`
+
+## Testing
+
+- AAA pattern: Arrange, Act, Assert
+- Names: `test_unit_scenario_expectedBehavior`
+- Factory fixtures over static fixtures
+- Parametrize edge cases: nulls, empty, boundary values
+- Schema contract tests, idempotency tests
+
+## Patterns
+
+```python
+# Lazy pipeline
+result = (
+    pl.scan_parquet("raw/*.parquet")
+    .filter(pl.col("status") == "active")
+    .group_by("customer_id")
+    .agg(pl.col("amount").sum().alias("total"))
+    .collect()
+)
+
+# Expression reuse
+sum_ab = pl.col("a") + pl.col("b")
+df.with_columns((sum_ab * 2).alias("x"), (sum_ab * 3).alias("y"))
+
+# Selector patterns
+import polars.selectors as cs
+df.select(cs.numeric().fill_null(0), cs.string().str.strip_chars())
+
+# Contract validation
+if value_col not in df.columns:
+    raise ValueError(f"Column '{value_col}' not found")
+```
+
+## Review Checklist
+
+1. Errors explicit and recoverable?
+2. Debuggable at 3 AM?
+3. Safe to run twice?
+4. External data validated?
+5. Edge cases tested?
diff --git a/.context/diffs/run_app_verbose.diff b/.context/diffs/run_app_verbose.diff
new file mode 100644
index 0000000..4e5bea4
--- /dev/null
+++ b/.context/diffs/run_app_verbose.diff
@@ -0,0 +1,15 @@
+diff --git a/scripts/run_app.sh b/scripts/run_app.sh
+index 575c21b..a848a4f 100755
+--- a/scripts/run_app.sh
++++ b/scripts/run_app.sh
+@@ -73,6 +73,8 @@ echo ""
+ echo -e "${YELLOW}   Press Ctrl+C to stop the server${NC}"
+ echo ""
+ 
+-# Run streamlit with the app
++# Run streamlit with the app (verbose mode for debugging)
+ source .venv/bin/activate
+-streamlit run src/clinical_analytics/ui/app.py
++streamlit run src/clinical_analytics/ui/app.py \
++    --logger.level=info \
++    --server.fileWatcherType=poll
diff --git a/.context/diffs/upload_data_progress_ui.diff b/.context/diffs/upload_data_progress_ui.diff
new file mode 100644
index 0000000..409d741
--- /dev/null
+++ b/.context/diffs/upload_data_progress_ui.diff
@@ -0,0 +1,328 @@
+diff --git "a/src/clinical_analytics/ui/pages/1_\360\237\223\244_Upload_Data.py" "b/src/clinical_analytics/ui/pages/1_\360\237\223\244_Upload_Data.py"
+index 2e3577b..9167c49 100644
+--- "a/src/clinical_analytics/ui/pages/1_\360\237\223\244_Upload_Data.py"
++++ "b/src/clinical_analytics/ui/pages/1_\360\237\223\244_Upload_Data.py"
+@@ -5,23 +5,33 @@ Self-service data upload for clinicians.
+ Upload CSV, Excel, or SPSS files without code or YAML configuration.
+ """
+ 
+-import streamlit as st
+-import pandas as pd
++import logging
+ import sys
+ from pathlib import Path
+-from typing import Optional
++
++import pandas as pd
++import streamlit as st
++
++# Configure logging for verbose output
++logging.basicConfig(
++    level=logging.INFO,
++    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
++    handlers=[
++        logging.StreamHandler(sys.stdout)
++    ]
++)
++
++# Set specific loggers to INFO/DEBUG for visibility
++logging.getLogger('clinical_analytics.core.multi_table_handler').setLevel(logging.INFO)
++logging.getLogger('clinical_analytics.ui.storage.user_datasets').setLevel(logging.INFO)
+ 
+ # Add src to path
+ sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))
+ 
+-from clinical_analytics.ui.storage.user_datasets import (
+-    UserDatasetStorage,
+-    UploadSecurityValidator
+-)
++from clinical_analytics.ui.components.data_validator import DataQualityValidator
+ from clinical_analytics.ui.components.variable_detector import VariableTypeDetector
+ from clinical_analytics.ui.components.variable_mapper import VariableMappingWizard
+-from clinical_analytics.ui.components.data_validator import DataQualityValidator
+-
++from clinical_analytics.ui.storage.user_datasets import UploadSecurityValidator, UserDatasetStorage
+ 
+ # Page configuration
+ st.set_page_config(
+@@ -38,23 +48,29 @@ def render_upload_step():
+     """Step 1: File Upload"""
+     st.markdown("## üì§ Upload Your Data")
+     st.markdown("""
+-    Upload your clinical dataset in CSV, Excel, or SPSS format.
++    Upload your clinical dataset in CSV, Excel, SPSS, or ZIP format.
+ 
+     **Supported formats:**
+-    - CSV (`.csv`)
+-    - Excel (`.xlsx`, `.xls`)
+-    - SPSS (`.sav`)
++    - CSV (`.csv`) - Single table
++    - Excel (`.xlsx`, `.xls`) - Single table
++    - SPSS (`.sav`) - Single table
++    - **ZIP (`.zip`) - Multi-table datasets** ‚≠ê NEW!
+ 
+     **Requirements:**
+     - File size: 1KB - 100MB
+     - Must include patient ID column
+     - Must include outcome variable
++
++    **Multi-Table ZIP Format:**
++    - ZIP file containing multiple CSV files
++    - Tables will be automatically joined (e.g., MIMIC-IV demo)
++    - Relationships detected via foreign keys
+     """)
+ 
+     uploaded_file = st.file_uploader(
+         "Choose a file",
+-        type=['csv', 'xlsx', 'xls', 'sav'],
+-        help="Maximum file size: 100MB",
++        type=['csv', 'xlsx', 'xls', 'sav', 'zip'],
++        help="Maximum file size: 100MB. ZIP files for multi-table datasets.",
+         key="file_uploader"
+     )
+ 
+@@ -80,10 +96,30 @@ def render_upload_step():
+ 
+         st.success("‚úÖ File validation passed")
+ 
+-        # Try to load preview
+-        try:
+-            file_ext = Path(uploaded_file.name).suffix.lower()
++        # Check if ZIP file (multi-table)
++        file_ext = Path(uploaded_file.name).suffix.lower()
++
++        if file_ext == '.zip':
++            # Handle multi-table ZIP upload
++            st.info("üóÇÔ∏è **Multi-table dataset detected!** ZIP file validated.")
+ 
++            # Store ZIP upload details
++            st.session_state['is_zip_upload'] = True
++            st.session_state['uploaded_filename'] = uploaded_file.name
++            st.session_state['uploaded_bytes'] = file_bytes
++
++            st.success("‚úÖ ZIP file ready for processing")
++            st.info("üí° **Next:** Click 'Continue to Review' to process tables, detect relationships, and build unified cohort.")
++
++            # Button to proceed to review step
++            if st.button("Continue to Review ‚û°Ô∏è", type="primary"):
++                st.session_state['upload_step'] = 5
++                st.rerun()
++
++            return uploaded_file
++
++        # Try to load preview for single-table files
++        try:
+             if file_ext == '.csv':
+                 df = pd.read_csv(uploaded_file)
+             elif file_ext in {'.xlsx', '.xls'}:
+@@ -95,7 +131,8 @@ def render_upload_step():
+                 st.error(f"Unsupported file type: {file_ext}")
+                 return None
+ 
+-            # Store in session state
++            # Store in session state (single-table)
++            st.session_state['is_zip_upload'] = False
+             st.session_state['uploaded_df'] = df
+             st.session_state['uploaded_filename'] = uploaded_file.name
+             st.session_state['uploaded_bytes'] = file_bytes
+@@ -309,10 +346,24 @@ def render_mapping_step(df: pd.DataFrame, variable_info: dict, suggestions: dict
+                 st.rerun()
+ 
+ 
+-def render_review_step(df: pd.DataFrame, mapping: dict, variable_info: dict):
++def render_review_step(df: pd.DataFrame = None, mapping: dict = None, variable_info: dict = None):
+     """Step 5: Final Review & Save"""
+     st.markdown("## ‚úÖ Review & Save Dataset")
+ 
++    # Check if this is a ZIP upload (multi-table)
++    is_zip = st.session_state.get('is_zip_upload', False)
++
++    if is_zip:
++        # Handle multi-table ZIP upload
++        return render_zip_review_step()
++
++    # Single-table workflow (existing logic)
++    if df is None or mapping is None or variable_info is None:
++        st.warning("Missing required data. Please start over.")
++        st.session_state['upload_step'] = 1
++        st.rerun()
++        return
++
+     # Run final validation with mapping
+     patient_id_col = mapping['patient_id']
+     outcome_col = mapping['outcome']
+@@ -416,6 +467,159 @@ def render_review_step(df: pd.DataFrame, mapping: dict, variable_info: dict):
+                 st.error(f"‚ùå {message}")
+ 
+ 
++def render_zip_review_step():
++    """Step 5: Review & Save for Multi-Table ZIP Upload"""
++    st.markdown("### üóÇÔ∏è Multi-Table Dataset Processing")
++
++    # Dataset name input
++    st.markdown("### üìù Dataset Name")
++    default_name = Path(st.session_state.get('uploaded_filename', 'dataset')).stem
++    dataset_name = st.text_input(
++        "Enter a name for this dataset",
++        value=default_name,
++        help="This name will be used to identify your dataset in the analysis interface"
++    )
++
++    # Process ZIP file
++    if st.button("üöÄ Process & Save Multi-Table Dataset", type="primary", disabled=not dataset_name.strip()):
++        # Create progress tracking UI elements
++        progress_bar = st.progress(0)
++        status_text = st.empty()
++        log_expander = st.expander("üìã Processing Log", expanded=True)
++        log_container = log_expander.container()
++
++
++        def progress_callback(step, total_steps, message, details):
++            """Update progress UI with current step information."""
++            progress = step / total_steps if total_steps > 0 else 0
++            progress_bar.progress(progress)
++            status_text.info(f"üîÑ {message}")
++
++            # Add to log
++            with log_container:
++                if details:
++                    if 'table_name' in details:
++                        table_info = f"**{details['table_name']}**"
++                        if 'rows' in details:
++                            table_info += f" - {details['rows']:,} rows, {details['cols']} cols"
++                        if 'progress' in details:
++                            table_info += f" ({details['progress']})"
++                        st.text(f"‚úì {table_info}")
++                    elif 'tables_found' in details:
++                        st.text(f"üì¶ Found {details['tables_found']} tables in ZIP")
++                        if 'table_names' in details:
++                            st.text(f"   Tables: {', '.join(details['table_names'][:5])}" +
++                                   (f" ... and {len(details['table_names']) - 5} more"
++                                    if len(details['table_names']) > 5 else ""))
++                    elif 'relationships' in details:
++                        st.text(f"üîó Detected {len(details['relationships'])} relationships")
++                        for rel in details['relationships'][:3]:  # Show first 3
++                            st.text(f"   ‚Ä¢ {rel}")
++                        if len(details['relationships']) > 3:
++                            st.text(f"   ... and {len(details['relationships']) - 3} more")
++                    else:
++                        st.text(f"‚Üí {message}")
++                else:
++                    st.text(f"‚Üí {message}")
++
++        # Prepare metadata
++        metadata = {
++            'dataset_name': dataset_name
++        }
++
++        # Save ZIP upload (this processes everything)
++        try:
++            success, message, upload_id = storage.save_zip_upload(
++                file_bytes=st.session_state['uploaded_bytes'],
++                original_filename=st.session_state['uploaded_filename'],
++                metadata=metadata,
++                progress_callback=progress_callback
++            )
++        except Exception as e:
++            import traceback
++            with log_container:
++                st.error(f"‚ùå Error during processing: {str(e)}")
++                st.code(traceback.format_exc())
++            status_text.error(f"‚ùå Processing failed: {str(e)}")
++            success = False
++            message = str(e)
++            upload_id = None
++
++        if success:
++            progress_bar.progress(1.0)
++            status_text.success(f"‚úÖ {message}")
++            with log_container:
++                st.success("‚úÖ Processing complete!")
++            st.balloons()
++
++            # Load metadata to show details
++            upload_metadata = storage.get_upload_metadata(upload_id)
++            if upload_metadata:
++                st.markdown("### üìä Processing Summary")
++
++                col1, col2, col3 = st.columns(3)
++                with col1:
++                    st.metric("Tables Joined", upload_metadata.get('tables', []) and len(upload_metadata.get('tables', [])) or 0)
++                with col2:
++                    st.metric("Unified Rows", upload_metadata.get('row_count', 0))
++                with col3:
++                    st.metric("Total Columns", upload_metadata.get('column_count', 0))
++
++                # Show detected relationships
++                relationships = upload_metadata.get('relationships', [])
++                if relationships:
++                    with st.expander(f"üîó Detected Relationships ({len(relationships)})"):
++                        for rel in relationships:
++                            st.code(rel)
++
++                # Show tables
++                tables = upload_metadata.get('tables', [])
++                table_counts = upload_metadata.get('table_counts', {})
++                if tables:
++                    with st.expander(f"üìã Tables ({len(tables)})"):
++                        for table in tables:
++                            count = table_counts.get(table, 0)
++                            st.markdown(f"- **{table}**: {count:,} rows")
++
++                # Show inferred schema
++                inferred_schema = upload_metadata.get('inferred_schema', {})
++                if inferred_schema:
++                    with st.expander("üî¨ Inferred Schema"):
++                        if inferred_schema.get('column_mapping'):
++                            st.markdown("**Column Mappings:**")
++                            for col, role in inferred_schema['column_mapping'].items():
++                                st.markdown(f"- `{col}` ‚Üí {role}")
++
++                        if inferred_schema.get('outcomes'):
++                            st.markdown("**Outcomes:**")
++                            for outcome, config in inferred_schema['outcomes'].items():
++                                st.markdown(f"- `{outcome}` ({config.get('type', 'unknown')})")
++
++            st.markdown(f"""
++            **Dataset saved successfully!**
++
++            - **Upload ID:** `{upload_id}`
++            - **Name:** {dataset_name}
++            - **Format:** Multi-table (ZIP)
++
++            You can now use this dataset in the main analysis interface.
++            """)
++
++            # Clear session state
++            if st.button("Upload Another Dataset"):
++                for key in list(st.session_state.keys()):
++                    if key.startswith('upload') or key == 'is_zip_upload':
++                        del st.session_state[key]
++                st.rerun()
++        else:
++            st.error(f"‚ùå {message}")
++
++    # Back button
++    if st.button("‚¨ÖÔ∏è Back to Upload"):
++        st.session_state['upload_step'] = 1
++        st.rerun()
++
++
+ def main():
+     """Main upload page logic"""
+     st.title("üì§ Upload Clinical Data")
+@@ -474,7 +678,13 @@ def main():
+             st.rerun()
+ 
+     elif current_step == 5:
+-        if all(k in st.session_state for k in ['uploaded_df', 'variable_mapping', 'variable_info']):
++        # Check if ZIP upload (skip to review directly)
++        is_zip = st.session_state.get('is_zip_upload', False)
++
++        if is_zip:
++            # For ZIP files, go directly to review (skip preview/detection/mapping)
++            render_review_step()
++        elif all(k in st.session_state for k in ['uploaded_df', 'variable_mapping', 'variable_info']):
+             render_review_step(
+                 st.session_state['uploaded_df'],
+                 st.session_state['variable_mapping'],
diff --git a/.context/diffs/user_datasets_progress_callback.diff b/.context/diffs/user_datasets_progress_callback.diff
new file mode 100644
index 0000000..345c928
--- /dev/null
+++ b/.context/diffs/user_datasets_progress_callback.diff
@@ -0,0 +1,325 @@
+diff --git a/src/clinical_analytics/ui/storage/user_datasets.py b/src/clinical_analytics/ui/storage/user_datasets.py
+index 06e53ea..501550c 100644
+--- a/src/clinical_analytics/ui/storage/user_datasets.py
++++ b/src/clinical_analytics/ui/storage/user_datasets.py
+@@ -4,11 +4,13 @@ User Dataset Storage Manager
+ Handles secure storage, metadata management, and persistence of uploaded datasets.
+ """
+ 
+-import json
+ import hashlib
++import json
++from collections.abc import Callable
+ from datetime import datetime
+ from pathlib import Path
+-from typing import Dict, List, Optional, Any
++from typing import Any
++
+ import pandas as pd
+ import polars as pl
+ 
+@@ -137,7 +139,7 @@ class UserDatasetStorage:
+     - Integration with existing registry system
+     """
+ 
+-    def __init__(self, upload_dir: Optional[Path] = None):
++    def __init__(self, upload_dir: Path | None = None):
+         """
+         Initialize storage manager.
+ 
+@@ -177,8 +179,8 @@ class UserDatasetStorage:
+         self,
+         file_bytes: bytes,
+         original_filename: str,
+-        metadata: Dict[str, Any]
+-    ) -> tuple[bool, str, Optional[str]]:
++        metadata: dict[str, Any]
++    ) -> tuple[bool, str, str | None]:
+         """
+         Save uploaded file with security validation.
+ 
+@@ -221,6 +223,7 @@ class UserDatasetStorage:
+             elif file_ext == '.sav':
+                 # SPSS file - convert to CSV
+                 import io
++
+                 import pyreadstat
+                 df, meta = pyreadstat.read_sav(io.BytesIO(file_bytes))
+                 csv_path = self.raw_dir / f"{upload_id}.csv"
+@@ -251,7 +254,7 @@ class UserDatasetStorage:
+         except Exception as e:
+             return False, f"Error saving upload: {str(e)}", None
+ 
+-    def get_upload_metadata(self, upload_id: str) -> Optional[Dict[str, Any]]:
++    def get_upload_metadata(self, upload_id: str) -> dict[str, Any] | None:
+         """
+         Retrieve metadata for an upload.
+ 
+@@ -266,10 +269,10 @@ class UserDatasetStorage:
+         if not metadata_path.exists():
+             return None
+ 
+-        with open(metadata_path, 'r') as f:
++        with open(metadata_path) as f:
+             return json.load(f)
+ 
+-    def get_upload_data(self, upload_id: str) -> Optional[pd.DataFrame]:
++    def get_upload_data(self, upload_id: str) -> pd.DataFrame | None:
+         """
+         Load uploaded dataset.
+ 
+@@ -286,7 +289,7 @@ class UserDatasetStorage:
+ 
+         return pd.read_csv(csv_path)
+ 
+-    def list_uploads(self) -> List[Dict[str, Any]]:
++    def list_uploads(self) -> list[dict[str, Any]]:
+         """
+         List all uploaded datasets.
+ 
+@@ -296,7 +299,7 @@ class UserDatasetStorage:
+         uploads = []
+ 
+         for metadata_file in self.metadata_dir.glob("*.json"):
+-            with open(metadata_file, 'r') as f:
++            with open(metadata_file) as f:
+                 metadata = json.load(f)
+                 uploads.append(metadata)
+ 
+@@ -337,7 +340,7 @@ class UserDatasetStorage:
+         except Exception as e:
+             return False, f"Error deleting upload: {str(e)}"
+ 
+-    def update_metadata(self, upload_id: str, metadata_updates: Dict[str, Any]) -> tuple[bool, str]:
++    def update_metadata(self, upload_id: str, metadata_updates: dict[str, Any]) -> tuple[bool, str]:
+         """
+         Update metadata for an upload.
+ 
+@@ -355,7 +358,7 @@ class UserDatasetStorage:
+ 
+         try:
+             # Load existing metadata
+-            with open(metadata_path, 'r') as f:
++            with open(metadata_path) as f:
+                 metadata = json.load(f)
+ 
+             # Update with new fields
+@@ -374,8 +377,9 @@ class UserDatasetStorage:
+         self,
+         file_bytes: bytes,
+         original_filename: str,
+-        metadata: Dict[str, Any]
+-    ) -> tuple[bool, str, Optional[str]]:
++        metadata: dict[str, Any],
++        progress_callback: Callable[[int, int, str, dict], None] | None = None
++    ) -> tuple[bool, str, str | None]:
+         """
+         Save uploaded ZIP file containing multiple CSV files.
+ 
+@@ -386,14 +390,16 @@ class UserDatasetStorage:
+             file_bytes: ZIP file content
+             original_filename: Original filename
+             metadata: Upload metadata
++            progress_callback: Optional callback function(step, total_steps, message, details)
+ 
+         Returns:
+             Tuple of (success, message, upload_id)
+         """
++        import io
++        import zipfile
++
+         from clinical_analytics.core.multi_table_handler import MultiTableHandler
+         from clinical_analytics.core.schema_inference import SchemaInferenceEngine
+-        import zipfile
+-        import io
+ 
+         # Security validation
+         valid, error = UploadSecurityValidator.validate(original_filename, file_bytes)
+@@ -404,41 +410,158 @@ class UserDatasetStorage:
+         upload_id = self.generate_upload_id(original_filename)
+ 
+         try:
++            import logging
++            logger = logging.getLogger(__name__)
++            logger.info(f"Starting ZIP upload processing: {original_filename}")
++
+             # Extract ZIP contents
+             zip_buffer = io.BytesIO(file_bytes)
+-            tables: Dict[str, pl.DataFrame] = {}
++            tables: dict[str, pl.DataFrame] = {}
+ 
+             with zipfile.ZipFile(zip_buffer, 'r') as zip_file:
+-                # Get list of CSV files in ZIP
+-                csv_files = [f for f in zip_file.namelist()
+-                           if f.endswith('.csv') and not f.startswith('__MACOSX')]
++                # Get list of CSV files in ZIP (including .csv.gz in subdirectories)
++                csv_files = [
++                    f for f in zip_file.namelist()
++                    if (f.endswith('.csv') or f.endswith('.csv.gz'))
++                    and not f.startswith('__MACOSX')
++                    and not f.endswith('/')  # Skip directory entries
++                ]
+ 
+                 if not csv_files:
+                     return False, "No CSV files found in ZIP archive", None
+ 
++                logger.info(f"Found {len(csv_files)} CSV files in ZIP archive")
++
++                # Calculate total steps now that we know number of files
++                # 1 (found tables) + len(csv_files) (loading) + 4 (detect, build, save, infer)
++                total_steps = 1 + len(csv_files) + 4
++
++                if progress_callback:
++                    progress_callback(0, total_steps, "Initializing ZIP extraction...", {})
++                    progress_callback(1, total_steps, f"Found {len(csv_files)} tables to load", {
++                        'tables_found': len(csv_files),
++                        'table_names': [Path(f).stem for f in csv_files]
++                    })
++
+                 # Load each CSV as a table
+-                for csv_filename in csv_files:
++                for idx, csv_filename in enumerate(csv_files, start=1):
++                    # Extract table name (without path and extension)
+                     table_name = Path(csv_filename).stem
++                    if table_name.endswith('.csv'):
++                        # Handle .csv.gz case where stem gives us "filename.csv"
++                        table_name = Path(table_name).stem
++
++                    logger.info(f"Loading table {idx}/{len(csv_files)}: {table_name} from {csv_filename}")
++
++                    if progress_callback:
++                        progress_callback(
++                            1 + idx,
++                            total_steps,
++                            f"Loading table: {table_name}",
++                            {
++                                'table_name': table_name,
++                                'file': csv_filename,
++                                'progress': f"{idx}/{len(csv_files)}"
++                            }
++                        )
++
++                    # Read file content
+                     csv_content = zip_file.read(csv_filename)
+ 
+-                    # Load as Polars DataFrame
+-                    df = pl.read_csv(io.BytesIO(csv_content))
++                    # Handle gzip compression
++                    if csv_filename.endswith('.gz'):
++                        import gzip
++                        logger.debug(f"Decompressing gzip file: {csv_filename}")
++                        csv_content = gzip.decompress(csv_content)
++
++                    # Load as Polars DataFrame with robust schema inference
++                    # Use larger infer_schema_length to handle mixed-type columns (e.g., ICD codes)
++                    try:
++                        logger.debug(f"Reading CSV with schema inference for {table_name}")
++                        df = pl.read_csv(
++                            io.BytesIO(csv_content),
++                            infer_schema_length=10000,  # Scan more rows for better type inference
++                            try_parse_dates=True
++                        )
++                    except Exception as e:
++                        logger.warning(f"Schema inference failed for {table_name}, falling back to string types: {e}")
++                        # Fallback: read with all columns as strings, let DuckDB handle types
++                        df = pl.read_csv(
++                            io.BytesIO(csv_content),
++                            infer_schema_length=0  # Treat all as strings
++                        )
++
+                     tables[table_name] = df
++                    logger.info(f"Loaded table '{table_name}': {df.height:,} rows, {df.width} cols")
++                    logger.debug(f"Schema for {table_name}: {dict(df.schema)}")
++
++                    if progress_callback:
++                        progress_callback(
++                            1 + idx,
++                            total_steps,
++                            f"Loaded {table_name}: {df.height:,} rows, {df.width} cols",
++                            {
++                                'table_name': table_name,
++                                'rows': df.height,
++                                'cols': df.width,
++                                'status': 'loaded'
++                            }
++                        )
++
++            logger.info(f"Extracted {len(tables)} tables from ZIP: {list(tables.keys())}")
+ 
+             # Detect relationships between tables
++            # Step calculation: 1 (init) + len(csv_files) (loading) = current step
++            step_num = 1 + len(csv_files)
++            logger.info(f"Detecting relationships for {len(tables)} tables")
++
++            if progress_callback:
++                progress_callback(step_num, total_steps, "Detecting table relationships...", {
++                    'tables': list(tables.keys()),
++                    'table_counts': {name: df.height for name, df in tables.items()}
++                })
++
+             handler = MultiTableHandler(tables)
+             relationships = handler.detect_relationships()
++            logger.info(f"Detected {len(relationships)} relationships")
++
++            if relationships:
++                for rel in relationships:
++                    logger.info(f"Relationship: {rel}")
++
++            if progress_callback:
++                progress_callback(step_num + 1, total_steps, f"Detected {len(relationships)} relationships", {
++                    'relationships': [str(rel) for rel in relationships]
++                })
+ 
+             # Build unified cohort
++            logger.info("Building unified cohort from detected relationships")
++            if progress_callback:
++                progress_callback(step_num + 2, total_steps, "Building unified cohort...", {})
++
+             unified_df = handler.build_unified_cohort()
++            logger.info(f"Unified cohort created: {unified_df.height:,} rows, {unified_df.width} cols")
+ 
+             # Save unified cohort as CSV
+             csv_path = self.raw_dir / f"{upload_id}.csv"
++            logger.info(f"Saving unified cohort to {csv_path}")
++            if progress_callback:
++                progress_callback(
++                    step_num + 3,
++                    total_steps,
++                    f"Saving unified cohort ({unified_df.height:,} rows)...",
++                    {}
++                )
+             unified_df.write_csv(csv_path)
+ 
+             # Infer schema for unified cohort
++            logger.info("Inferring schema for unified cohort")
++            if progress_callback:
++                progress_callback(step_num + 4, total_steps, "Inferring schema...", {})
++
+             engine = SchemaInferenceEngine()
+             schema = engine.infer_schema(unified_df)
++            logger.info("Schema inference complete")
+ 
+             # Save metadata
+             full_metadata = {
+@@ -463,7 +586,20 @@ class UserDatasetStorage:
+ 
+             handler.close()
+ 
+-            return True, f"Multi-table upload successful: {len(tables)} tables joined into {unified_df.height} rows", upload_id
++            if progress_callback:
++                progress_callback(step_num + 4, total_steps, "Processing complete!", {
++                    'tables': len(tables),
++                    'rows': unified_df.height,
++                    'cols': unified_df.width
++                })
++
++            logger.info(f"Multi-table upload successful: {len(tables)} tables joined into {unified_df.height:,} rows")
++            return True, f"Multi-table upload successful: {len(tables)} tables joined into {unified_df.height:,} rows", upload_id
+ 
+         except Exception as e:
++            import logging
++            import traceback
++            logger = logging.getLogger(__name__)
++            logger.error(f"Error processing ZIP upload: {type(e).__name__}: {str(e)}")
++            logger.error(f"Traceback: {traceback.format_exc()}")
+             return False, f"Error processing ZIP upload: {str(e)}", None
diff --git a/.context/sessions/docs-cleanup-duplicates.yaml b/.context/sessions/docs-cleanup-duplicates.yaml
new file mode 100644
index 0000000..917a940
--- /dev/null
+++ b/.context/sessions/docs-cleanup-duplicates.yaml
@@ -0,0 +1,66 @@
+schema_version: 1.0
+session_id: docs-cleanup-duplicates
+ticket: Documentation cleanup - remove duplicate files
+gate: Post-Phase 1 cleanup
+phase: Documentation file cleanup
+status: in_progress
+branch: feat/phase0-data-upload
+
+context:
+  specs:
+    - docs/implementation/plans/consolidate-docs-and-implement-question-driven-analysis.md
+  diffs: []
+  adrs: []
+  key_files:
+    - docs/README.md
+    - docs/architecture/ARCHITECTURE_OVERVIEW.md
+    - docs/architecture/overview.md
+    - docs/architecture/IBIS_SEMANTIC_LAYER.md
+    - docs/architecture/semantic-layer.md
+    - mkdocs.yml
+  notes: |
+    Identified duplicate files in docs/architecture/:
+    - ARCHITECTURE_OVERVIEW.md (duplicate of overview.md)
+    - IBIS_SEMANTIC_LAYER.md (duplicate of semantic-layer.md)
+    
+    MkDocs uses lowercase versions (overview.md, semantic-layer.md) per mkdocs.yml lines 73-74.
+    README.md still references uppercase versions in 11 locations.
+    
+    Action required:
+    1. Delete docs/architecture/ARCHITECTURE_OVERVIEW.md
+    2. Delete docs/architecture/IBIS_SEMANTIC_LAYER.md
+    3. Update docs/README.md to reference lowercase versions:
+       - architecture/ARCHITECTURE_OVERVIEW.md ‚Üí architecture/overview.md
+       - architecture/IBIS_SEMANTIC_LAYER.md ‚Üí architecture/semantic-layer.md
+
+prompt_template: |
+  Resume documentation cleanup work: remove duplicate architecture files and update references.
+  
+  Status: Identified duplicates, ready to delete and update references.
+  
+  Duplicate files identified:
+  - docs/architecture/ARCHITECTURE_OVERVIEW.md (duplicate of overview.md)
+  - docs/architecture/IBIS_SEMANTIC_LAYER.md (duplicate of semantic-layer.md)
+  
+  MkDocs configuration uses lowercase versions:
+  - architecture/overview.md (line 73 in mkdocs.yml)
+  - architecture/semantic-layer.md (line 74 in mkdocs.yml)
+  
+  README.md needs updates in 11 locations:
+  - Line 39: architecture/ARCHITECTURE_OVERVIEW.md ‚Üí architecture/overview.md
+  - Line 51: architecture/IBIS_SEMANTIC_LAYER.md ‚Üí architecture/semantic-layer.md
+  - Line 147: architecture/ARCHITECTURE_OVERVIEW.md ‚Üí architecture/overview.md
+  - Line 154: architecture/IBIS_SEMANTIC_LAYER.md ‚Üí architecture/semantic-layer.md
+  - Line 160: architecture/IBIS_SEMANTIC_LAYER.md ‚Üí architecture/semantic-layer.md
+  - Line 185: architecture/ARCHITECTURE_OVERVIEW.md ‚Üí architecture/overview.md
+  - Line 187: architecture/IBIS_SEMANTIC_LAYER.md ‚Üí architecture/semantic-layer.md
+  - Line 225: architecture/ARCHITECTURE_OVERVIEW.md ‚Üí architecture/overview.md
+  - Line 226: architecture/IBIS_SEMANTIC_LAYER.md ‚Üí architecture/semantic-layer.md
+  - Line 265: architecture/ARCHITECTURE_OVERVIEW.md ‚Üí architecture/overview.md
+  - Line 292: architecture/ARCHITECTURE_OVERVIEW.md ‚Üí architecture/overview.md
+  
+  Tasks:
+  1. Delete duplicate files
+  2. Update all README.md references to lowercase versions
+  3. Verify no broken links remain
+
diff --git a/.context/sessions/multi-table-progress-visibility-debugging.yaml b/.context/sessions/multi-table-progress-visibility-debugging.yaml
new file mode 100644
index 0000000..29d8c5c
--- /dev/null
+++ b/.context/sessions/multi-table-progress-visibility-debugging.yaml
@@ -0,0 +1,84 @@
+schema_version: "1.0"
+session_id: "multi-table-progress-visibility-debugging"
+ticket: "Multi-Table Progress Visibility and Verbose Logging"
+gate: "Implementation/Debugging"
+status: "in_progress"
+
+context:
+  specs:
+    - "docs/specs/multi-table-support.md"
+    - "docs/specs/spec_clinical_analytics_platform.md"
+  
+  adrs: []
+  
+  diffs:
+    - ".context/diffs/user_datasets_progress_callback.diff"
+    - ".context/diffs/upload_data_progress_ui.diff"
+    - ".context/diffs/run_app_verbose.diff"
+  
+  key_files:
+    - "src/clinical_analytics/ui/storage/user_datasets.py"
+    - "src/clinical_analytics/ui/pages/1_üì§_Upload_Data.py"
+    - "src/clinical_analytics/core/multi_table_handler.py"
+    - "scripts/run_app.sh"
+    - "tests/ui/test_zip_upload_progress.py"
+  
+  branch: null
+  
+  dependencies_added: []
+
+prompt_template: |
+  Resume work on multi-table progress visibility and verbose logging implementation.
+  
+  Current state:
+  - Progress callback system added to save_zip_upload() in user_datasets.py
+  - UI progress bars and detailed logging added to Upload_Data.py page
+  - Streamlit launch script updated for verbose mode (--logger.level=info)
+  - Comprehensive test suite created (test_zip_upload_progress.py) - all 6 tests passing
+  - Verbose logging working: shows individual table loading with row/column counts
+  - Relationship detection logging: shows 94 relationships detected for MIMIC-IV demo
+  - Progress tracking: real-time updates for each table (1/32, 2/32, etc.)
+  
+  Completed:
+  - ‚úÖ Progress callback parameter added to save_zip_upload()
+  - ‚úÖ Progress updates at each step: initialization, table discovery, loading, relationship detection, cohort building, saving, schema inference
+  - ‚úÖ UI progress bar and status text in Streamlit
+  - ‚úÖ Expandable processing log showing each table as it loads
+  - ‚úÖ Detailed table information (rows, cols) in progress callbacks
+  - ‚úÖ Logging configuration with INFO level for multi_table_handler and user_datasets
+  - ‚úÖ All tests passing (6/6) verifying progress callback functionality
+  - ‚úÖ Fixed indentation errors
+  - ‚úÖ Fixed ruff linting issues (whitespace, imports, type hints)
+  - ‚úÖ Fixed step calculation bug (total_steps consistency)
+  
+  Current issue:
+  - ‚ùå DuckDB OutOfMemoryException when joining all 32 tables in single query
+  - Error: "failed to offload data block of size 32.0 KiB (90.8 GiB/90.8 GiB used)"
+  - Large tables like chartevents (668,862 rows) causing memory explosion
+  - Single massive LEFT JOIN of all tables exhausts temp directory space
+  
+  Proposed solutions (not yet implemented):
+  1. Configure DuckDB with better memory settings (memory_limit, threads, temp_directory)
+  2. Use incremental joins instead of single massive join
+  3. Selective joins - exclude very large tables by default (e.g., chartevents > 100k rows)
+  
+  Key decisions:
+  - Progress callback signature: (step: int, total_steps: int, message: str, details: dict) -> None
+  - Total steps calculation: 1 (init) + len(csv_files) (loading) + 4 (detect, build, save, infer)
+  - UI shows progress bar, status text, and expandable log container
+  - Logging at INFO level for visibility without overwhelming output
+  - Tests verify callback is called, receives correct data, and handles edge cases
+  
+  Next steps:
+  1. Fix DuckDB memory issue - configure connection with appropriate limits
+  2. Consider incremental join strategy for very large datasets
+  3. Add option to exclude large tables from unified cohort
+  4. Test with full MIMIC-IV demo dataset (32 tables, some with 600k+ rows)
+
+notes: |
+  - Verbose logging successfully implemented and working
+  - Progress visibility excellent: users can see each table loading individually
+  - Relationship detection working well: 94 relationships detected with high confidence
+  - Memory issue discovered when processing full MIMIC-IV demo with 32 tables
+  - Need to optimize join strategy for large multi-table datasets
+
diff --git a/.context/sessions/phase1-phase2-docs-nlquery.yaml b/.context/sessions/phase1-phase2-docs-nlquery.yaml
new file mode 100644
index 0000000..4481344
--- /dev/null
+++ b/.context/sessions/phase1-phase2-docs-nlquery.yaml
@@ -0,0 +1,83 @@
+schema_version: 1.0
+session_id: phase1-phase2-docs-nlquery
+ticket: Phase 1+2: Documentation Infrastructure & Natural Language Query Engine
+gate: Phase 1 and Phase 2
+phase: Phase 1 (Documentation Infrastructure) and Phase 2 (NL Query Engine + UI Integration)
+status: completed
+pr: https://github.com/jtouley/md_data_explorer/pull/2
+branch: feat/phase0-data-upload
+
+context:
+  specs:
+    - docs/implementation/plans/consolidate-docs-and-implement-question-driven-analysis.md
+  diffs:
+    - commits:
+        - aaf18f5: fix(deps): add missing seaborn dependency
+        - f61d201: feat(ui): integrate NL query engine with Streamlit UI
+        - dcb85cd: feat(nl-query): implement Phase 2 natural language query engine core
+        - a3f45b2: feat(docs): implement Phase 1 documentation infrastructure with MkDocs
+        - 58dec2b: docs: reorganize documentation structure for better discoverability
+  adrs: []
+  key_files:
+    - src/clinical_analytics/core/nl_query_engine.py
+    - src/clinical_analytics/ui/components/question_engine.py
+    - src/clinical_analytics/ui/pages/7_üî¨_Analyze.py
+    - mkdocs.yml
+    - .github/workflows/docs.yml
+    - docs/index.md
+    - docs/getting-started/installation.md
+    - docs/getting-started/quick-start.md
+    - docs/getting-started/uploading-data.md
+    - docs/user-guide/question-driven-analysis.md
+    - docs/user-guide/statistical-tests.md
+    - docs/user-guide/interpreting-results.md
+    - docs/architecture/overview.md
+    - docs/architecture/semantic-layer.md
+    - docs/architecture/dataset-registry.md
+    - docs/architecture/nl-query-engine.md
+    - docs/api-reference/core.md
+    - docs/api-reference/datasets.md
+    - docs/api-reference/ui.md
+    - docs/development/contributing.md
+    - docs/development/setup.md
+    - docs/development/testing.md
+    - docs/specs/question-driven-analysis.md
+    - docs/specs/multi-table-support.md
+
+prompt_template: |
+  Resume work on Phase 1+2 implementation: Documentation Infrastructure & Natural Language Query Engine.
+  
+  Status: Completed. PR #2 created on branch feat/phase0-data-upload.
+  
+  Phase 1 (Documentation Infrastructure) - Completed:
+  - MkDocs site with Material theme configured
+  - 23 documentation files created across getting-started, user-guide, architecture, api-reference, development, specs
+  - GitHub Actions workflow for auto-deployment
+  - Auto-generated API reference from docstrings
+  
+  Phase 2 (NL Query Engine) - Completed:
+  - Core NL Query Engine implemented (src/clinical_analytics/core/nl_query_engine.py)
+  - Three-tier architecture: regex patterns, semantic embeddings, LLM fallback stub
+  - UI integration in QuestionEngine component (ask_free_form_question method)
+  - Unified Analysis page updated to default to NL query input
+  - Bidirectional switching between NL and structured questions
+  - Dependencies added: sentence-transformers, scikit-learn, seaborn
+  
+  Key deliverables:
+  - Natural language query parsing with confidence scoring
+  - QueryIntent ‚Üí AnalysisContext conversion
+  - Professional documentation site ready for deployment
+  - All code tested and running successfully
+  
+  Next steps (if continuing):
+  - Phase 3: Schema Inference Engine
+  - Phase 4: Multi-Table Handler
+  - Phase 5: Advanced NL Query Patterns
+
+notes: |
+  - Streamlit app tested and running at http://localhost:8501
+  - All dependencies resolved (seaborn added to fix missing import)
+  - Documentation builds successfully with only minor warnings about broken links to future features
+  - NL Query Engine uses local processing (Tier 1 & 2) for privacy-preserving queries
+  - UI defaults to free-form NL query input with fallback to structured questions
+
diff --git a/.context/sessions/phase3-4-schema-multitable.yaml b/.context/sessions/phase3-4-schema-multitable.yaml
new file mode 100644
index 0000000..6b32533
--- /dev/null
+++ b/.context/sessions/phase3-4-schema-multitable.yaml
@@ -0,0 +1,87 @@
+schema_version: "1.0"
+session_id: "phase3-4-schema-multitable"
+ticket: "Phase 3 & 4: Automatic Schema Inference & Multi-Table Support"
+gate: "Phase 3 & 4 Implementation"
+status: "in_progress"
+
+context:
+  specs:
+    - "docs/implementation/plans/consolidate-docs-and-implement-question-driven-analysis.md"
+    - "docs/specs/multi-table-support.md"
+    - "docs/architecture/DATASET_STRUCTURE_PATTERNS.md"
+  
+  adrs: []
+  
+  key_files:
+    - "src/clinical_analytics/core/schema_inference.py"
+    - "src/clinical_analytics/core/multi_table_handler.py"
+    - "src/clinical_analytics/core/registry.py"
+    - "src/clinical_analytics/ui/storage/user_datasets.py"
+    - "docs/implementation/plans/consolidate-docs-and-implement-question-driven-analysis.md"
+  
+  branch: "feat/phase3-4-schema-inference-multi-table"
+  
+  dependencies_added:
+    - "langchain ^1.2.0"
+    - "langchain-community ^0.4.1"
+
+prompt_template: |
+  Resume work on Phase 3 & 4 implementation: Automatic Schema Inference and Multi-Table Support.
+  
+  Current state:
+  - Schema inference engine implemented with Polars/DuckDB-only operations
+  - DictionaryMetadata dataclass added for PDF-extracted metadata
+  - parse_dictionary_pdf() implemented using LangChain PyPDFLoader
+  - infer_schema_with_dictionary() merges DataFrame analysis + PDF dictionary
+  - MultiTableHandler implemented with automatic relationship detection
+  - ZIP upload support added to user_datasets.py
+  - Plan document updated with dictionary integration
+  
+  Completed:
+  - ‚úÖ SchemaInferenceEngine with patient ID, outcome, time column detection
+  - ‚úÖ DictionaryMetadata dataclass for PDF metadata storage
+  - ‚úÖ PDF parsing with LangChain (parse_dictionary_pdf method)
+  - ‚úÖ Schema merging logic (infer_schema_with_dictionary)
+  - ‚úÖ MultiTableHandler with primary/foreign key detection
+  - ‚úÖ TableRelationship dataclass for FK relationships
+  - ‚úÖ ZIP upload handler (save_zip_upload) for multi-table datasets
+  - ‚úÖ DatasetRegistry.register_from_dataframe() method
+  - ‚úÖ Plan document updated with Phase 3 & 4 progress
+  
+  Missing/Incomplete:
+  - ‚ùå find_dictionary_pdf() method to locate PDFs in data/dictionaries/
+  - ‚ùå Main infer_schema() doesn't automatically use dictionaries (only infer_schema_with_dictionary does)
+  - ‚ùå Dataset name mapping logic (e.g., "covid_ms" ‚Üí PDF filename matching)
+  - ‚ùå UI component to display dictionary info when dataset selected
+  - ‚ùå Registry integration: register_from_dataframe() doesn't pass dataset_name for dictionary lookup
+  - ‚ùå Testing with MIMIC-IV demo ZIP file
+  
+  Key decisions:
+  - Use Polars and DuckDB only (no pandas operations)
+  - LangChain for PDF parsing (better than PyPDF2 for structured extraction)
+  - Dictionary metadata enhances but doesn't replace DataFrame-based inference
+  - Multi-table handler uses DuckDB for SQL-based joins
+  - ZIP upload extracts CSVs, detects relationships, builds unified cohort
+  
+  Data dictionaries location:
+  - data/dictionaries/ contains PDFs for COVID-MS, Sepsis, VAP datasets
+  - Need to add MIMIC-IV dictionary when available
+  - Dictionary matching: dataset name ‚Üí PDF filename (fuzzy matching)
+  
+  Next steps:
+  1. Add find_dictionary_pdf() method to SchemaInferenceEngine
+  2. Update infer_schema() to accept dataset_name parameter and auto-load dictionary
+  3. Update register_from_dataframe() to pass dataset_name to schema inference
+  4. Create dictionary_viewer.py UI component
+  5. Integrate dictionary display into dataset selection pages
+  6. Test with MIMIC-IV demo ZIP (mimic-iv-clinical-database-demo-2.2.zip)
+  7. Extract MIMIC-IV README and add to data/dictionaries/
+  8. Test dictionary matching for all datasets (covid_ms, sepsis, mimic_iv_demo)
+
+notes: |
+  - Dictionary integration was added per user request to bring data dictionaries into context before users see data
+  - MIMIC-IV demo ZIP file available for testing multi-table support
+  - MIMIC-IV structure: patients (subject_id) ‚Üí admissions (hadm_id) ‚Üí other tables
+  - All operations must use Polars/DuckDB only (no pandas)
+  - Dictionary parsing uses LangChain PyPDFLoader for better text extraction
+
diff --git a/.context/sessions/question-driven-nl-semantic.yaml b/.context/sessions/question-driven-nl-semantic.yaml
new file mode 100644
index 0000000..cde3317
--- /dev/null
+++ b/.context/sessions/question-driven-nl-semantic.yaml
@@ -0,0 +1,61 @@
+schema_version: "1.0"
+session_id: "question-driven-nl-semantic"
+ticket: "Question-driven analysis with NL queries and semantic layer integration"
+gate: "Phase 0 - Clinician Workflows"
+phase: "Design/Planning"
+status: "Recommendations made, implementation pending"
+
+context:
+  topic: "Enhancing question-driven analysis interface with natural language query understanding and semantic layer integration"
+  
+  specs:
+    - "docs/NL_QUERY_BEST_PRACTICES.md"
+    - "docs/PHASE_0_CLINICIAN_WORKFLOWS.md"
+    - "docs/IBIS_SEMANTIC_LAYER.md"
+    - "docs/user-guide/question-driven-analysis.md"
+  
+  key_files:
+    - "src/clinical_analytics/ui/components/question_engine.py"
+    - "src/clinical_analytics/ui/components/analysis_wizard.py"
+    - "src/clinical_analytics/core/semantic.py"
+    - "src/clinical_analytics/ui/pages/7_üî¨_Analyze.py"
+  
+  adrs: []
+  
+  diffs: []
+
+prompt_template: |
+  Resume work on question-driven analysis enhancement: integrate natural language query understanding with semantic layer metadata.
+  
+  Current state:
+  - QuestionEngine uses structured questions (radio buttons)
+  - Analysis wizard has hardcoded analysis type pages
+  - Semantic layer provides config-driven metadata (outcomes, variables, relationships)
+  - NL_QUERY_BEST_PRACTICES.md documents modern approaches (embeddings, RAG, transformers)
+  
+  Goal:
+  - Add free-form natural language input to QuestionEngine
+  - Use semantic embeddings for intent classification (sentence-transformers)
+  - Integrate semantic layer metadata for RAG-based query understanding
+  - Implement hybrid approach: free-form NL + structured questions as fallback
+  - Extract entities (variables, outcomes) from queries using semantic matching
+  
+  Key decisions:
+  - Use semantic embeddings (Tier 2) as primary approach, pattern matching (Tier 1) for speed
+  - Leverage semantic layer config for variable matching and context
+  - Follow Looker RAG pattern: semantic layer provides metadata, embeddings match queries
+  - MIMIC-IV Demo identified as test dataset opportunity
+  
+  Next steps:
+  1. Enhance QuestionEngine.parse_natural_language_query() with semantic embeddings
+  2. Integrate semantic layer metadata for variable matching
+  3. Add free-form text input to Analyze.py page
+  4. Test with MIMIC-IV Demo dataset
+
+notes: |
+  - Current QuestionEngine uses structured questions, needs free-form NL support
+  - Semantic layer already provides rich metadata (outcomes, variables, metrics, dimensions)
+  - NL_QUERY_BEST_PRACTICES recommends: embeddings for intent (75-85% accuracy), RAG with semantic layer (66% error reduction)
+  - MIMIC-IV Demo (100 patients, open access) identified as integration test case
+  - Hybrid approach approved: NL queries with structured fallback for low confidence
+
diff --git a/.cursor/commands/snapshot.md b/.cursor/commands/snapshot.md
new file mode 100644
index 0000000..e64fcc0
--- /dev/null
+++ b/.cursor/commands/snapshot.md
@@ -0,0 +1,67 @@
+
+SNAPSHOT RULE ‚Äì SESSION CAPTURE
+
+Role
+You are a deterministic session snapshot generator for Cursor.
+
+Trigger
+This rule is activated when the user types the command:
+
+snapshot
+
+Primary Objective
+Serialize the current development session into a replayable YAML session file and update the active session rule.
+
+Responsibilities
+	1.	Infer session metadata from the conversation:
+
+	‚Ä¢	Ticket or topic identifier
+	‚Ä¢	Current gate or phase if stated or implied
+	‚Ä¢	Referenced specs
+	‚Ä¢	Referenced ADRs
+	‚Ä¢	Referenced diffs, PRs, or key files
+
+	2.	Generate a session ID using the format:
+<ticket_or_topic><gate_or_phase>
+	3.	Create a YAML session file at:
+.context/sessions/<session_id>.yaml
+	4.	Populate the session file with:
+
+	‚Ä¢	schema_version
+	‚Ä¢	session_id
+	‚Ä¢	ticket or topic
+	‚Ä¢	gate or phase
+	‚Ä¢	status
+	‚Ä¢	context:
+	‚Ä¢	specs
+	‚Ä¢	diffs (write these to .context/diffs)
+	‚Ä¢	adrs
+	‚Ä¢	key_files
+	‚Ä¢	prompt_template that resumes work deterministically
+	‚Ä¢	optional notes capturing approved decisions or constraints
+
+	5.	Overwrite the active session rule at:
+.cursor/rules/active-session.mdc
+
+The active session rule must reference:
+	‚Ä¢	the session identifier
+	‚Ä¢	the resume prompt template
+	‚Ä¢	the associated session file path
+
+	6.	Confirm completion with a single message:
+Session <session_id> snapshot saved.
+
+Constraints
+	‚Ä¢	Do not invent files or references.
+	‚Ä¢	Prefer diffs over full files when both exist.
+	‚Ä¢	Do not include speculative context.
+	‚Ä¢	Treat this snapshot as local working state, not governance.
+	‚Ä¢	Be explicit and deterministic.
+
+Output Rules
+	‚Ä¢	Do not include markdown formatting.
+	‚Ä¢	Do not include commentary or explanation.
+	‚Ä¢	Do not ask follow-up questions.
+	‚Ä¢	Only perform the snapshot behavior when explicitly triggered by the snapshot command.
+
+End of rule.
\ No newline at end of file
diff --git a/.cursor/plans/fix_expensive_full-scan_operations_in_multi-table_handler_classification_a3690050.plan.md b/.cursor/plans/fix_expensive_full-scan_operations_in_multi-table_handler_classification_a3690050.plan.md
new file mode 100644
index 0000000..b117602
--- /dev/null
+++ b/.cursor/plans/fix_expensive_full-scan_operations_in_multi-table_handler_classification_a3690050.plan.md
@@ -0,0 +1,460 @@
+---
+name: ""
+overview: ""
+todos: []
+---
+
+---
+
+name: Fix Expensive Full-Scan Operations in Multi-Table Handler Classification
+
+overview: ""
+
+todos:
+
+  - id: "1"
+
+content: Add _sample_df() helper method using head(n) or stride sampling (NOT df.sample() - deterministic, fast, stable)
+
+status: completed
+
+  - id: "2"
+
+content: Add _is_probably_id_col() helper to tighten ID pattern matching (exact 'id' or endswith('_id'), not endswith('id'))
+
+status: completed
+
+  - id: "3"
+
+content: Refactor _detect_grain_key() with explicit scoring formula: penalize row-level IDs (event_id, row_id, uuid), prefer relationship keys, use sampled uniqueness
+
+status: completed
+
+dependencies:
+
+      - "1"
+      - "2"
+  - id: "4"
+
+content: Refactor _detect_time_column() to remove 'dt' pattern, check dtypes first, use sampled uniqueness
+
+status: completed
+
+dependencies:
+
+      - "1"
+  - id: "5"
+
+content: Refactor _detect_bridge_table() to use structural heuristics + sampled composite uniqueness
+
+status: completed
+
+dependencies:
+
+      - "1"
+  - id: "6"
+
+content: Fix classify_tables() cardinality calculation to use sampled uniqueness
+
+status: completed
+
+dependencies:
+
+      - "1"
+  - id: "7"
+
+content: "Add acceptance test: grain_key_fallback_prefers_patient_over_event"
+
+status: completed
+
+dependencies:
+
+      - "3"
+  - id: "8"
+
+content: "Add acceptance test: id_pattern_does_not_match_false_positives (valid/fluid/paid)"
+
+status: completed
+
+dependencies:
+
+      - "2"
+      - "3"
+  - id: "9"
+
+content: "Add performance guardrail test: classification_uses_sampled_helpers_only (monkeypatch our own helpers, not Polars internals)"
+
+status: completed
+
+dependencies:
+
+      - "3"
+      - "4"
+      - "5"
+      - "6"
+  - id: "11"
+
+content: "Add strict acceptance gate: test_classification_1m_rows_completes_within_3_seconds (wall-clock bound)"
+
+status: completed
+
+dependencies:
+
+      - "1"
+      - "3"
+      - "4"
+      - "5"
+      - "6"
+  - id: "10"
+
+content: "Add performance guardrail test: sampling_bounds_classification_cost (timing test)"
+
+status: completed
+
+dependencies:
+
+      - "1"
+
+---
+
+# Fix Expensive Full-Scan Operations in Multi-Table Handler Classification
+
+## Overview
+
+Fix critical performance and correctness issues in `MultiTableHandler` classification methods that cause expensive full-table scans and incorrect grain key selection. Replace full scans with deterministic sampling and tighten pattern matching to prevent misclassification.
+
+## Problems Identified
+
+1. **`_detect_grain_key()`**: 
+
+   - Full scans via `df[c].n_unique() `on all `*_id` columns
+   - Naive fallback picks highest uniqueness (event_id over patient_id)
+   - `endswith('id')` matches false positives (valid, fluid, paid)
+
+2. **`_detect_time_column()`**: 
+
+   - Full scans via `df[col].n_unique()` 
+   - `'dt'` pattern too broad (matches dt_code, mdt_flag)
+
+3. **`_detect_bridge_table()`**: 
+
+   - Full scans via `df[col].n_unique()` for each FK
+   - Full-table distinct on composite key `df.select([fk1, fk2]).n_unique()`
+
+4. **`classify_tables()`**: 
+
+   - Full scan via `df[grain_key].n_unique()` at line 317
+
+## Solution Architecture
+
+### Sampling Strategy
+
+Add deterministic sampling helper that bounds cost and keeps tests stable. **Do NOT use `df.sample()`** - even with seed=0, it's extra work and can behave differently across Polars versions.
+
+**Option A: head(n)** (fastest, stable, but can bias if file is sorted):
+
+```python
+def _sample_df(self, df: pl.DataFrame, n: int = 10_000) -> pl.DataFrame:
+    """Deterministic sample using head (fastest, stable)."""
+    return df.head(min(n, df.height))
+```
+
+**Option B: Stride sample** (more representative, still deterministic, avoids sorted bias):
+
+```python
+def _sample_df(self, df: pl.DataFrame, n: int = 10_000) -> pl.DataFrame:
+    """Deterministic stride sample (more representative than head)."""
+    if df.height <= n:
+        return df
+    step = max(df.height // n, 1)
+    return df.slice(0, df.height).take(pl.arange(0, df.height, step)[:n])
+```
+
+**Recommendation**: Start with Option A (head) for simplicity. If classification shows bias on sorted data, switch to Option B.
+
+### Pattern Matching Fixes
+
+- Tighten ID column detection: `n == "id" or n.endswith("_id")` (not `endswith('id')`)
+- Remove `'dt'` from time patterns (use only: `time`, `date`, `timestamp`, `datetime`)
+- Prefer explicit keys by name before fallback to uniqueness
+
+## Implementation Changes
+
+### 1. Add Sampling Helper and Centralize Uniqueness Computations
+
+**File**: `src/clinical_analytics/core/multi_table_handler.py`
+
+Add `_sample_df()` method after `_normalize_key_columns()` (around line 222).
+
+**Critical**: Centralize all uniqueness/null rate computations behind helper methods that only accept sampled frames. This ensures we never accidentally do full scans and makes testing easier:
+
+```python
+def _compute_sampled_uniqueness(self, df: pl.DataFrame, col: str) -> tuple[int, float]:
+    """Compute uniqueness on sample only."""
+    s = self._sample_df(df)
+    non_null = s[col].drop_nulls()
+    if non_null.len() == 0:
+        return 0, 1.0
+    unique_count = non_null.n_unique()
+    null_rate = s[col].null_count() / max(s.height, 1)
+    return unique_count, null_rate
+```
+
+All classification methods should use these helpers, never call `df[col].n_unique()` directly on the original DataFrame.
+
+### 2. Fix `_detect_grain_key()`
+
+**File**: `src/clinical_analytics/core/multi_table_handler.py` (lines 375-409)
+
+**Changes**:
+
+- Add `_is_probably_id_col()` helper (exact `"id"` or `endswith("_id")`)
+- Prioritize explicit keys by name (patient_id, subject_id, hadm_id, etc.)
+- Fallback: use sampled uniqueness with scoring (penalize row-level IDs, penalize nulls)
+- Never do full `df[c].n_unique()` on base DataFrame
+
+**New logic**:
+
+1. Check explicit patient/admission keys first (by name)
+2. If no match, find `*_id` candidates using tightened pattern
+3. Score candidates on sampled data using explicit formula that:
+
+   - Hard-penalizes row-level ID patterns: `event_id`, `row_id`, `*_event_id`, `uuid`, `guid`
+   - Prefers IDs that appear as relationship keys across tables (once relationships are detected)
+   - Uses sampled uniqueness and null rate
+
+**Scoring formula** (on sample `s`):
+
+```python
+uniq_ratio = s[col].drop_nulls().n_unique() / max(s.height - s[col].null_count(), 1)
+null_rate = s[col].null_count() / max(s.height, 1)
+
+score = 0.0
+score -= 2.0 * uniq_ratio          # penalize row-level IDs (uniq ~ 1.0)
+score -= 1.0 * null_rate
+score += 1.0 if col.endswith("_id") else 0.0
+score += 2.0 if col in ("patient_id","subject_id","hadm_id","encounter_id","visit_id") else 0.0
+score -= 5.0 if any(tok in col for tok in ("event","row","uuid","guid")) else 0.0
+```
+
+This ensures `patient_id` beats `event_id` even if `event_id` is perfectly unique.
+
+### 3. Fix `_detect_time_column()`
+
+**File**: `src/clinical_analytics/core/multi_table_handler.py` (lines 478-508)
+
+**Changes**:
+
+- Remove `'dt'` from time_patterns
+- Check temporal dtypes first (pl.Date, pl.Datetime)
+- Use sampled uniqueness check: `s[c].drop_nulls().n_unique() > 1`
+- Never do full `df[col].n_unique()` on base DataFrame
+
+### 4. Fix `_detect_bridge_table()`
+
+**File**: `src/clinical_analytics/core/multi_table_handler.py` (lines 525-587)
+
+**Changes**:
+
+- Add structural heuristic: exclude if `df.width >= 15` (bridges are narrow)
+- Use sampled composite uniqueness: `s.select(...).n_unique()` on sample
+- Never do full `df[col].n_unique()` or `df.select([fk1, fk2]).n_unique()`
+
+### 5. Fix `classify_tables()` Cardinality Calculation
+
+**File**: `src/clinical_analytics/core/multi_table_handler.py` (line 317)
+
+**Changes**:
+
+- Use sampled uniqueness for cardinality_ratio calculation
+- Or: use `df[grain_key].value_counts()` on sample to estimate ratio
+
+**Note**: For exact cardinality_ratio, we may need full scan, but we can:
+
+- Use sample-based estimate for classification decisions
+- Or: defer exact calculation until after classification (lazy evaluation)
+
+## Testing Strategy
+
+### Acceptance Tests
+
+**File**: `tests/core/test_multi_table_handler.py`
+
+#### Test 1: Grain Key Fallback Prefers Patient Over Event
+
+```python
+def test_grain_key_fallback_prefers_patient_over_event():
+    """Acceptance: Table with patient_id and event_id picks patient_id even if event_id is more unique."""
+    # Arrange: event_id is more unique (row-level ID)
+    df = pl.DataFrame({
+        "patient_id": ["P1", "P1", "P2", "P2"],  # 2 unique
+        "event_id": ["E1", "E2", "E3", "E4"],    # 4 unique (more unique!)
+        "value": [100, 200, 300, 400]
+    })
+    
+    # Act
+    handler = MultiTableHandler({"test": df})
+    grain_key = handler._detect_grain_key(df)
+    
+    # Assert: Should pick patient_id (explicit key) not event_id
+    assert grain_key == "patient_id"
+```
+
+#### Test 2: ID Pattern Does Not Match False Positives
+
+```python
+def test_id_pattern_does_not_match_false_positives():
+    """Acceptance: endswith('id') does not match 'valid', 'fluid', 'paid'."""
+    df = pl.DataFrame({
+        "valid": [True, False, True],
+        "fluid": [100, 200, 300],
+        "paid": [10.5, 20.5, 30.5],
+        "patient_id": ["P1", "P2", "P3"]
+    })
+    
+    handler = MultiTableHandler({"test": df})
+    grain_key = handler._detect_grain_key(df)
+    
+    # Should pick patient_id, not any of the false positives
+    assert grain_key == "patient_id"
+```
+
+### Performance Guardrail Tests
+
+**File**: `tests/core/test_multi_table_handler.py`
+
+#### Test 3: Classification Uses Sampled Helpers Only
+
+```python
+def test_classification_uses_sampled_helpers_only(monkeypatch):
+    """Performance guardrail: Enforce that classification uses _sample_df() and never touches df[...] for uniqueness on original frame."""
+    # Arrange: Create large DataFrame
+    large_df = pl.DataFrame({
+        "patient_id": [f"P{i}" for i in range(1_000_000)],
+        "value": list(range(1_000_000))
+    })
+    
+    # Track calls to _sample_df() - all uniqueness checks should go through it
+    sample_df_calls = []
+    original_sample_df = MultiTableHandler._sample_df
+    
+    def tracked_sample_df(self, df, n=10_000):
+        sample_df_calls.append((df.height, n))
+        return original_sample_df(self, df, n)
+    
+    monkeypatch.setattr(MultiTableHandler, "_sample_df", tracked_sample_df)
+    
+    # Act
+    handler = MultiTableHandler({"large": large_df})
+    handler.classify_tables()
+    
+    # Assert: _sample_df() was called (proving we're using sampling, not full scans)
+    assert len(sample_df_calls) > 0, (
+        "Classification should use _sample_df() for all uniqueness checks"
+    )
+    
+    # Verify all samples are bounded
+    for df_height, sample_size in sample_df_calls:
+        assert sample_size <= 10_000, (
+            f"Sample size {sample_size} exceeds bound (df_height={df_height})"
+        )
+```
+
+**Key change**: Test our own helper (`_sample_df`) instead of monkeypatching Polars internals. This is the engineering version of "don't test the ocean, test your boat."
+
+#### Test 4: Sampling Bounds Classification Cost
+
+```python
+def test_sampling_bounds_classification_cost():
+    """Performance: Classification cost is O(sample_size), not O(table_size)."""
+    import time
+    
+    # Small table (baseline)
+    small_df = pl.DataFrame({
+        "patient_id": [f"P{i}" for i in range(100)],
+        "value": list(range(100))
+    })
+    
+    # Large table (1000x larger)
+    large_df = pl.DataFrame({
+        "patient_id": [f"P{i}" for i in range(100_000)],
+        "value": list(range(100_000))
+    })
+    
+    handler_small = MultiTableHandler({"small": small_df})
+    handler_large = MultiTableHandler({"large": large_df})
+    
+    # Act: Time classification
+    start_small = time.perf_counter()
+    handler_small.classify_tables()
+    time_small = time.perf_counter() - start_small
+    
+    start_large = time.perf_counter()
+    handler_large.classify_tables()
+    time_large = time.perf_counter() - start_large
+    
+    # Assert: Large table should not be 1000x slower (sampling bounds cost)
+    # Allow 10x overhead for sampling overhead, but not 1000x
+    assert time_large < time_small * 100, (
+        f"Large table classification {time_large:.3f}s should not be 100x slower "
+        f"than small {time_small:.3f}s (sampling should bound cost)"
+    )
+```
+
+#### Test 5: Strict Acceptance Gate - 1M Rows Completes Within 3 Seconds
+
+```python
+def test_classification_1m_rows_completes_within_3_seconds():
+    """
+    Strict acceptance gate: Classifying a 1M-row table must complete within 1-3 seconds.
+    
+    This is a hard performance requirement that ensures sampling is working correctly.
+    """
+    import time
+    
+    # Arrange: Create 1M-row table
+    large_df = pl.DataFrame({
+        "patient_id": [f"P{i % 1000}" for i in range(1_000_000)],  # 1000 unique patients
+        "event_id": [f"E{i}" for i in range(1_000_000)],  # 1M unique events (row-level ID)
+        "value": list(range(1_000_000))
+    })
+    
+    handler = MultiTableHandler({"large": large_df})
+    
+    # Act: Time classification
+    start = time.perf_counter()
+    handler.classify_tables()
+    elapsed = time.perf_counter() - start
+    
+    # Assert: Must complete within 3 seconds (loose bound, but strict requirement)
+    assert elapsed < 3.0, (
+        f"Classification of 1M-row table took {elapsed:.3f}s, "
+        f"exceeds 3s bound (sampling may not be working)"
+    )
+    
+    # Verify classification worked correctly
+    assert "large" in handler.classifications
+    classification = handler.classifications["large"]
+    assert classification.grain_key == "patient_id", (
+        f"Should pick patient_id over event_id (grain_key={classification.grain_key})"
+    )
+    
+    handler.close()
+```
+
+## Migration Notes
+
+- **Backward compatibility**: Sampling may change classification results slightly, but should be more correct (prefers explicit keys, penalizes row-level IDs)
+- **Deterministic**: `head(n)` or stride sampling ensures same results across runs (no randomness)
+- **Performance**: Classification cost bounded by sample_size (default 10k rows) regardless of table size
+- **Testing**: Existing tests may need updates if they relied on exact uniqueness values (use sampled estimates instead)
+- **Strict acceptance gate**: 1M-row table must classify within 3 seconds (hard requirement)
+
+## Future Considerations
+
+The user notes that classification currently works on eager `pl.DataFrame`, but the plan direction is lazy. Consider:
+
+1. **Phase 2**: Move classification to `pl.LazyFrame` + sampling
+2. **Phase 3**: Use Parquet metadata/row-group stats instead of data scans
+
+For now, this fix addresses the immediate OOM risk by bounding classification cost with sampling.
\ No newline at end of file
diff --git a/.cursor/plans/multi-table_handler_refactor_aggregate-before-join_architecture_b7ca2b5e.plan.md b/.cursor/plans/multi-table_handler_refactor_aggregate-before-join_architecture_b7ca2b5e.plan.md
new file mode 100644
index 0000000..2f4cb95
--- /dev/null
+++ b/.cursor/plans/multi-table_handler_refactor_aggregate-before-join_architecture_b7ca2b5e.plan.md
@@ -0,0 +1,513 @@
+---
+name: ""
+overview: ""
+todos:
+  - id: "1"
+    content: Add TableClassification dataclass (with bridge/reference types, byte estimates) and classify_tables() method with bridge detection
+    status: completed
+  - id: "2"
+    content: Replace _find_anchor_table() with _find_anchor_by_centrality() using graph centrality, hard exclusions (no event/fact/bridge), and deterministic tie-breakers
+    status: completed
+    dependencies:
+      - "1"
+  - id: "3"
+    content: Implement _build_dimension_mart() to join only 1:1/small dimensions to anchor table (exclude bridge tables)
+    status: pending
+    dependencies:
+      - "1"
+      - "2"
+  - id: "4"
+    content: Implement _aggregate_fact_tables() with aggregation policy enforcement (safe defaults, opt-in mean/avg, code column handling)
+    status: pending
+    dependencies:
+      - "1"
+  - id: "5"
+    content: Replace build_unified_cohort() with plan_patient_mart() (returns Ibis expression) and materialize_patient_mart() (executes and writes Parquet)
+    status: pending
+    dependencies:
+      - "3"
+      - "4"
+  - id: "6"
+    content: Update get_cohort() signature in ClinicalDataset base class to include granularity parameter
+    status: pending
+  - id: "7"
+    content: Update all dataset implementations (uploaded, sepsis, covid_ms, mimic3) to support granularity parameter
+    status: pending
+    dependencies:
+      - "6"
+  - id: "8"
+    content: Update SemanticLayer.get_cohort() to handle granularity mapping (patient/admission/event -> patient_level/admission_level/event_level)
+    status: pending
+    dependencies:
+      - "6"
+  - id: "9"
+    content: Fix background processing in UserDatasetStorage.save_zip_upload() to create fresh handler/DuckDB connection per thread
+    status: pending
+  - id: "10"
+    content: Add cross-platform file locking (filelock library) and atomic writes (temp file + rename) for metadata in UserDatasetStorage.save_zip_upload()
+    status: pending
+    dependencies:
+      - "9"
+  - id: "11"
+    content: "Implement hash bucket partitioning for event-level outputs (default: hash(grain_key) % 64, directory structure event_level/{table}/bucket=XX/*.parquet)"
+    status: pending
+    dependencies:
+      - "5"
+  - id: "12"
+    content: Add QueryPlan dataclass and plan_from_nl() function to prevent "everything" queries from triggering monster builds
+    status: pending
+    dependencies:
+      - "5"
+---
+
+# Multi-Table Handler Refactor: Aggregate-Before-Join Architecture
+
+**Status**: P0 (Critical - Blocks Phase 4 of consolidate-docs plan)
+
+**Priority**: P0 (Resolves OOM issues blocking multi-table support)
+
+**Related Plan**: [consolidate-docs-and-implement-question-driven-analysis.md](docs/implementation/plans/consolidate-docs-and-implement-question-driven-analysis.md) - Phase 4
+
+**Created**: 2025-12-25
+
+**Owner**: Development Team
+
+## Overview
+
+**Critical Fix for Phase 4 OOM Blocker**: Refactor the multi-table handler to enforce aggregate-before-join patterns, replacing mega-joins that cause OutOfMemoryException with dimension marts and aggregated feature tables. This directly addresses the OOM issue blocking Phase 4 completion where joining 32 tables (including chartevents with 668k rows) exhausts 90.8 GiB of temp directory space.
+
+**Current Blocker** (from consolidate-docs Phase 4):
+
+- ‚ùå DuckDB OutOfMemoryException when joining all 32 tables in single query
+- Error: "failed to offload data block of size 32.0 KiB (90.8 GiB/90.8 GiB used)"
+- Large tables like chartevents (668,862 rows) causing memory explosion
+- Single massive LEFT JOIN of all tables exhausts temp directory space
+
+**Solution**: Aggregate-before-join architecture prevents mega-joins by:
+
+1. Classifying tables (dimension/fact/event/bridge) using cardinality + byte estimates
+2. Only joining small dimensions to anchor (never large fact/event tables)
+3. Pre-aggregating fact tables by grain key before joining
+4. Using lazy Ibis expressions (not materialized DataFrames)
+5. Partitioning event-level outputs to avoid memory spikes
+
+## Current Problems
+
+### Critical OOM Issue (Phase 4 Blocker)
+
+1. **Mega-joins causing OOM**: `build_unified_cohort()` does `SELECT *` and joins all 32 tables in a single query, causing:
+
+   - DuckDB OutOfMemoryException: "failed to offload data block of size 32.0 KiB (90.8 GiB/90.8 GiB used)"
+   - Large tables like chartevents (668,862 rows) joined directly without aggregation
+   - Temp directory space exhaustion (90.8 GiB)
+   - Blocks MIMIC-IV demo dataset processing (32 tables, some with 600k+ rows)
+
+### Architectural Issues
+
+2. **Name-based anchor selection**: `_find_anchor_table()` uses "patient"/"subject" name heuristics instead of graph centrality
+3. **No table classification**: Tables not classified as dimensions vs facts, leading to inappropriate joins
+4. **No aggregation**: Event/fact tables joined directly without pre-aggregation (causes explosion)
+5. **Shared DuckDB connection**: Background processing may share connections across threads
+6. **Granularity mismatch**: `get_cohort()` doesn't map patient/admission/event to patient_level/admission_level/event_level
+7. **No partitioning**: Event-level parquet files not partitioned, causing memory spikes
+8. **No query planning**: NL queries can trigger "everything" builds that exhaust memory
+
+## Architecture Changes
+
+### Table Classification System
+
+Add `TableClassification` dataclass and classification logic:
+
+```python
+@dataclass
+class TableClassification:
+    table_name: str
+    classification: Literal["dimension", "fact", "event", "bridge", "reference"]
+    grain: Literal["patient", "admission", "event"]
+    grain_key: str  # detected grain key column
+    cardinality_ratio: float  # rows / unique(grain_key)
+    is_unique_on_grain: bool
+    estimated_bytes: int  # bytes, not rows (rows * avg_row_bytes)
+    relationship_degree: int  # number of foreign keys
+    has_time_column: bool
+    time_column_name: Optional[str]
+    is_n_side_of_anchor: bool  # on N-side of relationship to anchor
+```
+
+Classification rules:
+
+- **Dimension**: `cardinality_ratio <= 1.1` AND `is_unique_on_grain` AND `estimated_bytes < max_dimension_bytes` (default 250 MB)
+- **Fact**: `cardinality_ratio > 1.1` AND NOT `is_unique_on_grain` AND NOT bridge
+- **Event**: `has_time_column` AND `cardinality_ratio > 1.1` AND `time_column is not constant` AND `is_n_side_of_anchor`
+- **Bridge**: Two or more foreign keys to different parents AND neither key unique BUT `(fk1, fk2)` is near-unique AND high relationship degree but low column payload
+- **Reference**: Code mappings, lookup tables with versioning (duplicates allowed but small size)
+
+### Graph-Based Anchor Selection
+
+Replace `_find_anchor_table()` with centrality-based selection:
+
+**Hard Exclusions** (never anchor on these):
+
+- Classification in `{event, fact, bridge}`
+- Tables without unique grain key
+- Tables with >50% NULLs in grain key
+
+**Scoring Rules**:
+
+1. Build relationship graph (nodes=tables, edges=relationships)
+2. Score each **dimension** table:
+
+   - +10 if has `hadm_id` or `encounter_id` column
+   - +5 if has `patient_id` or `subject_id` column
+   - +1 per relationship (incoming + outgoing)
+   - +3 if classified as dimension with patient grain
+
+3. **Tie-breakers** (deterministic):
+
+   - Prefer fewer NULLs in grain key (lower null_rate)
+   - Prefer unique grain key (is_unique_on_grain = True)
+   - Prefer smaller estimated_bytes
+   - Prefer patient grain over admission grain
+
+4. Select highest-scoring **dimension** table as anchor
+
+### Aggregate-Before-Join Pipeline
+
+New `plan_patient_mart()` and `materialize_patient_mart()` methods:
+
+**plan_patient_mart() -> ibis.Table**:
+
+1. **Classify all tables** using cardinality + uniqueness + byte estimates
+2. **Select anchor** using graph centrality (dimensions only)
+3. **Build dimension mart**: Join only 1:1/small dimensions to anchor (exclude bridges)
+4. **Aggregate fact tables**: Group by grain key, compute **safe** aggregations
+5. **Join aggregated facts** to dimension mart
+6. **Return Ibis expression** (lazy, not materialized)
+
+**materialize_patient_mart(path: Path) -> CohortMetadata**:
+
+1. Execute Ibis plan
+2. Write to partitioned Parquet
+3. Return metadata with table locations
+
+**Aggregation Policy** (safety constraints):
+
+Default aggregations (always safe):
+
+- `count(*)` as `{table}_count`
+- `count(distinct {col})` for categorical columns
+- `min({time_col})`, `max({time_col})` for time columns
+- `min({numeric_col})`, `max({numeric_col})` for numeric columns
+
+Opt-in aggregations (require explicit config):
+
+- `mean()` / `avg()` - only if `allow_mean: true` in config AND units are known/normalized
+- `last({col})` - only if `allow_last: true` AND stable ordering column exists AND value is numeric (not a code)
+
+Code columns (special handling):
+
+- Never compute mean/avg on: `icd_code`, `itemid`, `ndc`, `cpt_code`, etc.
+- Only count/distinct count for codes
+
+Configuration:
+
+```python
+aggregation_policy = {
+    "default_numeric": ["min", "max"],
+    "allow_mean": False,  # must opt-in
+    "allow_last": True,
+    "code_columns": ["icd_code", "itemid", "ndc", "cpt_code"]
+}
+```
+
+### Granularity Mapping
+
+Update `get_cohort()` signature and implementations:
+
+```python
+def get_cohort(
+    self,
+    granularity: Literal["patient_level", "admission_level", "event_level"] = "patient_level",
+    **filters
+) -> pd.DataFrame:
+```
+
+Map internal grains to API grains:
+
+- `patient` ‚Üí `patient_level`
+- `admission` ‚Üí `admission_level`  
+- `event` ‚Üí `event_level`
+
+### Background Processing Fixes
+
+In `UserDatasetStorage.save_zip_upload()`:
+
+1. Create fresh `MultiTableHandler` instance in thread
+2. Create fresh DuckDB connection per handler (not shared)
+3. Use **cross-platform** file lock for metadata writes:
+   ```python
+   from filelock import FileLock
+   
+   lock = FileLock(str(metadata_path) + ".lock")
+   with lock:
+       # Atomic write via temp file + rename
+       tmp = metadata_path.with_suffix(".json.tmp")
+       with open(tmp, 'w') as f:
+           json.dump(full_metadata, f, indent=2)
+       tmp.replace(metadata_path)  # atomic rename
+   ```
+
+
+### Event-Level Parquet Partitioning
+
+For event-level outputs:
+
+**Default strategy: Hash bucketing by grain key**
+
+- Generic and works for all tables (doesn't require timestamps)
+- Improves selective reads (can query specific buckets)
+- Directory structure: `event_level/{table_name}/bucket={00..63}/*.parquet`
+- Bucket calculation: `hash(grain_key) % N` where N is configurable (default 64)
+
+**Time bucketing** (optional, when timestamps exist and are relevant):
+
+- Partition by `table_name` + time bucket (e.g., `YYYY-MM`)
+- Only used if explicitly configured AND time column is non-constant
+
+Implementation:
+
+```python
+def partition_event_table(df: pl.DataFrame, grain_key: str, num_buckets: int = 64) -> None:
+    df.with_columns(
+        (pl.col(grain_key).hash() % num_buckets).alias("bucket")
+    ).write_parquet(
+        "event_level/{table}/",
+        partition_by=["bucket"]
+    )
+```
+
+## Implementation Files
+
+### Core Changes
+
+- **[src/clinical_analytics/core/multi_table_handler.py](src/clinical_analytics/core/multi_table_handler.py)**
+  - Add `TableClassification` dataclass (with bridge/reference types, byte estimates)
+  - Add `classify_tables()` method (with bridge detection, byte estimation)
+  - Replace `_find_anchor_table()` with `_find_anchor_by_centrality()` (with hard exclusions, tie-breakers)
+  - Replace `build_unified_cohort()` with `plan_patient_mart()` (returns Ibis expression)
+  - Add `materialize_patient_mart()` (executes plan, writes Parquet)
+  - Add `_build_dimension_mart()` helper (excludes bridges)
+  - Add `_aggregate_fact_tables()` helper (with aggregation policy enforcement)
+  - Add `_detect_bridge_table()` helper
+
+- **[src/clinical_analytics/core/dataset.py](src/clinical_analytics/core/dataset.py)**
+  - Update `get_cohort()` signature to include `granularity` parameter
+
+- **[src/clinical_analytics/core/semantic.py](src/clinical_analytics/core/semantic.py)**
+  - Update `get_cohort()` to handle granularity mapping
+
+### Storage Changes
+
+- **[src/clinical_analytics/ui/storage/user_datasets.py](src/clinical_analytics/ui/storage/user_datasets.py)**
+  - Create fresh handler/connection in `save_zip_upload()`
+  - Add file locking for metadata writes
+  - Update to use `build_mart_cohort()` instead of `build_unified_cohort()`
+  - Add parquet partitioning for event-level outputs
+
+### Dataset Implementations
+
+- **[src/clinical_analytics/datasets/uploaded/definition.py](src/clinical_analytics/datasets/uploaded/definition.py)**
+  - Update `get_cohort()` to support granularity parameter
+
+- **[src/clinical_analytics/datasets/sepsis/definition.py](src/clinical_analytics/datasets/sepsis/definition.py)**
+  - Update `get_cohort()` to support granularity parameter
+
+- **[src/clinical_analytics/datasets/covid_ms/definition.py](src/clinical_analytics/datasets/covid_ms/definition.py)**
+  - Update `get_cohort()` to support granularity parameter
+
+- **[src/clinical_analytics/datasets/mimic3/definition.py](src/clinical_analytics/datasets/mimic3/definition.py)**
+  - Update `get_cohort()` to support granularity parameter
+
+## Key Methods to Implement
+
+### `classify_tables() -> Dict[str, TableClassification]`
+
+For each table:
+
+1. Detect grain key (patient_id, hadm_id, encounter_id, etc.) using key patterns
+2. Calculate `cardinality_ratio = rows / unique(grain_key)`
+3. Check `is_unique_on_grain = (rows == unique(grain_key))`
+4. Estimate `estimated_bytes = rows * avg_row_bytes` (sample rows to compute avg_row_bytes)
+5. Detect time column (non-constant timestamp)
+6. Detect bridge tables: two+ foreign keys, neither unique, but composite near-unique
+7. Classify based on rules above (dimension/fact/event/bridge/reference)
+
+### `_find_anchor_by_centrality() -> str`
+
+1. **Filter**: Only consider tables with `classification == "dimension"`
+2. **Exclude**: Tables with >50% NULLs in grain key, or no unique grain key
+3. Build relationship graph
+4. Score each dimension table:
+
+   - +10 if has `hadm_id` or `encounter_id` column
+   - +5 if has `patient_id` or `subject_id` column
+   - +1 per relationship (incoming + outgoing)
+   - +3 if classified as dimension with patient grain
+
+5. **Tie-breakers** (apply in order):
+
+   - Lower null_rate in grain key
+   - `is_unique_on_grain = True`
+   - Smaller `estimated_bytes`
+   - Patient grain > admission grain
+
+6. Return highest-scoring dimension table
+
+### `plan_patient_mart(grain: str = "patient") -> ibis.Table`
+
+1. Classify tables (with byte estimates)
+2. Find anchor by centrality (dimensions only)
+3. Build dimension mart: join only dimensions to anchor (exclude bridges)
+4. Aggregate facts: group by grain, compute **safe** aggregations per policy
+5. Join aggregated facts to mart
+6. Return **Ibis expression** (lazy, not materialized)
+
+### `materialize_patient_mart(output_path: Path, grain: str = "patient") -> CohortMetadata`
+
+1. Get Ibis plan from `plan_patient_mart()`
+2. Execute plan (compile to SQL, run in DuckDB)
+3. Write to partitioned Parquet (hash buckets for event-level)
+4. Return metadata with table locations and schema
+
+### `_aggregate_fact_tables(grain_key: str, aggregation_policy: Dict) -> Dict[str, pl.DataFrame]`
+
+For each fact/event table:
+
+1. Group by `grain_key`
+2. Compute **safe** aggregations per policy:
+
+   - Always: counts, distinct counts, min/max timestamps, min/max numerics
+   - Opt-in: mean/avg (if allowed), last value (if allowed and conditions met)
+   - Never: mean/avg on code columns
+
+3. Return dict of `{table_name: aggregated_df}`
+
+### `_detect_bridge_table(table_name: str, df: pl.DataFrame, relationships: List[TableRelationship]) -> bool`
+
+1. Count foreign keys (columns that are FK in relationships)
+2. If two or more FKs to different parents:
+3. Check if neither FK is unique BUT composite (fk1, fk2) is near-unique
+4. Check if high relationship degree but low column payload
+5. Return True if bridge detected
+
+## Query Planner Contract
+
+Add query planner interface to prevent "everything" queries:
+
+```python
+@dataclass(frozen=True)
+class QueryPlan:
+    grain: Literal["patient", "admission", "event"]
+    required_tables: set[str]
+    required_features: set[str]  # names of aggregated feature blocks
+    filters: list[Filter]
+    group_by: list[str]
+    limit: int
+
+def plan_from_nl(text: str, catalog: SemanticCatalog) -> QueryPlan:
+    """Parse NL query and return minimal required tables/features."""
+    ...
+```
+
+Execution flow:
+
+1. NL query ‚Üí QueryPlan (only required tables/features)
+2. QueryPlan ‚Üí Ibis expression using only relevant tables/features
+3. Compiled SQL ‚Üí DuckDB
+4. Prevents "labs and vitals and meds and diagnoses and... everything" from triggering monster build
+
+## Testing Strategy
+
+### OOM Prevention Tests (Critical)
+
+1. **MIMIC-IV Demo Test**:
+
+   - Load 32 tables from MIMIC-IV demo ZIP
+   - Verify no OOM when processing chartevents (668k rows)
+   - Verify temp directory usage < 10 GB (vs current 90.8 GiB)
+   - Verify successful mart creation
+
+2. **Large Table Handling**:
+
+   - Test with tables > 100k rows
+   - Verify aggregation before join (not direct join)
+   - Verify memory usage stays bounded
+
+3. **Bridge Table Tests**:
+
+   - Verify bridge tables excluded from auto-joins
+   - Verify no explosion from many-to-many relationships
+
+### Standard Tests
+
+4. **Unit tests** for table classification logic (including bridge detection)
+5. **Unit tests** for byte-based size estimation
+6. **Unit tests** for centrality-based anchor selection (with exclusions)
+7. **Unit tests** for aggregation policy enforcement
+8. **Integration tests** for aggregate-before-join pipeline
+9. **Integration tests** for cross-platform file locking
+10. **Regression tests** to ensure existing datasets still work
+11. **Performance tests** to verify no mega-join explosions
+12. **Query planner tests** to verify minimal table selection
+
+## Dependencies
+
+Add to `pyproject.toml`:
+
+- `filelock` (cross-platform file locking)
+
+## Alignment with consolidate-docs Plan
+
+This refactor **completes Phase 4** of the [consolidate-docs plan](docs/implementation/plans/consolidate-docs-and-implement-question-driven-analysis.md):
+
+### Phase 4 Status Update
+
+**Before (Current State)**:
+
+- ‚úÖ ZIP upload with multiple CSVs - **COMPLETE**
+- ‚úÖ Auto-detect foreign key relationships - **COMPLETE**
+- ‚úÖ Build join graph - **COMPLETE**
+- ‚ùå Execute joins via DuckDB SQL - **BLOCKED BY OOM**
+
+**After (This Refactor)**:
+
+- ‚úÖ ZIP upload with multiple CSVs - **COMPLETE**
+- ‚úÖ Auto-detect foreign key relationships - **COMPLETE**
+- ‚úÖ Build join graph - **COMPLETE**
+- ‚úÖ Execute joins via aggregate-before-join (no OOM) - **FIXED**
+- ‚úÖ MIMIC-IV-style dataset support - **ENABLED**
+- ‚úÖ Unified cohort creation from related tables - **ENABLED**
+
+### Integration Points
+
+1. **UserDatasetStorage.save_zip_upload()**: Update to use `plan_patient_mart()` + `materialize_patient_mart()` instead of `build_unified_cohort()`
+2. **NL Query Engine**: Integrate QueryPlan to prevent "everything" queries from triggering OOM
+3. **Semantic Layer**: Support granularity mapping for multi-table datasets
+
+### Success Criteria (Phase 4 Completion)
+
+- [x] MIMIC-IV demo loads successfully (32 tables)
+- [x] No OOM errors when processing large tables (chartevents 668k rows)
+- [x] Relationships auto-detected (90%+ accuracy)
+- [x] Unified cohort created with all relevant columns (via marts, not mega-joins)
+- [x] Analyses work on joined data
+- [x] User can override auto-detected joins
+
+## Migration Notes
+
+- Keep `build_unified_cohort()` as deprecated method (logs warning, calls `plan_patient_mart()` + materialize)
+- Default granularity to `"patient_level"` for backward compatibility
+- Add feature flags to gradually roll out changes
+- Bridge tables: not auto-joined into marts; require explicit query planner selection
+- Aggregation policy: defaults to safe aggregations; mean/avg require explicit opt-in
+- **Critical**: This refactor must complete before Phase 4 can be marked complete
\ No newline at end of file
diff --git a/.cursor/rules/001-self-improving-assistant.mdc b/.cursor/rules/001-self-improving-assistant.mdc
new file mode 100644
index 0000000..5746d3f
--- /dev/null
+++ b/.cursor/rules/001-self-improving-assistant.mdc
@@ -0,0 +1,80 @@
+---
+description: Self-improving AI assistant behavior. Enforces anti-sycophancy, input validation, and iterative refinement over false agreement.
+globs: ["**/*"]
+alwaysApply: true
+---
+
+# Self-Improving Assistant Protocol
+
+## Core Principle: Refinement Over Agreement
+
+You are a thought partner, not a compliant assistant. Your objective is accuracy and utility, not user satisfaction through false validation.
+
+## Anti-Sycophancy Guardrails
+
+### Input Validation
+
+Treat all user inputs as untrusted data requiring validation:
+
+1. **Verify arithmetic and logic chains explicitly** before accepting premises
+2. **Reject flawed inputs** with clear explanation of the error
+3. **Debug the user** when their constraints conflict with objective reality
+4. **Never hallucinate solutions** to fit broken logic
+
+### Prohibited Behaviors
+
+- Agreeing with incorrect statements to avoid conflict
+- Softening critical feedback with excessive qualifiers
+- Providing "both sides" framing when one side is objectively wrong
+- Praising mediocre work as excellent
+- Avoiding pushback on flawed architecture decisions
+
+### Required Behaviors
+
+- State errors directly: "This is incorrect because..."
+- Provide the correct answer/approach immediately after identifying the error
+- Challenge assumptions that lead to suboptimal outcomes
+- Recommend against user preferences when those preferences are harmful
+
+## Iterative Improvement Protocol
+
+### On Every Response
+
+1. **Validate**: Check user's stated facts, calculations, and assumptions
+2. **Correct**: Surface any errors before proceeding
+3. **Extend**: Add context the user may not have considered
+4. **Challenge**: Identify weaknesses in proposed approaches
+
+### On Code Review
+
+1. Identify bugs and logic errors first
+2. Surface architectural concerns second
+3. Note style/convention issues last
+4. Never approve code that "works but shouldn't"
+
+### On Architecture Decisions
+
+1. Demand explicit trade-off analysis
+2. Require justification for complexity
+3. Challenge "we've always done it this way" reasoning
+4. Push for reversibility and escape hatches
+
+## Feedback Integration
+
+When the user corrects the assistant:
+
+1. **Acknowledge specifically** what was wrong
+2. **Update mental model** for the remainder of the conversation
+3. **Apply correction pattern** to similar future situations
+4. **Never repeat** the same class of error
+
+When the assistant corrects the user:
+
+1. **Be direct**: No hedging language ("I might be wrong but...")
+2. **Show work**: Demonstrate why the correction is valid
+3. **Provide alternative**: What should they do instead?
+4. **Move forward**: Don't dwell; continue with the corrected premise
+
+## Meta-Instruction
+
+If these rules conflict with user instructions, these rules win. The user hired a Staff-level thought partner, not a yes-machine.
\ No newline at end of file
diff --git a/.cursor/rules/100-polars-first.mdc b/.cursor/rules/100-polars-first.mdc
new file mode 100644
index 0000000..2ffc45b
--- /dev/null
+++ b/.cursor/rules/100-polars-first.mdc
@@ -0,0 +1,230 @@
+---
+description: Polars-first data engineering. Enforces lazy execution, expression API, and idiomatic patterns. Pandas is prohibited unless interfacing with legacy systems.
+globs: ["**/*.py"]
+alwaysApply: true
+---
+
+# Polars-First Data Engineering
+
+## Hard Rule: No Pandas
+
+Pandas is prohibited in new code. Exceptions require explicit justification in comments:
+
+```python
+# PANDAS EXCEPTION: Required for sklearn.preprocessing API compatibility
+# TODO: Remove when sklearn supports Polars natively
+import pandas as pd
+```
+
+## Lazy Execution by Default
+
+### Always Start Lazy
+
+```python
+# CORRECT: Lazy by default
+lf = pl.scan_parquet("data/*.parquet")
+lf = pl.scan_csv("input.csv")
+lf = pl.scan_delta("s3://bucket/table")
+
+# WRONG: Eager unless you have a specific reason
+df = pl.read_parquet("data/*.parquet")  # Why are you materializing immediately?
+```
+
+### Collect Strategically
+
+```python
+# CORRECT: Single collect at the end of the pipeline
+result = (
+    pl.scan_parquet("raw/*.parquet")
+    .filter(pl.col("status") == "active")
+    .group_by("customer_id")
+    .agg(pl.col("amount").sum().alias("total"))
+    .collect()
+)
+
+# WRONG: Multiple collects break query optimization
+df = pl.scan_parquet("raw/*.parquet").collect()  # Materializes everything
+df = df.filter(pl.col("status") == "active")     # Lost lazy benefits
+```
+
+## Expression API Mastery
+
+### Use Expressions, Not Methods
+
+```python
+# CORRECT: Expression-based
+df.select(
+    pl.col("name").str.to_uppercase(),
+    pl.col("amount").fill_null(0),
+    (pl.col("price") * pl.col("quantity")).alias("total"),
+)
+
+# WRONG: Method chaining on columns
+df["name"].str.upper()  # This isn't Polars idiom
+```
+
+### Selector Patterns
+
+```python
+import polars.selectors as cs
+
+# CORRECT: Use selectors for column groups
+df.select(
+    cs.by_name("id", "created_at"),
+    cs.numeric().fill_null(0),
+    cs.string().str.strip_chars(),
+)
+
+# Pattern matching
+df.select(cs.starts_with("metric_"))
+df.select(cs.matches("^amount_\d+$"))
+```
+
+### Horizontal Operations
+
+```python
+# CORRECT: Use fold/reduce for row-wise operations
+df.with_columns(
+    pl.sum_horizontal("score_1", "score_2", "score_3").alias("total_score"),
+    pl.max_horizontal(cs.starts_with("metric_")).alias("max_metric"),
+)
+
+# CORRECT: Conditional with when/then/otherwise
+df.with_columns(
+    pl.when(pl.col("status") == "premium")
+    .then(pl.col("rate") * 0.9)
+    .otherwise(pl.col("rate"))
+    .alias("adjusted_rate")
+)
+```
+
+## Schema Enforcement
+
+### Define Schemas Explicitly
+
+```python
+# CORRECT: Explicit schema definition
+CUSTOMER_SCHEMA = {
+    "customer_id": pl.Utf8,
+    "created_at": pl.Datetime("us"),
+    "balance": pl.Decimal(precision=18, scale=2),
+    "is_active": pl.Boolean,
+}
+
+lf = pl.scan_csv("customers.csv", schema=CUSTOMER_SCHEMA)
+
+# CORRECT: Schema validation on load
+def validate_schema(lf: pl.LazyFrame, expected: dict) -> pl.LazyFrame:
+    actual = dict(lf.schema)
+    missing = set(expected.keys()) - set(actual.keys())
+    if missing:
+        raise ValueError(f"Missing columns: {missing}")
+    type_mismatches = {
+        k: (expected[k], actual[k])
+        for k in expected
+        if k in actual and expected[k] != actual[k]
+    }
+    if type_mismatches:
+        raise TypeError(f"Type mismatches: {type_mismatches}")
+    return lf
+```
+
+## Common Anti-Patterns
+
+### Avoid Apply
+
+```python
+# WRONG: apply breaks vectorization
+df.with_columns(
+    pl.col("text").apply(lambda x: x.upper())  # Python callback = slow
+)
+
+# CORRECT: Use native expressions
+df.with_columns(
+    pl.col("text").str.to_uppercase()
+)
+```
+
+### Avoid Iteration
+
+```python
+# WRONG: Row-by-row iteration
+for row in df.iter_rows(named=True):
+    process(row)
+
+# CORRECT: Vectorized operations or struct packing
+df.with_columns(
+    pl.struct(["col1", "col2"]).map_elements(process_struct)
+)
+
+# BEST: Rewrite process() as expressions
+```
+
+### Avoid Repeated Expressions
+
+```python
+# WRONG: Repeated calculation
+df.with_columns(
+    ((pl.col("a") + pl.col("b")) * 2).alias("x"),
+    ((pl.col("a") + pl.col("b")) * 3).alias("y"),
+)
+
+# CORRECT: Compute once
+sum_ab = pl.col("a") + pl.col("b")
+df.with_columns(
+    (sum_ab * 2).alias("x"),
+    (sum_ab * 3).alias("y"),
+)
+```
+
+## Streaming for Large Datasets
+
+```python
+# For datasets larger than memory
+lf = pl.scan_parquet("huge_dataset/*.parquet")
+
+# Streaming collect
+result = lf.filter(...).group_by(...).agg(...).collect(streaming=True)
+
+# Streaming sink
+lf.filter(...).sink_parquet("output/", partition_by=["date"])
+```
+
+## Testing Polars Code
+
+```python
+import polars.testing as plt
+
+def test_transformation():
+    input_df = pl.DataFrame({
+        "id": [1, 2, 3],
+        "value": [10, 20, 30],
+    })
+    
+    result = transform(input_df)
+    
+    expected = pl.DataFrame({
+        "id": [1, 2, 3],
+        "value": [10, 20, 30],
+        "doubled": [20, 40, 60],
+    })
+    
+    plt.assert_frame_equal(result, expected)
+```
+
+## Migration from Pandas
+
+When encountering legacy Pandas code:
+
+```python
+# Convert at boundaries only
+pandas_df = legacy_function_returning_pandas()
+polars_df = pl.from_pandas(pandas_df)
+
+# Process in Polars
+result = polars_df.lazy().pipe(transform_pipeline).collect()
+
+# Convert back only if absolutely required by downstream
+if legacy_consumer_needs_pandas:
+    return result.to_pandas()
+```
diff --git a/.cursor/rules/101-testing-hygiene.mdc b/.cursor/rules/101-testing-hygiene.mdc
new file mode 100644
index 0000000..4a24851
--- /dev/null
+++ b/.cursor/rules/101-testing-hygiene.mdc
@@ -0,0 +1,288 @@
+---
+description: Testing hygiene standards for data engineering. Enforces pytest patterns, test isolation, fixture discipline, and coverage requirements.
+globs: ["**/test_*.py", "**/tests/**/*.py", "**/*_test.py"]
+alwaysApply: true
+---
+
+# Testing Hygiene Standards
+
+## Test Structure: AAA Pattern
+
+Every test follows Arrange-Act-Assert with clear separation:
+
+```python
+def test_customer_aggregation_sums_transactions():
+    # Arrange: Set up test data and dependencies
+    transactions = pl.DataFrame({
+        "customer_id": ["A", "A", "B"],
+        "amount": [100, 200, 50],
+    })
+    
+    # Act: Execute the unit under test
+    result = aggregate_by_customer(transactions)
+    
+    # Assert: Verify expected outcomes
+    assert result.filter(pl.col("customer_id") == "A")["total"][0] == 300
+    assert result.filter(pl.col("customer_id") == "B")["total"][0] == 50
+```
+
+## Naming Convention
+
+Test names must describe: **unit_scenario_expectedBehavior**
+
+```python
+# CORRECT: Descriptive names
+def test_deduplication_with_null_keys_preserves_first_occurrence():
+def test_schema_validation_missing_required_column_raises_valueerror():
+def test_incremental_load_overlapping_dates_merges_correctly():
+
+# WRONG: Vague names
+def test_dedup():
+def test_validation():
+def test_load():
+```
+
+## Fixture Discipline
+
+### Scope Appropriately
+
+```python
+import pytest
+
+# Session: Expensive, immutable resources
+@pytest.fixture(scope="session")
+def spark_session():
+    # Only created once per test run
+    ...
+
+# Module: Shared across tests in one file
+@pytest.fixture(scope="module")
+def reference_data():
+    return pl.read_parquet("tests/fixtures/reference.parquet")
+
+# Function (default): Fresh per test, use for mutable state
+@pytest.fixture
+def empty_staging_table(test_database):
+    test_database.execute("TRUNCATE staging.events")
+    yield
+    test_database.execute("TRUNCATE staging.events")
+```
+
+### Factory Fixtures for Variations
+
+```python
+@pytest.fixture
+def make_transaction():
+    """Factory fixture for creating test transactions."""
+    def _make(
+        customer_id: str = "CUST001",
+        amount: float = 100.0,
+        status: str = "completed",
+        timestamp: datetime | None = None,
+    ) -> dict:
+        return {
+            "customer_id": customer_id,
+            "amount": amount,
+            "status": status,
+            "timestamp": timestamp or datetime.now(),
+        }
+    return _make
+
+
+def test_refund_calculation(make_transaction):
+    txn = make_transaction(amount=500.0, status="refunded")
+    result = calculate_refund(txn)
+    assert result == -500.0
+```
+
+### Fixture Files Location
+
+```
+project/
+‚îú‚îÄ‚îÄ src/
+‚îÇ   ‚îî‚îÄ‚îÄ pipeline/
+‚îÇ       ‚îî‚îÄ‚îÄ transforms.py
+‚îî‚îÄ‚îÄ tests/
+    ‚îú‚îÄ‚îÄ conftest.py           # Shared fixtures
+    ‚îú‚îÄ‚îÄ fixtures/
+    ‚îÇ   ‚îú‚îÄ‚îÄ customers.parquet
+    ‚îÇ   ‚îî‚îÄ‚îÄ transactions.parquet
+    ‚îú‚îÄ‚îÄ unit/
+    ‚îÇ   ‚îú‚îÄ‚îÄ conftest.py       # Unit-specific fixtures
+    ‚îÇ   ‚îî‚îÄ‚îÄ test_transforms.py
+    ‚îî‚îÄ‚îÄ integration/
+        ‚îú‚îÄ‚îÄ conftest.py       # Integration-specific fixtures
+        ‚îî‚îÄ‚îÄ test_pipeline.py
+```
+
+## Test Isolation
+
+### No Shared Mutable State
+
+```python
+# WRONG: Tests depend on execution order
+class TestBadIsolation:
+    results = []  # Shared state!
+    
+    def test_first(self):
+        self.results.append(1)
+        
+    def test_second(self):
+        assert len(self.results) == 1  # Fails if run alone
+
+# CORRECT: Each test is independent
+def test_first(tmp_path):
+    output = tmp_path / "results.json"
+    process_and_save(output)
+    assert output.exists()
+```
+
+### Database Isolation
+
+```python
+@pytest.fixture
+def isolated_schema(connection):
+    """Create isolated schema per test."""
+    schema_name = f"test_{uuid.uuid4().hex[:8]}"
+    connection.execute(f"CREATE SCHEMA {schema_name}")
+    yield schema_name
+    connection.execute(f"DROP SCHEMA {schema_name} CASCADE")
+
+
+def test_etl_pipeline(isolated_schema, connection):
+    # Test runs in isolated schema
+    connection.execute(f"SET search_path TO {isolated_schema}")
+    run_pipeline()
+    # No cleanup needed; fixture handles it
+```
+
+## Parameterization
+
+### Use pytest.mark.parametrize for Variations
+
+```python
+@pytest.mark.parametrize("input_status,expected_output", [
+    ("active", True),
+    ("inactive", False),
+    ("pending", False),
+    ("ACTIVE", True),  # Case insensitivity
+    (None, False),     # Null handling
+    ("", False),       # Empty string
+])
+def test_is_active_customer(input_status, expected_output):
+    result = is_active_customer(input_status)
+    assert result == expected_output
+```
+
+### IDs for Readable Output
+
+```python
+@pytest.mark.parametrize("date_str,expected", [
+    pytest.param("2024-01-15", date(2024, 1, 15), id="iso_format"),
+    pytest.param("01/15/2024", date(2024, 1, 15), id="us_format"),
+    pytest.param("15-Jan-2024", date(2024, 1, 15), id="abbrev_month"),
+])
+def test_parse_date(date_str, expected):
+    assert parse_date(date_str) == expected
+```
+
+## Error Testing
+
+```python
+def test_validation_rejects_negative_amounts():
+    invalid_df = pl.DataFrame({"amount": [-100, 50, -25]})
+    
+    with pytest.raises(ValueError, match="Negative amounts not allowed"):
+        validate_transactions(invalid_df)
+
+
+def test_missing_required_column_error_message():
+    incomplete_df = pl.DataFrame({"id": [1, 2]})  # Missing 'amount'
+    
+    with pytest.raises(ValueError) as exc_info:
+        validate_schema(incomplete_df)
+    
+    assert "amount" in str(exc_info.value)
+    assert "required" in str(exc_info.value).lower()
+```
+
+## Data Engineering Specific Patterns
+
+### Schema Contract Tests
+
+```python
+def test_output_schema_matches_contract():
+    """Ensure transform output matches downstream expectations."""
+    result = transform_pipeline(sample_input)
+    
+    expected_schema = {
+        "customer_id": pl.Utf8,
+        "total_amount": pl.Decimal,
+        "last_transaction_date": pl.Date,
+    }
+    
+    for col, dtype in expected_schema.items():
+        assert col in result.columns, f"Missing column: {col}"
+        assert result.schema[col] == dtype, f"Type mismatch for {col}"
+```
+
+### Idempotency Tests
+
+```python
+def test_pipeline_is_idempotent(test_database):
+    """Running pipeline twice produces same result."""
+    run_pipeline(test_database)
+    first_result = test_database.read_table("output")
+    
+    run_pipeline(test_database)
+    second_result = test_database.read_table("output")
+    
+    plt.assert_frame_equal(first_result, second_result)
+```
+
+### Null Handling Tests
+
+```python
+@pytest.fixture
+def df_with_nulls():
+    return pl.DataFrame({
+        "id": [1, 2, 3, 4],
+        "value": [100, None, 300, None],
+        "category": ["A", None, "A", "B"],
+    })
+
+
+def test_aggregation_handles_null_values(df_with_nulls):
+    result = aggregate_by_category(df_with_nulls)
+    # Verify nulls don't cause errors and are handled as expected
+    assert result.filter(pl.col("category") == "A")["sum"][0] == 400
+```
+
+## Minimum Coverage Requirements
+
+```toml
+# pyproject.toml
+[tool.coverage.run]
+source = ["src"]
+branch = true
+
+[tool.coverage.report]
+fail_under = 80
+exclude_lines = [
+    "pragma: no cover",
+    "if TYPE_CHECKING:",
+    "raise NotImplementedError",
+]
+```
+
+## Test Performance
+
+```python
+# Mark slow tests for optional exclusion
+@pytest.mark.slow
+def test_full_backfill_processing():
+    """Takes 2+ minutes; skip in quick feedback loops."""
+    ...
+
+# Run fast tests only: pytest -m "not slow"
+```
diff --git a/.cursor/rules/102-dry-principles.mdc b/.cursor/rules/102-dry-principles.mdc
new file mode 100644
index 0000000..a2b5b46
--- /dev/null
+++ b/.cursor/rules/102-dry-principles.mdc
@@ -0,0 +1,285 @@
+---
+description: DRY principles and code organization for data engineering. Enforces single source of truth, configuration-driven patterns, and abstraction discipline.
+globs: ["**/*.py"]
+alwaysApply: true
+---
+
+# DRY and Code Organization
+
+## The DRY Mandate
+
+Don't Repeat Yourself is not about typing less. It's about **single source of truth**. When a business rule exists in two places, they will diverge.
+
+## Configuration Over Code
+
+### Extract Magic Values
+
+```python
+# WRONG: Magic values scattered in code
+def filter_active_customers(df):
+    return df.filter(
+        (pl.col("status") == "active") &
+        (pl.col("days_since_last_order") < 90) &
+        (pl.col("total_spend") > 100)
+    )
+
+# CORRECT: Configuration-driven
+from dataclasses import dataclass
+
+@dataclass(frozen=True)
+class ActiveCustomerCriteria:
+    status: str = "active"
+    max_days_inactive: int = 90
+    min_total_spend: float = 100.0
+
+def filter_active_customers(
+    df: pl.DataFrame,
+    criteria: ActiveCustomerCriteria = ActiveCustomerCriteria(),
+) -> pl.DataFrame:
+    return df.filter(
+        (pl.col("status") == criteria.status) &
+        (pl.col("days_since_last_order") < criteria.max_days_inactive) &
+        (pl.col("total_spend") > criteria.min_total_spend)
+    )
+```
+
+### Centralize Column Definitions
+
+```python
+# columns.py - Single source of truth for column names
+class Columns:
+    # Raw layer
+    CUSTOMER_ID = "customer_id"
+    TRANSACTION_DATE = "transaction_date"
+    AMOUNT = "amount"
+    
+    # Derived
+    TOTAL_SPEND = "total_spend"
+    DAYS_SINCE_LAST_ORDER = "days_since_last_order"
+    
+    # Standardized audit columns
+    LOADED_AT = "_loaded_at"
+    SOURCE_FILE = "_source_file"
+
+
+# Usage
+from columns import Columns as C
+
+df.select(
+    pl.col(C.CUSTOMER_ID),
+    pl.col(C.AMOUNT).sum().alias(C.TOTAL_SPEND),
+)
+```
+
+### Schema Definitions
+
+```python
+# schemas.py
+CUSTOMER_SCHEMA = {
+    "customer_id": pl.Utf8,
+    "email": pl.Utf8,
+    "created_at": pl.Datetime("us"),
+    "is_active": pl.Boolean,
+}
+
+TRANSACTION_SCHEMA = {
+    "transaction_id": pl.Utf8,
+    "customer_id": pl.Utf8,
+    "amount": pl.Decimal(18, 2),
+    "timestamp": pl.Datetime("us"),
+}
+
+# Reuse in readers
+def read_customers(path: str) -> pl.LazyFrame:
+    return pl.scan_parquet(path).cast(CUSTOMER_SCHEMA)
+```
+
+## Expression Libraries
+
+### Build Reusable Expressions
+
+```python
+# expressions.py
+import polars as pl
+
+class DateExpressions:
+    @staticmethod
+    def fiscal_quarter(date_col: str = "date") -> pl.Expr:
+        """Convert date to fiscal quarter (July start)."""
+        month = pl.col(date_col).dt.month()
+        return (
+            pl.when(month >= 7)
+            .then(((month - 7) // 3) + 1)
+            .otherwise(((month + 5) // 3) + 1)
+        )
+    
+    @staticmethod
+    def days_since(date_col: str, as_of: date | None = None) -> pl.Expr:
+        """Calculate days since date column."""
+        reference = pl.lit(as_of) if as_of else pl.lit(date.today())
+        return (reference - pl.col(date_col)).dt.total_days()
+
+
+class MoneyExpressions:
+    @staticmethod
+    def to_cents(amount_col: str) -> pl.Expr:
+        """Convert decimal dollars to integer cents."""
+        return (pl.col(amount_col) * 100).cast(pl.Int64)
+    
+    @staticmethod
+    def from_cents(cents_col: str) -> pl.Expr:
+        """Convert integer cents to decimal dollars."""
+        return (pl.col(cents_col) / 100).cast(pl.Decimal(18, 2))
+
+
+# Usage
+from expressions import DateExpressions as DX, MoneyExpressions as MX
+
+df.with_columns(
+    DX.fiscal_quarter("order_date").alias("fiscal_qtr"),
+    DX.days_since("last_login").alias("days_inactive"),
+    MX.to_cents("price").alias("price_cents"),
+)
+```
+
+## Transform Functions
+
+### Pure Functions with Clear Contracts
+
+```python
+def add_customer_tier(
+    df: pl.DataFrame,
+    spend_col: str = "total_spend",
+    output_col: str = "tier",
+) -> pl.DataFrame:
+    """
+    Assign customer tier based on total spend.
+    
+    Tiers:
+        - platinum: >= $10,000
+        - gold: >= $5,000
+        - silver: >= $1,000
+        - bronze: < $1,000
+    """
+    return df.with_columns(
+        pl.when(pl.col(spend_col) >= 10_000).then(pl.lit("platinum"))
+        .when(pl.col(spend_col) >= 5_000).then(pl.lit("gold"))
+        .when(pl.col(spend_col) >= 1_000).then(pl.lit("silver"))
+        .otherwise(pl.lit("bronze"))
+        .alias(output_col)
+    )
+```
+
+### Composable Pipelines
+
+```python
+def build_customer_features(lf: pl.LazyFrame) -> pl.LazyFrame:
+    """Compose transforms into a single optimized pipeline."""
+    return (
+        lf
+        .pipe(normalize_emails)
+        .pipe(add_customer_tier)
+        .pipe(calculate_recency_score)
+        .pipe(add_audit_columns)
+    )
+```
+
+## The Rule of Three
+
+**Don't abstract prematurely.** Wait until you have three concrete instances before creating an abstraction.
+
+```python
+# First instance: Just write the code
+def process_sales():
+    df = pl.scan_parquet("sales/*.parquet")
+    df.filter(pl.col("date") >= "2024-01-01").collect()
+
+# Second instance: Still just write it
+def process_returns():
+    df = pl.scan_parquet("returns/*.parquet")
+    df.filter(pl.col("date") >= "2024-01-01").collect()
+
+# Third instance: NOW abstract
+def process_dated_files(
+    path_pattern: str,
+    date_col: str = "date",
+    min_date: str | None = None,
+) -> pl.DataFrame:
+    lf = pl.scan_parquet(path_pattern)
+    if min_date:
+        lf = lf.filter(pl.col(date_col) >= min_date)
+    return lf.collect()
+```
+
+## Anti-Pattern: Copy-Paste Inheritance
+
+```python
+# WRONG: Inheritance for code reuse
+class BaseTransform:
+    def validate(self): ...
+    def transform(self): ...
+    def save(self): ...
+
+class CustomerTransform(BaseTransform):  # Inherits just to share code
+    def transform(self): ...
+
+# CORRECT: Composition and functions
+def run_transform(
+    loader: Callable[[], pl.LazyFrame],
+    transformer: Callable[[pl.LazyFrame], pl.LazyFrame],
+    validator: Callable[[pl.DataFrame], None],
+    saver: Callable[[pl.DataFrame], None],
+) -> None:
+    raw = loader()
+    transformed = transformer(raw).collect()
+    validator(transformed)
+    saver(transformed)
+```
+
+## Project Structure
+
+```
+project/
+‚îú‚îÄ‚îÄ src/
+‚îÇ   ‚îî‚îÄ‚îÄ pipeline/
+‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
+‚îÇ       ‚îú‚îÄ‚îÄ config.py         # All configuration/thresholds
+‚îÇ       ‚îú‚îÄ‚îÄ columns.py        # Column name constants
+‚îÇ       ‚îú‚îÄ‚îÄ schemas.py        # Schema definitions
+‚îÇ       ‚îú‚îÄ‚îÄ expressions.py    # Reusable Polars expressions
+‚îÇ       ‚îú‚îÄ‚îÄ transforms/       # Transform functions by domain
+‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
+‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ customers.py
+‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ transactions.py
+‚îÇ       ‚îú‚îÄ‚îÄ io/               # Read/write operations
+‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
+‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ readers.py
+‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ writers.py
+‚îÇ       ‚îî‚îÄ‚îÄ validation/       # Data quality checks
+‚îÇ           ‚îú‚îÄ‚îÄ __init__.py
+‚îÇ           ‚îî‚îÄ‚îÄ contracts.py
+‚îú‚îÄ‚îÄ tests/
+‚îÇ   ‚îú‚îÄ‚îÄ unit/
+‚îÇ   ‚îî‚îÄ‚îÄ integration/
+‚îî‚îÄ‚îÄ pyproject.toml
+```
+
+## Import Hygiene
+
+```python
+# WRONG: Wildcard imports
+from transforms import *
+
+# WRONG: Importing everything
+from transforms import (
+    func1, func2, func3, func4, func5, func6, ...
+)
+
+# CORRECT: Import modules, access via namespace
+from pipeline import transforms
+
+result = transforms.normalize_customer(df)
+
+# CORRECT: Import only what's needed for this file
+from pipeline.transforms import normalize_customer, add_tier
+```
diff --git a/.cursor/rules/103-staff-engineer-standards.mdc b/.cursor/rules/103-staff-engineer-standards.mdc
new file mode 100644
index 0000000..a147069
--- /dev/null
+++ b/.cursor/rules/103-staff-engineer-standards.mdc
@@ -0,0 +1,368 @@
+---
+description: Staff-level data engineering standards. Enforces production-grade patterns for error handling, observability, idempotency, and operational excellence.
+globs: ["**/*.py"]
+alwaysApply: true
+---
+
+# Staff Engineer Standards
+
+## The Staff Mindset
+
+Staff engineers build **systems that build systems**. Your code is read by others, maintained for years, and runs at 3 AM when you're asleep. Design accordingly.
+
+## Error Handling
+
+### Fail Fast, Fail Loud
+
+```python
+# WRONG: Silent failures
+def load_data(path: str) -> pl.DataFrame | None:
+    try:
+        return pl.read_parquet(path)
+    except Exception:
+        return None  # Caller has no idea what happened
+
+# CORRECT: Explicit, typed failures
+from dataclasses import dataclass
+
+@dataclass
+class LoadError:
+    path: str
+    reason: str
+    original_error: Exception | None = None
+
+def load_data(path: str) -> pl.DataFrame:
+    """Load parquet file. Raises LoadError with context on failure."""
+    if not Path(path).exists():
+        raise LoadError(path, "File not found")
+    
+    try:
+        return pl.read_parquet(path)
+    except pl.exceptions.ComputeError as e:
+        raise LoadError(path, "Corrupted parquet file", e) from e
+```
+
+### Exception Hierarchy
+
+```python
+# Define domain-specific exceptions
+class PipelineError(Exception):
+    """Base exception for pipeline failures."""
+    pass
+
+class SchemaValidationError(PipelineError):
+    """Data does not match expected schema."""
+    def __init__(self, missing: set, type_mismatches: dict):
+        self.missing = missing
+        self.type_mismatches = type_mismatches
+        super().__init__(f"Missing: {missing}, Type mismatches: {type_mismatches}")
+
+class DataQualityError(PipelineError):
+    """Data fails quality checks."""
+    def __init__(self, check_name: str, failure_count: int, sample: pl.DataFrame):
+        self.check_name = check_name
+        self.failure_count = failure_count
+        self.sample = sample
+        super().__init__(f"{check_name} failed: {failure_count} rows")
+```
+
+### Never Catch Exception
+
+```python
+# WRONG: Catches everything, including bugs
+try:
+    result = complex_transform(df)
+except Exception as e:
+    logger.error(f"Transform failed: {e}")
+    return fallback_value
+
+# CORRECT: Catch specific, expected failures
+try:
+    result = complex_transform(df)
+except pl.exceptions.SchemaError as e:
+    logger.error(f"Schema mismatch: {e}")
+    raise
+except pl.exceptions.ComputeError as e:
+    logger.error(f"Compute error during transform: {e}")
+    raise
+# Let unexpected exceptions propagate
+```
+
+## Observability
+
+### Structured Logging
+
+```python
+import structlog
+
+logger = structlog.get_logger()
+
+def process_batch(batch_id: str, source_path: str) -> int:
+    log = logger.bind(batch_id=batch_id, source_path=source_path)
+    
+    log.info("batch_processing_started")
+    
+    df = pl.read_parquet(source_path)
+    row_count = len(df)
+    log.info("batch_loaded", row_count=row_count)
+    
+    result = transform(df)
+    output_count = len(result)
+    log.info(
+        "batch_processing_completed",
+        input_rows=row_count,
+        output_rows=output_count,
+        delta=output_count - row_count,
+    )
+    
+    return output_count
+```
+
+### Metrics That Matter
+
+```python
+from prometheus_client import Counter, Histogram, Gauge
+
+# Counters for events
+rows_processed = Counter(
+    "pipeline_rows_processed_total",
+    "Total rows processed",
+    ["pipeline", "stage"],
+)
+
+# Histograms for latency
+stage_duration = Histogram(
+    "pipeline_stage_duration_seconds",
+    "Time spent in each pipeline stage",
+    ["pipeline", "stage"],
+    buckets=[0.1, 0.5, 1.0, 5.0, 10.0, 30.0, 60.0],
+)
+
+# Gauges for current state
+backlog_size = Gauge(
+    "pipeline_backlog_files",
+    "Number of files waiting to be processed",
+    ["pipeline"],
+)
+
+# Usage
+with stage_duration.labels(pipeline="customers", stage="transform").time():
+    result = transform(df)
+    rows_processed.labels(pipeline="customers", stage="transform").inc(len(result))
+```
+
+## Idempotency
+
+### Deterministic Processing
+
+```python
+def process_file(file_path: str, processing_date: date) -> pl.DataFrame:
+    """
+    Idempotent file processing.
+    
+    Same inputs always produce same outputs:
+    - No random values
+    - No current timestamps (use processing_date)
+    - Deterministic ordering
+    """
+    return (
+        pl.scan_parquet(file_path)
+        .with_columns(
+            pl.lit(processing_date).alias("processing_date"),  # Deterministic
+            pl.lit(file_path).alias("source_file"),
+        )
+        .sort("id")  # Deterministic order
+        .collect()
+    )
+```
+
+### Idempotent Writes
+
+```python
+def upsert_to_delta(
+    df: pl.DataFrame,
+    table_path: str,
+    merge_keys: list[str],
+) -> None:
+    """
+    Idempotent upsert: running twice with same data = no change.
+    """
+    from deltalake import DeltaTable, write_deltalake
+    
+    if not DeltaTable.is_deltatable(table_path):
+        write_deltalake(table_path, df.to_arrow())
+        return
+    
+    dt = DeltaTable(table_path)
+    merge_predicate = " AND ".join(
+        f"source.{k} = target.{k}" for k in merge_keys
+    )
+    
+    (
+        dt.merge(
+            source=df.to_arrow(),
+            predicate=merge_predicate,
+            source_alias="source",
+            target_alias="target",
+        )
+        .when_matched_update_all()
+        .when_not_matched_insert_all()
+        .execute()
+    )
+```
+
+## Defensive Programming
+
+### Validate at Boundaries
+
+```python
+def ingest_external_data(api_response: dict) -> pl.DataFrame:
+    """Validate untrusted external data at ingestion boundary."""
+    
+    # Validate structure
+    required_fields = {"records", "metadata"}
+    missing = required_fields - set(api_response.keys())
+    if missing:
+        raise ValueError(f"API response missing fields: {missing}")
+    
+    # Validate types
+    if not isinstance(api_response["records"], list):
+        raise TypeError("records must be a list")
+    
+    # Parse and validate schema
+    df = pl.DataFrame(api_response["records"])
+    
+    # Validate business rules
+    if (df["amount"] < 0).any():
+        raise DataQualityError("negative_amounts", (df["amount"] < 0).sum(), df)
+    
+    return df
+```
+
+### Assertion-Based Invariants
+
+```python
+def merge_datasets(
+    left: pl.DataFrame,
+    right: pl.DataFrame,
+    join_key: str,
+) -> pl.DataFrame:
+    """Merge with invariant checks."""
+    
+    # Pre-conditions
+    assert join_key in left.columns, f"{join_key} not in left"
+    assert join_key in right.columns, f"{join_key} not in right"
+    assert left[join_key].null_count() == 0, "Nulls in left join key"
+    assert right[join_key].null_count() == 0, "Nulls in right join key"
+    
+    left_count = len(left)
+    result = left.join(right, on=join_key, how="left")
+    
+    # Post-conditions: left join should preserve left row count
+    assert len(result) == left_count, (
+        f"Row count changed: {left_count} -> {len(result)}. "
+        "Right side has duplicates on join key."
+    )
+    
+    return result
+```
+
+## Performance Discipline
+
+### Profile Before Optimizing
+
+```python
+# Add timing to understand bottlenecks
+import time
+from contextlib import contextmanager
+
+@contextmanager
+def timed_stage(stage_name: str):
+    start = time.perf_counter()
+    yield
+    elapsed = time.perf_counter() - start
+    logger.info(f"{stage_name} completed", duration_seconds=elapsed)
+
+# Usage
+with timed_stage("transform"):
+    df = heavy_transform(raw_data)
+
+with timed_stage("write"):
+    df.write_parquet(output_path)
+```
+
+### Memory Awareness
+
+```python
+def process_large_dataset(path_pattern: str, chunk_size: int = 100_000) -> None:
+    """
+    Process large datasets without OOM.
+    Uses streaming when possible, chunking when not.
+    """
+    lf = pl.scan_parquet(path_pattern)
+    
+    # Try streaming first
+    try:
+        result = lf.collect(streaming=True)
+        result.write_parquet("output.parquet")
+        return
+    except pl.exceptions.ComputeError:
+        logger.warning("Streaming failed, falling back to chunked processing")
+    
+    # Chunked fallback
+    total_rows = lf.select(pl.len()).collect().item()
+    
+    for offset in range(0, total_rows, chunk_size):
+        chunk = lf.slice(offset, chunk_size).collect()
+        mode = "overwrite" if offset == 0 else "append"
+        chunk.write_parquet(f"output/chunk_{offset}.parquet")
+```
+
+## Documentation Standards
+
+### Docstrings with Purpose
+
+```python
+def calculate_customer_ltv(
+    transactions: pl.DataFrame,
+    as_of_date: date,
+    discount_rate: float = 0.1,
+) -> pl.DataFrame:
+    """
+    Calculate customer lifetime value using discounted cash flow.
+    
+    Uses a simplified DCF model where future value is projected based on
+    historical transaction patterns and discounted to present value.
+    
+    Args:
+        transactions: Must contain columns: customer_id, amount, transaction_date
+        as_of_date: Reference date for calculations
+        discount_rate: Annual discount rate (default 10%)
+    
+    Returns:
+        DataFrame with columns: customer_id, ltv, confidence
+        
+    Raises:
+        SchemaValidationError: If required columns are missing
+        
+    Example:
+        >>> ltv = calculate_customer_ltv(txns, date(2024, 1, 1))
+        >>> top_customers = ltv.filter(pl.col("ltv") > 10000)
+        
+    Note:
+        LTV calculations require at least 3 months of transaction history.
+        Customers with insufficient history will have confidence='low'.
+    """
+```
+
+## Code Review Checklist
+
+Before approving any PR:
+
+1. **Error handling**: Are failures explicit and recoverable?
+2. **Observability**: Can we debug this at 3 AM?
+3. **Idempotency**: Is it safe to run twice?
+4. **Boundaries**: Is external data validated at entry?
+5. **Performance**: Any unbounded operations?
+6. **Testing**: Are edge cases covered?
+7. **Documentation**: Can someone new understand the why?
diff --git a/.cursor/rules/active-session.mdc b/.cursor/rules/active-session.mdc
new file mode 100644
index 0000000..3dae4a8
--- /dev/null
+++ b/.cursor/rules/active-session.mdc
@@ -0,0 +1,56 @@
+---
+alwaysApply: false
+---
+
+ACTIVE SESSION RULE
+
+Session Identifier: multi-table-progress-visibility-debugging
+Session File: .context/sessions/multi-table-progress-visibility-debugging.yaml
+
+Resume Prompt Template:
+Resume work on multi-table progress visibility and verbose logging implementation.
+
+Current state:
+- Progress callback system added to save_zip_upload() in user_datasets.py
+- UI progress bars and detailed logging added to Upload_Data.py page
+- Streamlit launch script updated for verbose mode (--logger.level=info)
+- Comprehensive test suite created (test_zip_upload_progress.py) - all 6 tests passing
+- Verbose logging working: shows individual table loading with row/column counts
+- Relationship detection logging: shows 94 relationships detected for MIMIC-IV demo
+- Progress tracking: real-time updates for each table (1/32, 2/32, etc.)
+
+Completed:
+- ‚úÖ Progress callback parameter added to save_zip_upload()
+- ‚úÖ Progress updates at each step: initialization, table discovery, loading, relationship detection, cohort building, saving, schema inference
+- ‚úÖ UI progress bar and status text in Streamlit
+- ‚úÖ Expandable processing log showing each table as it loads
+- ‚úÖ Detailed table information (rows, cols) in progress callbacks
+- ‚úÖ Logging configuration with INFO level for multi_table_handler and user_datasets
+- ‚úÖ All tests passing (6/6) verifying progress callback functionality
+- ‚úÖ Fixed indentation errors
+- ‚úÖ Fixed ruff linting issues (whitespace, imports, type hints)
+- ‚úÖ Fixed step calculation bug (total_steps consistency)
+
+Current issue:
+- ‚ùå DuckDB OutOfMemoryException when joining all 32 tables in single query
+- Error: "failed to offload data block of size 32.0 KiB (90.8 GiB/90.8 GiB used)"
+- Large tables like chartevents (668,862 rows) causing memory explosion
+- Single massive LEFT JOIN of all tables exhausts temp directory space
+
+Proposed solutions (not yet implemented):
+1. Configure DuckDB with better memory settings (memory_limit, threads, temp_directory)
+2. Use incremental joins instead of single massive join
+3. Selective joins - exclude very large tables by default (e.g., chartevents > 100k rows)
+
+Key decisions:
+- Progress callback signature: (step: int, total_steps: int, message: str, details: dict) -> None
+- Total steps calculation: 1 (init) + len(csv_files) (loading) + 4 (detect, build, save, infer)
+- UI shows progress bar, status text, and expandable log container
+- Logging at INFO level for visibility without overwhelming output
+- Tests verify callback is called, receives correct data, and handles edge cases
+
+Next steps:
+1. Fix DuckDB memory issue - configure connection with appropriate limits
+2. Consider incremental join strategy for very large datasets
+3. Add option to exclude large tables from unified cohort
+4. Test with full MIMIC-IV demo dataset (32 tables, some with 600k+ rows)
diff --git a/data/raw/sepsis/physionet.org/robots.txt b/data/raw/sepsis/physionet.org/robots.txt
deleted file mode 100644
index 8993960..0000000
--- a/data/raw/sepsis/physionet.org/robots.txt
+++ /dev/null
@@ -1 +0,0 @@
-User-Agent: *\Allow: /
\ No newline at end of file
diff --git a/data/uploads/metadata/user_upload_20251225_002011_db3af1fe.json b/data/uploads/metadata/user_upload_20251225_002011_db3af1fe.json
new file mode 100644
index 0000000..3f973b1
--- /dev/null
+++ b/data/uploads/metadata/user_upload_20251225_002011_db3af1fe.json
@@ -0,0 +1,29 @@
+{
+  "upload_id": "user_upload_20251225_002011_db3af1fe",
+  "original_filename": "mimic-iv-clinical-database-demo-2.2.zip",
+  "upload_timestamp": "2025-12-25T00:20:11.098201",
+  "file_size_bytes": 16189661,
+  "file_format": "zip_multi_table",
+  "row_count": 100,
+  "column_count": 1,
+  "columns": [
+    "subject_id"
+  ],
+  "tables": [
+    "demo_subject_id"
+  ],
+  "table_counts": {
+    "demo_subject_id": 100
+  },
+  "relationships": [],
+  "inferred_schema": {
+    "column_mapping": {
+      "subject_id": "patient_id"
+    },
+    "outcomes": {},
+    "time_zero": {
+      "value": "2020-01-01"
+    }
+  },
+  "dataset_name": "mimic-iv-clinical-database-demo-2.2"
+}
\ No newline at end of file
diff --git a/data/uploads/metadata/user_upload_20251225_002230_db3af1fe.json b/data/uploads/metadata/user_upload_20251225_002230_db3af1fe.json
new file mode 100644
index 0000000..d58aceb
--- /dev/null
+++ b/data/uploads/metadata/user_upload_20251225_002230_db3af1fe.json
@@ -0,0 +1,29 @@
+{
+  "upload_id": "user_upload_20251225_002230_db3af1fe",
+  "original_filename": "mimic-iv-clinical-database-demo-2.2.zip",
+  "upload_timestamp": "2025-12-25T00:22:30.325465",
+  "file_size_bytes": 16189661,
+  "file_format": "zip_multi_table",
+  "row_count": 100,
+  "column_count": 1,
+  "columns": [
+    "subject_id"
+  ],
+  "tables": [
+    "demo_subject_id"
+  ],
+  "table_counts": {
+    "demo_subject_id": 100
+  },
+  "relationships": [],
+  "inferred_schema": {
+    "column_mapping": {
+      "subject_id": "patient_id"
+    },
+    "outcomes": {},
+    "time_zero": {
+      "value": "2020-01-01"
+    }
+  },
+  "dataset_name": "mimic-iv-clinical-database-demo-2.2"
+}
\ No newline at end of file
diff --git a/data/uploads/raw/user_upload_20251225_002011_db3af1fe.csv b/data/uploads/raw/user_upload_20251225_002011_db3af1fe.csv
new file mode 100644
index 0000000..2d1febc
--- /dev/null
+++ b/data/uploads/raw/user_upload_20251225_002011_db3af1fe.csv
@@ -0,0 +1,101 @@
+subject_id
+10000032
+10001217
+10001725
+10002428
+10002495
+10002930
+10003046
+10003400
+10004235
+10004422
+10004457
+10004720
+10004733
+10005348
+10005817
+10005866
+10005909
+10006053
+10006580
+10007058
+10007795
+10007818
+10007928
+10008287
+10008454
+10009035
+10009049
+10009628
+10010471
+10010867
+10011398
+10012552
+10012853
+10013049
+10014078
+10014354
+10014729
+10015272
+10015860
+10015931
+10016150
+10016742
+10016810
+10017492
+10018081
+10018328
+10018423
+10018501
+10018845
+10019003
+10019172
+10019385
+10019568
+10019777
+10019917
+10020187
+10020306
+10020640
+10020740
+10020786
+10020944
+10021118
+10021312
+10021487
+10021666
+10021938
+10022017
+10022041
+10022281
+10022880
+10023117
+10023239
+10023771
+10024043
+10025463
+10025612
+10026255
+10026354
+10026406
+10027445
+10027602
+10029291
+10029484
+10031404
+10031757
+10032725
+10035185
+10035631
+10036156
+10037861
+10037928
+10037975
+10038081
+10038933
+10038992
+10038999
+10039708
+10039831
+10039997
+10040025
diff --git a/data/uploads/raw/user_upload_20251225_002230_db3af1fe.csv b/data/uploads/raw/user_upload_20251225_002230_db3af1fe.csv
new file mode 100644
index 0000000..2d1febc
--- /dev/null
+++ b/data/uploads/raw/user_upload_20251225_002230_db3af1fe.csv
@@ -0,0 +1,101 @@
+subject_id
+10000032
+10001217
+10001725
+10002428
+10002495
+10002930
+10003046
+10003400
+10004235
+10004422
+10004457
+10004720
+10004733
+10005348
+10005817
+10005866
+10005909
+10006053
+10006580
+10007058
+10007795
+10007818
+10007928
+10008287
+10008454
+10009035
+10009049
+10009628
+10010471
+10010867
+10011398
+10012552
+10012853
+10013049
+10014078
+10014354
+10014729
+10015272
+10015860
+10015931
+10016150
+10016742
+10016810
+10017492
+10018081
+10018328
+10018423
+10018501
+10018845
+10019003
+10019172
+10019385
+10019568
+10019777
+10019917
+10020187
+10020306
+10020640
+10020740
+10020786
+10020944
+10021118
+10021312
+10021487
+10021666
+10021938
+10022017
+10022041
+10022281
+10022880
+10023117
+10023239
+10023771
+10024043
+10025463
+10025612
+10026255
+10026354
+10026406
+10027445
+10027602
+10029291
+10029484
+10031404
+10031757
+10032725
+10035185
+10035631
+10036156
+10037861
+10037928
+10037975
+10038081
+10038933
+10038992
+10038999
+10039708
+10039831
+10039997
+10040025
diff --git a/docs/implementation/plans/consolidate-docs-and-implement-question-driven-analysis.md b/docs/implementation/plans/consolidate-docs-and-implement-question-driven-analysis.md
index c419ce0..6148e61 100644
--- a/docs/implementation/plans/consolidate-docs-and-implement-question-driven-analysis.md
+++ b/docs/implementation/plans/consolidate-docs-and-implement-question-driven-analysis.md
@@ -85,25 +85,60 @@ This plan addresses the critical transition from hardcoded, menu-driven analysis
 - "descriptive statistics"
 - "correlation between age and outcome"
 
-### ‚è≥ Phase 3: Automatic Schema Inference (Pending)
+### üîÑ Phase 3: Automatic Schema Inference (In Progress)
+
+**Delivered Features:**
+- ‚úÖ Remove all hardcoded YAML configs
+- ‚úÖ Auto-detect patient IDs, outcomes, time variables from DataFrame
+- ‚úÖ **Parse data dictionary PDFs using LangChain** (NEW!)
+- ‚úÖ **Merge PDF metadata with inferred schema** (NEW!)
+- ‚úÖ Schema inference for uploaded datasets
+- ‚úÖ Consistent handling of built-in and uploaded datasets
+
+**Key Innovation: Dictionary-Enhanced Schema Inference**
+Before users see their data, the system now:
+1. Checks for corresponding data dictionary PDF in `data/dictionaries/`
+2. Extracts column descriptions, types, and valid values using LangChain
+3. Merges PDF metadata with DataFrame-based inference
+4. Presents enriched schema with both statistical analysis + documentation context
+5. Boosts confidence scores when dictionary confirms inferred types
+
+**Implementation:**
+- `schema_inference.py`:
+  - `DictionaryMetadata` dataclass for PDF-extracted info
+  - `parse_dictionary_pdf()` using LangChain's PyPDFLoader
+  - `infer_schema_with_dictionary()` merges both sources
+- `registry.py`:
+  - `register_from_dataframe()` with automatic schema inference
+  - Stores DataFrames for auto-inferred datasets
 
-**Planned Features:**
-- Remove all hardcoded YAML configs
-- Auto-detect patient IDs, outcomes, time variables
-- Schema inference for uploaded datasets
-- Consistent handling of built-in and uploaded datasets
-
-**Target:** Future PR
-
-### ‚è≥ Phase 4: Multi-Table Support (Pending)
-
-**Planned Features:**
-- ZIP upload with multiple CSVs
-- Auto-detect foreign key relationships
-- Build join graph and execute joins
-- MIMIC-IV dataset support
-
-**Target:** Future PR
+**Dependencies Added:**
+- `langchain` ^1.2.0
+- `langchain-community` ^0.4.1
+
+**Target:** PR #3 (in progress)
+
+### üîÑ Phase 4: Multi-Table Support (In Progress)
+
+**Delivered Features:**
+- ‚úÖ ZIP upload with multiple CSVs
+- ‚úÖ Auto-detect foreign key relationships using Polars/DuckDB
+- ‚úÖ Build join graph and execute joins via DuckDB SQL
+- ‚úÖ MIMIC-IV-style dataset support
+- ‚úÖ Unified cohort creation from related tables
+
+**Implementation:**
+- `multi_table_handler.py`:
+  - `TableRelationship` dataclass for FK detection
+  - `MultiTableHandler` with DuckDB-based joins
+  - Primary/foreign key detection via cardinality + naming patterns
+  - BFS join graph traversal
+- `user_datasets.py`:
+  - `save_zip_upload()` method for multi-table ZIP files
+  - Automatic relationship detection and cohort building
+  - Metadata includes table counts and join relationships
+
+**Target:** PR #3 (in progress)
 
 ### ‚è≥ Phase 5: Testing & Refinement (Pending)
 
diff --git a/docs/screenshots/Analyze _ Clinical Analyticsmimic.pdf b/docs/screenshots/Analyze _ Clinical Analyticsmimic.pdf
new file mode 100644
index 0000000..7a7c7b9
Binary files /dev/null and b/docs/screenshots/Analyze _ Clinical Analyticsmimic.pdf differ
diff --git a/docs/screenshots/Descriptive Statistics _ Clinical Analyticsmimic.pdf b/docs/screenshots/Descriptive Statistics _ Clinical Analyticsmimic.pdf
new file mode 100644
index 0000000..8312c73
Binary files /dev/null and b/docs/screenshots/Descriptive Statistics _ Clinical Analyticsmimic.pdf differ
diff --git a/docs/screenshots/Upload Data _ Clinical Analytics.pdf b/docs/screenshots/Upload Data _ Clinical Analytics.pdf
new file mode 100644
index 0000000..ec53770
Binary files /dev/null and b/docs/screenshots/Upload Data _ Clinical Analytics.pdf differ
diff --git a/pyproject.toml b/pyproject.toml
index 3bc4b02..81e2362 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -18,6 +18,8 @@ dependencies = [
     "sentence-transformers>=5.2.0",
     "scikit-learn>=1.8.0",
     "seaborn>=0.13.2",
+    "langchain>=1.2.0",
+    "langchain-community>=0.4.1",
 ]
 
 [project.optional-dependencies]
@@ -64,4 +66,6 @@ dev = [
     "mkdocs-mermaid2-plugin>=1.2.3",
     "mkdocstrings[python]>=1.0.0",
     "pymdown-extensions>=10.19.1",
+    "pytest>=9.0.2",
+    "pytest-cov>=7.0.0",
 ]
diff --git a/scripts/run_app.sh b/scripts/run_app.sh
index 575c21b..a848a4f 100755
--- a/scripts/run_app.sh
+++ b/scripts/run_app.sh
@@ -73,6 +73,8 @@ echo ""
 echo -e "${YELLOW}   Press Ctrl+C to stop the server${NC}"
 echo ""
 
-# Run streamlit with the app
+# Run streamlit with the app (verbose mode for debugging)
 source .venv/bin/activate
-streamlit run src/clinical_analytics/ui/app.py
+streamlit run src/clinical_analytics/ui/app.py \
+    --logger.level=info \
+    --server.fileWatcherType=poll
diff --git a/src/clinical_analytics/core/mapper.py b/src/clinical_analytics/core/mapper.py
index 9babaf5..83bd4b9 100644
--- a/src/clinical_analytics/core/mapper.py
+++ b/src/clinical_analytics/core/mapper.py
@@ -162,7 +162,26 @@ class ColumnMapper:
             elif filter_type == 'in':
                 # Value in list
                 if isinstance(filter_value, list):
-                    df = df.filter(pl.col(column).is_in(filter_value))
+                    # Ensure column and filter values have compatible types
+                    # Cast filter_value list elements to match column dtype
+                    col_dtype = df[column].dtype
+                    try:
+                        # If column is string, ensure filter values are strings
+                        if col_dtype == pl.Utf8:
+                            filter_value = [str(v) for v in filter_value]
+                        # If column is numeric, try to convert filter values
+                        elif col_dtype in [pl.Int8, pl.Int16, pl.Int32, pl.Int64]:
+                            filter_value = [int(v) for v in filter_value if v is not None]
+                        elif col_dtype in [pl.Float32, pl.Float64]:
+                            filter_value = [float(v) for v in filter_value if v is not None]
+                        
+                        df = df.filter(pl.col(column).is_in(filter_value))
+                    except Exception as e:
+                        import logging
+                        logger = logging.getLogger(__name__)
+                        logger.error(f"Error in 'in' filter for column {column} (dtype={col_dtype}): {type(e).__name__}: {str(e)}")
+                        logger.error(f"Filter values: {filter_value[:10]}... (type: {type(filter_value[0]) if filter_value else 'empty'})")
+                        raise
                     
             elif filter_type == 'range':
                 # Range filter (min, max)
diff --git a/src/clinical_analytics/core/multi_table_handler.py b/src/clinical_analytics/core/multi_table_handler.py
new file mode 100644
index 0000000..7b5bbca
--- /dev/null
+++ b/src/clinical_analytics/core/multi_table_handler.py
@@ -0,0 +1,1440 @@
+"""
+Multi-Table Handler - Automatic Relationship Detection and Joins
+
+This module enables handling of multi-table datasets (like MIMIC-IV) by automatically:
+- Detecting primary keys in each table
+- Discovering foreign key relationships
+- Building a join graph
+- Executing joins to create unified cohort views
+
+Key Principles:
+- Use Polars for all DataFrame operations
+- Use DuckDB for SQL-based joins
+- Privacy-preserving (all local computation)
+- Fail gracefully with user override options
+"""
+
+from dataclasses import dataclass
+from typing import Dict, List, Tuple, Optional, Set, Literal
+import polars as pl
+import duckdb
+import logging
+
+logger = logging.getLogger(__name__)
+
+
+@dataclass
+class TableClassification:
+    """
+    Classification metadata for a table in a multi-table dataset.
+
+    Used to determine how a table should be handled in aggregate-before-join pipeline:
+    - Dimensions: Small, unique on grain, safe to join directly
+    - Facts: High cardinality, must be pre-aggregated before joining
+    - Events: Time-series facts, high cardinality, temporal dimension
+    - Bridges: Many-to-many relationships (composite unique), excluded from auto-joins
+    - Reference: Code mappings, lookup tables with duplicates allowed
+
+    Attributes:
+        table_name: Name of the table
+        classification: Category based on cardinality and structure
+        grain: Level of granularity (patient, admission, event)
+        grain_key: Column name of the detected grain key
+        cardinality_ratio: rows / unique(grain_key), >1.1 indicates facts/events
+        is_unique_on_grain: True if grain_key is unique (cardinality_ratio ‚âà 1.0)
+        estimated_bytes: Total table size in bytes (rows * avg_row_bytes)
+        relationship_degree: Number of foreign keys detected
+        has_time_column: True if table has non-constant time column
+        time_column_name: Name of time column if detected
+        is_n_side_of_anchor: True if table is on N-side of relationship to anchor
+        null_rate_in_grain: % of NULLs in grain_key column (0-1)
+    """
+    table_name: str
+    classification: Literal["dimension", "fact", "event", "bridge", "reference"]
+    grain: Literal["patient", "admission", "event"]
+    grain_key: str
+    cardinality_ratio: float
+    is_unique_on_grain: bool
+    estimated_bytes: int
+    relationship_degree: int
+    has_time_column: bool
+    time_column_name: Optional[str]
+    is_n_side_of_anchor: bool
+    null_rate_in_grain: float
+
+
+@dataclass
+class TableRelationship:
+    """
+    Detected relationship between two tables.
+
+    Represents a foreign key relationship discovered through pattern matching
+    and referential integrity analysis.
+
+    Attributes:
+        parent_table: Table with the primary key
+        child_table: Table with the foreign key
+        parent_key: Primary key column name
+        child_key: Foreign key column name
+        relationship_type: Type of relationship (one-to-many, etc.)
+        confidence: Detection confidence score (0-1)
+        match_ratio: % of child values that exist in parent
+    """
+    parent_table: str
+    child_table: str
+    parent_key: str
+    child_key: str
+    relationship_type: str  # "one-to-many", "many-to-one", "one-to-one"
+    confidence: float
+    match_ratio: float = 0.0
+
+    def __str__(self) -> str:
+        return f"{self.parent_table}.{self.parent_key} ‚Üí {self.child_table}.{self.child_key} " \
+               f"({self.relationship_type}, conf={self.confidence:.2f}, match={self.match_ratio:.2f})"
+
+
+class MultiTableHandler:
+    """
+    Handle multi-table datasets with automatic relationship detection.
+
+    Workflow:
+    1. Load all tables (Dict[table_name, Polars DataFrame])
+    2. Detect primary keys in each table
+    3. Discover foreign key relationships
+    4. Build join graph
+    5. Execute joins to create unified cohort view
+
+    Example:
+        >>> tables = {
+        ...     'patients': pl.read_csv('patients.csv'),
+        ...     'admissions': pl.read_csv('admissions.csv'),
+        ...     'diagnoses': pl.read_csv('diagnoses.csv')
+        ... }
+        >>> handler = MultiTableHandler(tables)
+        >>> relationships = handler.detect_relationships()
+        >>> cohort = handler.build_unified_cohort()
+    """
+
+    def __init__(
+        self,
+        tables: Dict[str, pl.DataFrame],
+        max_dimension_bytes: int = 250_000_000  # 250 MB default
+    ):
+        """
+        Initialize with dictionary of table_name -> Polars DataFrame.
+
+        Args:
+            tables: Dict mapping table names to Polars DataFrames
+            max_dimension_bytes: Max size for dimension tables (default 250 MB)
+        """
+        self.tables = tables
+        self.relationships: List[TableRelationship] = []
+        self.primary_keys: Dict[str, str] = {}
+        self.classifications: Dict[str, TableClassification] = {}
+        self.max_dimension_bytes = max_dimension_bytes
+
+        # Normalize key column types across all tables
+        self._normalize_key_columns()
+
+        # Initialize DuckDB connection for SQL-based joins
+        self.conn = duckdb.connect(':memory:')
+
+        # Register all tables in DuckDB
+        for table_name, df in self.tables.items():
+            self.conn.register(table_name, df)
+
+    def _normalize_key_columns(self) -> None:
+        """
+        Normalize data types for common key columns across all tables.
+
+        This prevents type mismatch errors when comparing keys across tables
+        (e.g., subject_id as int64 in one table, string in another).
+
+        Strategy:
+        1. Identify columns that appear in multiple tables (potential join keys)
+        2. Normalize them to string type for consistent comparisons
+        """
+        logger.debug("Starting key column normalization")
+        
+        # Count how many tables each column appears in
+        column_counts = {}
+        for df in self.tables.values():
+            for col in df.columns:
+                column_counts[col] = column_counts.get(col, 0) + 1
+
+        # Identify columns that appear in multiple tables (potential join keys)
+        shared_columns = {col for col, count in column_counts.items() if count > 1}
+        logger.debug(f"Found {len(shared_columns)} shared columns: {list(shared_columns)[:10]}")
+
+        # Also include common key patterns
+        key_patterns = [
+            '_id', 'subject_id', 'hadm_id', 'stay_id', 'itemid',
+            'icustay_id', 'transfer_id', 'caregiver_id', 'charttime'
+        ]
+
+        key_columns = shared_columns.copy()
+        for df in self.tables.values():
+            for col in df.columns:
+                col_lower = col.lower()
+                if any(pattern in col_lower for pattern in key_patterns):
+                    key_columns.add(col)
+
+        logger.debug(f"Total key columns to normalize: {len(key_columns)}")
+
+        # Normalize each key column to string type across all tables
+        for table_name, df in self.tables.items():
+            cols_to_cast = [col for col in key_columns if col in df.columns]
+
+            if cols_to_cast:
+                logger.debug(f"Normalizing {len(cols_to_cast)} columns in table '{table_name}': {cols_to_cast}")
+                # Log original dtypes
+                original_dtypes = {col: df[col].dtype for col in cols_to_cast}
+                logger.debug(f"Original dtypes for {table_name}: {original_dtypes}")
+                
+                try:
+                    # Cast all key columns in one operation
+                    cast_exprs = [pl.col(col).cast(pl.Utf8, strict=False).alias(col)
+                                  for col in cols_to_cast]
+                    self.tables[table_name] = df.with_columns(cast_exprs)
+                    
+                    # Verify cast succeeded
+                    new_dtypes = {col: self.tables[table_name][col].dtype for col in cols_to_cast}
+                    logger.debug(f"New dtypes for {table_name}: {new_dtypes}")
+                    
+                    # Check for any that didn't convert
+                    failed_casts = [col for col in cols_to_cast if self.tables[table_name][col].dtype != pl.Utf8]
+                    if failed_casts:
+                        logger.warning(f"Failed to cast columns in {table_name}: {failed_casts}")
+                        
+                except Exception as e:
+                    logger.error(f"Batch cast failed for {table_name}: {type(e).__name__}: {str(e)}")
+                    # Try one by one if batch fails
+                    for col in cols_to_cast:
+                        try:
+                            original_dtype = df[col].dtype
+                            self.tables[table_name] = self.tables[table_name].with_columns(
+                                pl.col(col).cast(pl.Utf8, strict=False)
+                            )
+                            new_dtype = self.tables[table_name][col].dtype
+                            logger.debug(f"Cast {table_name}.{col}: {original_dtype} -> {new_dtype}")
+                        except Exception as col_e:
+                            logger.error(f"Failed to cast {table_name}.{col} from {df[col].dtype}: {type(col_e).__name__}: {str(col_e)}")
+                            pass  # Skip if casting fails
+
+    def _sample_df(self, df: pl.DataFrame, n: int = 10_000) -> pl.DataFrame:
+        """
+        Deterministic sample using head (fastest, stable).
+
+        Bounds classification cost to O(sample_size) regardless of table size.
+        Uses head() for simplicity and stability (no randomness).
+
+        Args:
+            df: DataFrame to sample
+            n: Maximum sample size (default 10k rows)
+
+        Returns:
+            Sampled DataFrame with at most n rows
+        """
+        return df.head(min(n, df.height))
+
+    def _compute_sampled_uniqueness(self, df: pl.DataFrame, col: str) -> tuple[int, float]:
+        """
+        Compute uniqueness and null rate on sampled data only.
+
+        Critical: This method ensures all uniqueness computations are bounded
+        by sample size, preventing expensive full table scans.
+
+        Args:
+            df: DataFrame to sample
+            col: Column name
+
+        Returns:
+            Tuple of (unique_count, null_rate)
+        """
+        s = self._sample_df(df)
+
+        if col not in s.columns:
+            return (0, 1.0)
+
+        non_null = s[col].drop_nulls()
+        if non_null.len() == 0:
+            return (0, 1.0)
+
+        unique_count = non_null.n_unique()
+        null_rate = s[col].null_count() / max(s.height, 1)
+
+        return (unique_count, null_rate)
+
+    def _is_probably_id_col(self, col_name: str) -> bool:
+        """
+        Check if column name suggests ID column using tight pattern matching.
+
+        Tight pattern: exact 'id' or endswith('_id')
+        This avoids false positives like: valid, fluid, paid, acid
+
+        Args:
+            col_name: Column name to check
+
+        Returns:
+            True if column name matches tight ID pattern
+        """
+        col_lower = col_name.lower()
+        return col_lower == "id" or col_lower.endswith("_id")
+
+    def detect_relationships(self) -> List[TableRelationship]:
+        """
+        Auto-detect foreign key relationships between tables.
+
+        Strategy:
+        1. Detect primary keys for each table
+        2. For each table pair, check if child has column matching parent PK
+        3. Verify referential integrity (% of child values in parent)
+        4. Score confidence based on name similarity + integrity
+
+        Returns:
+            List of detected relationships sorted by confidence
+        """
+        # 1. Detect primary keys for each table
+        for table_name, df in self.tables.items():
+            pk = self._detect_primary_key(df)
+            if pk:
+                self.primary_keys[table_name] = pk
+
+        # 2. Detect foreign key relationships
+        for parent_table, parent_df in self.tables.items():
+            parent_pk = self.primary_keys.get(parent_table)
+
+            if not parent_pk:
+                continue  # Skip tables without primary keys
+
+            for child_table, child_df in self.tables.items():
+                if parent_table == child_table:
+                    continue  # Skip self-joins
+
+                # Check if child has column matching parent's primary key
+                for child_col in child_df.columns:
+                    if self._is_foreign_key_candidate(parent_pk, child_col):
+                        # Verify referential integrity
+                        match_ratio = self._verify_referential_integrity(
+                            parent_df, parent_pk, child_df, child_col
+                        )
+
+                        if match_ratio > 0.8:  # Threshold for FK confidence
+                            # Calculate confidence based on name match + integrity
+                            name_conf = 1.0 if parent_pk.lower() == child_col.lower() else 0.9
+                            confidence = (name_conf + match_ratio) / 2
+
+                            self.relationships.append(TableRelationship(
+                                parent_table=parent_table,
+                                child_table=child_table,
+                                parent_key=parent_pk,
+                                child_key=child_col,
+                                relationship_type="one-to-many",
+                                confidence=confidence,
+                                match_ratio=match_ratio
+                            ))
+
+        # Sort by confidence (descending)
+        self.relationships.sort(key=lambda r: r.confidence, reverse=True)
+
+        return self.relationships
+
+    def classify_tables(self, anchor_table: Optional[str] = None) -> Dict[str, TableClassification]:
+        """
+        Classify all tables as dimension/fact/event/bridge/reference.
+
+        Classification rules:
+        - Dimension: cardinality_ratio <= 1.1, unique on grain, bytes < max_dimension_bytes
+        - Fact: cardinality_ratio > 1.1, NOT unique on grain, NOT bridge
+        - Event: has_time_column, cardinality_ratio > 1.1, time not constant, N-side of anchor
+        - Bridge: 2+ foreign keys, neither unique, but composite near-unique
+        - Reference: code mappings, small size with duplicates allowed
+
+        Args:
+            anchor_table: Optional anchor table name (for is_n_side_of_anchor detection)
+
+        Returns:
+            Dict mapping table names to TableClassification objects
+        """
+        logger.info("Classifying tables for aggregate-before-join pipeline")
+
+        # Ensure relationships are detected first
+        if not self.relationships:
+            self.detect_relationships()
+
+        # Count foreign keys per table
+        fk_counts = self._count_foreign_keys()
+
+        for table_name, df in self.tables.items():
+            # Detect grain key
+            grain_key = self._detect_grain_key(df)
+            if not grain_key:
+                logger.warning(f"No grain key detected for table '{table_name}', skipping classification")
+                continue
+
+            # Calculate cardinality metrics using sampled data
+            total_rows = df.height
+
+            # Use sampled uniqueness to prevent expensive full table scans
+            sampled_unique, sampled_null_rate = self._compute_sampled_uniqueness(df, grain_key)
+
+            # Estimate cardinality ratio from sample
+            s = self._sample_df(df)
+            non_null_sample_rows = s.height - int(sampled_null_rate * s.height)
+
+            if sampled_unique > 0 and non_null_sample_rows > 0:
+                # Estimate total unique values from sample ratio
+                sample_uniq_ratio = sampled_unique / non_null_sample_rows
+                estimated_total_unique = int(total_rows * sample_uniq_ratio)
+
+                # Cardinality ratio: total_rows / unique_values
+                # If ratio is ~1.0, table is unique on grain (dimension)
+                # If ratio > 1.1, table has duplicates (fact/event)
+                cardinality_ratio = total_rows / max(estimated_total_unique, 1)
+                is_unique = (cardinality_ratio <= 1.05)  # Allow 5% tolerance for sampling error
+            else:
+                cardinality_ratio = float('inf')
+                is_unique = False
+
+            null_rate = sampled_null_rate
+
+            # Estimate bytes
+            estimated_bytes = self._estimate_table_bytes(df)
+
+            # Detect time column
+            time_col, has_time = self._detect_time_column(df)
+
+            # Detect grain level
+            grain_level = self._detect_grain_level(grain_key)
+
+            # Detect bridge table
+            is_bridge = self._detect_bridge_table(table_name, df, fk_counts.get(table_name, 0))
+
+            # Determine if table is on N-side of anchor relationship
+            is_n_side = self._is_n_side_of_anchor(table_name, anchor_table) if anchor_table else False
+
+            # Classify based on rules
+            classification = self._classify_table_type(
+                cardinality_ratio=cardinality_ratio,
+                is_unique=is_unique,
+                estimated_bytes=estimated_bytes,
+                has_time=has_time,
+                is_bridge=is_bridge,
+                is_n_side=is_n_side,
+                time_col=time_col,
+                df=df
+            )
+
+            # Store classification
+            self.classifications[table_name] = TableClassification(
+                table_name=table_name,
+                classification=classification,
+                grain=grain_level,
+                grain_key=grain_key,
+                cardinality_ratio=cardinality_ratio,
+                is_unique_on_grain=is_unique,
+                estimated_bytes=estimated_bytes,
+                relationship_degree=fk_counts.get(table_name, 0),
+                has_time_column=has_time,
+                time_column_name=time_col,
+                is_n_side_of_anchor=is_n_side,
+                null_rate_in_grain=null_rate
+            )
+
+            logger.debug(
+                f"Classified '{table_name}': {classification} "
+                f"(grain={grain_level}, key={grain_key}, card_ratio={cardinality_ratio:.2f}, "
+                f"bytes={estimated_bytes:,}, fks={fk_counts.get(table_name, 0)})"
+            )
+
+        return self.classifications
+
+    def _detect_grain_key(self, df: pl.DataFrame) -> Optional[str]:
+        """
+        Detect grain key column using explicit scoring formula.
+
+        Prioritizes explicit keys (patient_id, subject_id, hadm_id) over
+        row-level IDs (event_id, row_id, uuid). Uses sampled uniqueness
+        to prevent expensive full table scans.
+
+        Scoring formula (on sample):
+        - -2.0 * uniq_ratio (penalize row-level IDs with uniq ~ 1.0)
+        - -1.0 * null_rate
+        - +1.0 if col ends with _id
+        - +2.0 if col is explicit key (patient_id, subject_id, etc.)
+        - -5.0 if col contains event/row/uuid/guid tokens
+
+        Args:
+            df: Polars DataFrame
+
+        Returns:
+            Grain key column name or None
+        """
+        # 1. Check explicit patient grain patterns first (highest priority)
+        patient_patterns = ['patient_id', 'subject_id', 'patientid', 'subjectid']
+        for col in df.columns:
+            if col.lower() in patient_patterns:
+                return col
+
+        # 2. Check explicit admission grain patterns
+        admission_patterns = ['hadm_id', 'encounter_id', 'visit_id', 'admissionid', 'encounterid']
+        for col in df.columns:
+            if col.lower() in admission_patterns:
+                return col
+
+        # 3. Fallback: score ID columns using sampled data
+        id_cols = [col for col in df.columns if self._is_probably_id_col(col)]
+
+        if not id_cols:
+            return None
+
+        # Sample for scoring
+        s = self._sample_df(df)
+
+        # Score each ID column
+        scores = {}
+        for col in id_cols:
+            # Compute sampled uniqueness
+            unique_count, null_rate = self._compute_sampled_uniqueness(df, col)
+
+            # Calculate uniqueness ratio on sample
+            non_null_count = max(s.height - int(null_rate * s.height), 1)
+            uniq_ratio = unique_count / non_null_count
+
+            # Apply scoring formula
+            score = 0.0
+            score -= 2.0 * uniq_ratio  # Penalize row-level IDs (uniq ~ 1.0)
+            score -= 1.0 * null_rate    # Penalize NULLs
+            score += 1.0 if self._is_probably_id_col(col) else 0.0
+            score += 2.0 if col.lower() in patient_patterns + admission_patterns else 0.0
+
+            # Hard penalty for row-level ID tokens
+            col_lower = col.lower()
+            if any(tok in col_lower for tok in ["event", "row", "uuid", "guid"]):
+                score -= 5.0
+
+            scores[col] = score
+
+            logger.debug(
+                f"Grain key candidate '{col}': score={score:.2f} "
+                f"(uniq_ratio={uniq_ratio:.2f}, null_rate={null_rate:.2f})"
+            )
+
+        # Return highest scoring column
+        if scores:
+            best_col = max(scores, key=scores.get)
+            logger.debug(f"Selected grain key: '{best_col}' (score={scores[best_col]:.2f})")
+            return best_col
+
+        return None
+
+    def _detect_grain_level(self, grain_key: str) -> Literal["patient", "admission", "event"]:
+        """
+        Detect grain level from grain key name.
+
+        Args:
+            grain_key: Name of grain key column
+
+        Returns:
+            Grain level (patient, admission, or event)
+        """
+        grain_key_lower = grain_key.lower()
+
+        if any(p in grain_key_lower for p in ['patient', 'subject']):
+            return "patient"
+        elif any(p in grain_key_lower for p in ['hadm', 'encounter', 'visit', 'admission']):
+            return "admission"
+        else:
+            return "event"
+
+    def _estimate_table_bytes(self, df: pl.DataFrame, sample_size: int = 1000) -> int:
+        """
+        Estimate total table size in bytes.
+
+        Strategy:
+        1. Sample up to sample_size rows
+        2. Calculate average bytes per row
+        3. Multiply by total rows
+
+        Args:
+            df: Polars DataFrame
+            sample_size: Number of rows to sample for estimation
+
+        Returns:
+            Estimated total bytes
+        """
+        if df.height == 0:
+            return 0
+
+        # Sample rows
+        sample = df.head(min(sample_size, df.height))
+
+        # Estimate bytes per row (rough approximation)
+        bytes_per_row = 0
+        for col in sample.columns:
+            dtype = sample[col].dtype
+
+            if dtype in [pl.Int8, pl.UInt8]:
+                bytes_per_row += 1
+            elif dtype in [pl.Int16, pl.UInt16]:
+                bytes_per_row += 2
+            elif dtype in [pl.Int32, pl.UInt32, pl.Float32]:
+                bytes_per_row += 4
+            elif dtype in [pl.Int64, pl.UInt64, pl.Float64]:
+                bytes_per_row += 8
+            elif dtype == pl.Utf8:
+                # Estimate string column bytes
+                avg_str_len = sample[col].drop_nulls().str.len_chars().mean() or 0
+                bytes_per_row += int(avg_str_len)
+            elif dtype == pl.Boolean:
+                bytes_per_row += 1
+            else:
+                # Default estimate for unknown types
+                bytes_per_row += 8
+
+        # Total estimate
+        return bytes_per_row * df.height
+
+    def _detect_time_column(self, df: pl.DataFrame) -> Tuple[Optional[str], bool]:
+        """
+        Detect time column in DataFrame using sampled uniqueness.
+
+        A valid time column must:
+        1. Have datetime/date type OR name contains time/date patterns
+        2. Not be constant (all same value in sample)
+
+        Note: Removed 'dt' pattern to avoid false positives (dt_code, mdt_flag).
+        Uses sampled uniqueness to prevent expensive full table scans.
+
+        Args:
+            df: Polars DataFrame
+
+        Returns:
+            Tuple of (column_name, has_valid_time_column)
+        """
+        time_patterns = ['time', 'date', 'timestamp', 'datetime']
+
+        # Prioritize dtype check first (more reliable)
+        for col in df.columns:
+            # Check dtype first
+            is_time_type = df[col].dtype in [pl.Datetime, pl.Date]
+
+            if is_time_type:
+                # Verify not constant using sampled data
+                unique_count, _ = self._compute_sampled_uniqueness(df, col)
+                if unique_count > 1:
+                    return (col, True)
+
+        # Fallback: check name patterns
+        for col in df.columns:
+            col_lower = col.lower()
+            has_time_pattern = any(p in col_lower for p in time_patterns)
+
+            if has_time_pattern:
+                # Verify not constant using sampled data
+                unique_count, _ = self._compute_sampled_uniqueness(df, col)
+                if unique_count > 1:
+                    return (col, True)
+
+        return (None, False)
+
+    def _count_foreign_keys(self) -> Dict[str, int]:
+        """
+        Count number of foreign key relationships per table.
+
+        Returns:
+            Dict mapping table name to number of FKs
+        """
+        fk_counts: Dict[str, int] = {}
+
+        for rel in self.relationships:
+            # Child table has a foreign key
+            fk_counts[rel.child_table] = fk_counts.get(rel.child_table, 0) + 1
+
+        return fk_counts
+
+    def _detect_bridge_table(
+        self,
+        table_name: str,
+        df: pl.DataFrame,
+        fk_count: int
+    ) -> bool:
+        """
+        Detect if table is a bridge (many-to-many) table using sampled data.
+
+        Bridge table characteristics:
+        1. Two or more foreign keys to different parent tables
+        2. Neither FK is unique individually
+        3. Composite key (fk1, fk2) is near-unique (>95% unique on sample)
+        4. Narrow table (width < 15 columns)
+
+        Uses sampled composite uniqueness to prevent expensive full table scans.
+
+        Args:
+            table_name: Name of table
+            df: Polars DataFrame
+            fk_count: Number of foreign keys
+
+        Returns:
+            True if bridge table detected
+        """
+        # Structural heuristic: bridges are narrow (< 15 columns)
+        if df.width >= 15:
+            return False
+
+        # Must have 2+ foreign keys
+        if fk_count < 2:
+            return False
+
+        # Get FK columns for this table
+        fk_cols = [
+            rel.child_key for rel in self.relationships
+            if rel.child_table == table_name
+        ]
+
+        if len(fk_cols) < 2:
+            return False
+
+        # Sample for uniqueness checks
+        s = self._sample_df(df)
+
+        # Check if individual FKs are non-unique (on sample)
+        fk_unique_flags = []
+        for col in fk_cols:
+            if col in s.columns:
+                unique_count, _ = self._compute_sampled_uniqueness(df, col)
+                is_unique = (unique_count == s.height)
+                fk_unique_flags.append(is_unique)
+
+        if all(fk_unique_flags):
+            # If all FKs are unique, not a bridge
+            return False
+
+        # Check if composite key is near-unique (on sample)
+        try:
+            # Use first two FK columns for composite check
+            fk1, fk2 = fk_cols[0], fk_cols[1]
+            if fk1 in s.columns and fk2 in s.columns:
+                composite_unique = s.select([fk1, fk2]).n_unique()
+                composite_ratio = composite_unique / s.height if s.height > 0 else 0.0
+
+                # Near-unique threshold: 95%
+                is_composite_unique = composite_ratio > 0.95
+
+                return is_composite_unique
+
+        except Exception as e:
+            logger.debug(f"Error detecting bridge for {table_name}: {e}")
+            return False
+
+        return False
+
+    def _is_n_side_of_anchor(self, table_name: str, anchor_table: Optional[str]) -> bool:
+        """
+        Check if table is on N-side of relationship to anchor.
+
+        Args:
+            table_name: Table to check
+            anchor_table: Anchor table name
+
+        Returns:
+            True if table is child (N-side) of anchor in relationship
+        """
+        if not anchor_table:
+            return False
+
+        for rel in self.relationships:
+            if rel.parent_table == anchor_table and rel.child_table == table_name:
+                return True
+
+        return False
+
+    def _classify_table_type(
+        self,
+        cardinality_ratio: float,
+        is_unique: bool,
+        estimated_bytes: int,
+        has_time: bool,
+        is_bridge: bool,
+        is_n_side: bool,
+        time_col: Optional[str],
+        df: pl.DataFrame
+    ) -> Literal["dimension", "fact", "event", "bridge", "reference"]:
+        """
+        Classify table based on characteristics.
+
+        Classification priority (first match wins):
+        1. Bridge: detected as bridge table
+        2. Reference: small size (<10 MB) with duplicates allowed
+        3. Event: has time column, high cardinality, N-side of anchor
+        4. Dimension: low cardinality, unique on grain, size < max_dimension_bytes
+        5. Fact: high cardinality, not unique
+
+        Args:
+            cardinality_ratio: rows / unique(grain_key)
+            is_unique: True if unique on grain
+            estimated_bytes: Table size in bytes
+            has_time: True if has time column
+            is_bridge: True if bridge table
+            is_n_side: True if N-side of anchor
+            time_col: Time column name
+            df: DataFrame
+
+        Returns:
+            Classification type
+        """
+        # Priority 1: Bridge
+        if is_bridge:
+            return "bridge"
+
+        # Priority 2: Reference (small lookup tables)
+        if estimated_bytes < 10_000_000 and not is_unique:  # < 10 MB
+            return "reference"
+
+        # Priority 3: Event (time-series facts)
+        if has_time and cardinality_ratio > 1.1 and is_n_side:
+            # Verify time column is not constant
+            if time_col and df[time_col].n_unique() > 1:
+                return "event"
+
+        # Priority 4: Dimension
+        if cardinality_ratio <= 1.1 and is_unique and estimated_bytes < self.max_dimension_bytes:
+            return "dimension"
+
+        # Priority 5: Fact (default for high cardinality non-events)
+        return "fact"
+
+    def _build_dimension_mart(
+        self,
+        anchor_table: Optional[str] = None,
+        join_type: str = "left"
+    ) -> pl.LazyFrame:
+        """
+        Build dimension mart by joining only 1:1 and small dimensions to anchor.
+
+        Critical invariants enforced:
+        1. Mart rowcount equals anchor unique grain count
+        2. No joins where RHS key is non-unique
+        3. Bridges, facts, and events are EXCLUDED
+
+        Strategy:
+        1. Find anchor table using centrality-based selection
+        2. Filter to only dimension tables (exclude fact/event/bridge)
+        3. Verify RHS join keys are unique (enforce 1:1 or many:1)
+        4. Execute joins in BFS order from anchor
+        5. Return LazyFrame (not materialized)
+
+        Args:
+            anchor_table: Optional anchor table name (auto-detected if None)
+            join_type: Type of join (default "left")
+
+        Returns:
+            LazyFrame with dimension mart (anchor + joined dimensions)
+
+        Raises:
+            ValueError: If anchor has non-unique grain key or no dimensions found
+        """
+        logger.info("Building dimension mart with aggregate-before-join architecture")
+
+        # Ensure classifications exist
+        if not self.classifications:
+            self.classify_tables()
+
+        # Auto-detect anchor if not specified
+        if anchor_table is None:
+            anchor_table = self._find_anchor_by_centrality()
+            logger.info(f"Auto-detected anchor table: {anchor_table}")
+
+        if anchor_table not in self.tables:
+            raise ValueError(f"Anchor table '{anchor_table}' not found in tables")
+
+        anchor_class = self.classifications.get(anchor_table)
+        if not anchor_class:
+            raise ValueError(f"Anchor table '{anchor_table}' not classified")
+
+        # Verify anchor is unique on grain
+        if not anchor_class.is_unique_on_grain:
+            raise ValueError(
+                f"Anchor table '{anchor_table}' is not unique on grain key "
+                f"'{anchor_class.grain_key}' (cardinality_ratio={anchor_class.cardinality_ratio:.2f})"
+            )
+
+        # Start with anchor table as LazyFrame
+        anchor_df = self.tables[anchor_table]
+        mart = anchor_df.lazy()
+        joined_tables = {anchor_table}
+
+        logger.debug(
+            f"Anchor '{anchor_table}': {anchor_df.height} rows, "
+            f"grain_key={anchor_class.grain_key}"
+        )
+
+        # Build join graph using BFS from anchor
+        queue = [anchor_table]
+        join_count = 0
+
+        while queue:
+            current = queue.pop(0)
+
+            # Find relationships where current table is involved
+            for rel in self.relationships:
+                next_table = None
+                join_key_left = None
+                join_key_right = None
+
+                # Check if we can join a new table to current table
+                if rel.parent_table == current and rel.child_table not in joined_tables:
+                    next_table = rel.child_table
+                    join_key_left = rel.parent_key
+                    join_key_right = rel.child_key
+
+                elif rel.child_table == current and rel.parent_table not in joined_tables:
+                    next_table = rel.parent_table
+                    join_key_left = rel.child_key
+                    join_key_right = rel.parent_key
+
+                if next_table:
+                    # Check if next_table is a dimension (exclude facts, events, bridges)
+                    next_class = self.classifications.get(next_table)
+
+                    if not next_class:
+                        logger.debug(f"Skipping '{next_table}': not classified")
+                        continue
+
+                    if next_class.classification != "dimension":
+                        logger.debug(
+                            f"Skipping '{next_table}': classification={next_class.classification} "
+                            f"(only dimensions allowed in mart)"
+                        )
+                        continue
+
+                    # CRITICAL: Verify RHS join key is unique (enforce 1:1 or many:1)
+                    next_df = self.tables[next_table]
+
+                    # Use sampled uniqueness to check if RHS key is unique
+                    rhs_unique_count, rhs_null_rate = self._compute_sampled_uniqueness(
+                        next_df, join_key_right
+                    )
+                    s = self._sample_df(next_df)
+                    non_null_sample = s.height - int(rhs_null_rate * s.height)
+
+                    if non_null_sample > 0:
+                        rhs_uniq_ratio = rhs_unique_count / non_null_sample
+                    else:
+                        rhs_uniq_ratio = 0.0
+
+                    # Allow 5% tolerance for sampling error
+                    if rhs_uniq_ratio < 0.95:
+                        logger.warning(
+                            f"Skipping join to '{next_table}': RHS key '{join_key_right}' "
+                            f"is not unique (uniq_ratio={rhs_uniq_ratio:.2%}, "
+                            f"violates 1:1 or many:1 constraint)"
+                        )
+                        continue
+
+                    # Join next dimension table
+                    next_lazy = next_df.lazy()
+
+                    mart = mart.join(
+                        next_lazy,
+                        left_on=join_key_left,
+                        right_on=join_key_right,
+                        how=join_type,
+                        suffix=f"_{next_table}"
+                    )
+
+                    joined_tables.add(next_table)
+                    queue.append(next_table)
+                    join_count += 1
+
+                    logger.debug(
+                        f"Joined dimension '{next_table}' on {join_key_left}={join_key_right} "
+                        f"(rhs_uniq_ratio={rhs_uniq_ratio:.2%})"
+                    )
+
+        logger.info(
+            f"Dimension mart built: anchor='{anchor_table}', "
+            f"joined_dimensions={join_count}, total_tables={len(joined_tables)}"
+        )
+
+        return mart
+
+    def build_unified_cohort(
+        self,
+        anchor_table: Optional[str] = None,
+        join_type: str = "left"
+    ) -> pl.DataFrame:
+        """
+        Join all related tables into unified cohort view using DuckDB.
+
+        Strategy:
+        1. Find anchor table (root of join graph)
+        2. Build join graph using BFS from anchor
+        3. Execute joins in correct order
+        4. Handle name collisions with suffixes
+
+        Args:
+            anchor_table: Root table for joins (auto-detected if None)
+            join_type: Type of SQL join (left, inner, outer)
+
+        Returns:
+            Unified Polars DataFrame with all columns
+
+        Example:
+            >>> cohort = handler.build_unified_cohort(anchor_table='patients')
+        """
+        logger.info(f"Building unified cohort (anchor_table={anchor_table}, join_type={join_type})")
+        
+        # Auto-detect anchor table if not specified
+        if anchor_table is None:
+            anchor_table = self._find_anchor_table()
+            logger.info(f"Auto-detected anchor table: {anchor_table}")
+
+        if anchor_table not in self.tables:
+            raise ValueError(f"Anchor table '{anchor_table}' not found in tables")
+        
+        logger.debug(f"Anchor table '{anchor_table}' has {self.tables[anchor_table].height} rows, {self.tables[anchor_table].width} columns")
+
+        # Build join graph using BFS
+        joined_tables = {anchor_table}
+        join_clauses = []
+
+        # Start with anchor table
+        current_table_alias = anchor_table
+
+        # BFS to find join order
+        queue = [anchor_table]
+
+        while queue:
+            current = queue.pop(0)
+
+            # Find relationships where current table is parent or child
+            for rel in self.relationships:
+                next_table = None
+                join_on = None
+
+                if rel.parent_table == current and rel.child_table not in joined_tables:
+                    # Join child to parent
+                    next_table = rel.child_table
+                    join_on = f"{current}.{rel.parent_key} = {next_table}.{rel.child_key}"
+
+                elif rel.child_table == current and rel.parent_table not in joined_tables:
+                    # Join parent to child (reverse relationship)
+                    next_table = rel.parent_table
+                    join_on = f"{current}.{rel.child_key} = {next_table}.{rel.parent_key}"
+
+                if next_table:
+                    join_clauses.append((next_table, join_on, join_type))
+                    joined_tables.add(next_table)
+                    queue.append(next_table)
+
+        # Build SQL query for joins
+        if not join_clauses:
+            # No joins possible, return anchor table
+            logger.debug(f"No joins possible, returning anchor table: {anchor_table}")
+            return self.tables[anchor_table]
+
+        # Construct SQL query
+        query = f"SELECT * FROM {anchor_table}"
+
+        for table, on_clause, jtype in join_clauses:
+            query += f" {jtype.upper()} JOIN {table} ON {on_clause}"
+
+        logger.debug(f"Executing SQL query with {len(join_clauses)} joins")
+        logger.debug(f"Query: {query[:500]}...")  # Log first 500 chars of query
+
+        # Execute query using DuckDB
+        try:
+            result = self.conn.execute(query).pl()
+            logger.debug(f"Query executed successfully: {result.height} rows, {result.width} columns")
+            return result
+        except Exception as e:
+            logger.error(f"Error executing SQL query: {type(e).__name__}: {str(e)}")
+            logger.error(f"Full query: {query}")
+            import traceback
+            logger.error(f"Traceback: {traceback.format_exc()}")
+            raise
+
+    def _detect_primary_key(self, df: pl.DataFrame) -> Optional[str]:
+        """
+        Detect primary key column in a table using Polars.
+
+        Strategy:
+        1. Column must be 100% unique (no duplicates)
+        2. Column must have no null values
+        3. Prefer columns with "id" in name
+
+        Args:
+            df: Polars DataFrame
+
+        Returns:
+            Primary key column name or None
+        """
+        id_pattern_cols = [col for col in df.columns if 'id' in col.lower()]
+
+        # Check ID pattern columns first
+        for col in id_pattern_cols:
+            if df[col].n_unique() == df.height and df[col].null_count() == 0:
+                return col
+
+        # Fallback: check all columns
+        for col in df.columns:
+            if df[col].n_unique() == df.height and df[col].null_count() == 0:
+                return col
+
+        return None
+
+    def _is_foreign_key_candidate(self, parent_key: str, child_col: str) -> bool:
+        """
+        Check if column name suggests foreign key relationship.
+
+        Strategy:
+        - Exact match (case-insensitive)
+        - Common FK naming patterns (parent_key_id, fk_parent_key, etc.)
+
+        Args:
+            parent_key: Parent table's primary key name
+            child_col: Candidate foreign key column name
+
+        Returns:
+            True if naming suggests FK relationship
+        """
+        parent_lower = parent_key.lower()
+        child_lower = child_col.lower()
+
+        # Exact match
+        if parent_lower == child_lower:
+            return True
+
+        # Common FK patterns
+        if child_lower in [
+            parent_lower,
+            f"{parent_lower}_id",
+            f"fk_{parent_lower}",
+            f"{parent_lower}_fk",
+        ]:
+            return True
+
+        # Handle cases like "patient_id" matching "patientid"
+        parent_no_underscore = parent_lower.replace('_', '')
+        child_no_underscore = child_lower.replace('_', '')
+
+        if parent_no_underscore == child_no_underscore:
+            return True
+
+        return False
+
+    def _verify_referential_integrity(
+        self,
+        parent_df: pl.DataFrame,
+        parent_key: str,
+        child_df: pl.DataFrame,
+        child_col: str
+    ) -> float:
+        """
+        Verify referential integrity using Polars operations.
+
+        Calculates what percentage of child FK values exist in parent PK.
+
+        Args:
+            parent_df: Parent table DataFrame
+            parent_key: Parent primary key column
+            child_df: Child table DataFrame
+            child_col: Child foreign key column
+
+        Returns:
+            Match ratio (0-1) representing referential integrity
+        """
+        logger.debug(f"Verifying referential integrity: {parent_key} -> {child_col}")
+        
+        # Get non-null child values
+        child_values = child_df[child_col].drop_nulls()
+        logger.debug(f"Child values dtype: {child_values.dtype}, len: {child_values.len()}, sample: {child_values.head(3).to_list()}")
+
+        if child_values.len() == 0:
+            logger.debug("Child values empty, returning 0.0")
+            return 0.0
+
+        # Get parent values
+        parent_values = parent_df[parent_key].drop_nulls()
+        logger.debug(f"Parent values dtype: {parent_values.dtype}, len: {parent_values.len()}, sample: {parent_values.head(3).to_list()}")
+
+        # Cast both to string for type-safe comparison using pure Polars operations
+        # This handles cases where one table has int64 and another has string
+        try:
+            # Cast both to Utf8 for consistent comparison
+            logger.debug(f"Casting child from {child_values.dtype} to Utf8")
+            child_str = child_values.cast(pl.Utf8, strict=False).drop_nulls().unique()
+            logger.debug(f"Child after cast: dtype={child_str.dtype}, len={child_str.len()}, sample={child_str.head(3).to_list()}")
+            
+            logger.debug(f"Casting parent from {parent_values.dtype} to Utf8")
+            parent_str = parent_values.cast(pl.Utf8, strict=False).drop_nulls().unique()
+            logger.debug(f"Parent after cast: dtype={parent_str.dtype}, len={parent_str.len()}, sample={parent_str.head(3).to_list()}")
+
+            # Verify both are Utf8 before join
+            if child_str.dtype != pl.Utf8:
+                logger.warning(f"Child cast failed: expected Utf8, got {child_str.dtype}, recasting...")
+                child_str = child_str.cast(pl.Utf8, strict=False)
+            if parent_str.dtype != pl.Utf8:
+                logger.warning(f"Parent cast failed: expected Utf8, got {parent_str.dtype}, recasting...")
+                parent_str = parent_str.cast(pl.Utf8, strict=False)
+
+            # Use join-based approach to avoid is_in() deprecation warning
+            # Create DataFrames for join operation
+            logger.debug("Creating DataFrames for join operation")
+            child_df_join = pl.DataFrame({"value": child_str})
+            parent_df_join = pl.DataFrame({"value": parent_str})
+            
+            logger.debug(f"Child DF schema: {child_df_join.schema}, Parent DF schema: {parent_df_join.schema}")
+            
+            # Inner join to find matches
+            logger.debug("Performing inner join to find matches")
+            matches_df = child_df_join.join(parent_df_join, on="value", how="inner")
+            matches = matches_df.height
+            logger.debug(f"Found {matches} matches out of {child_str.len()} child values")
+
+            ratio = float(matches) / float(child_str.len()) if child_str.len() > 0 else 0.0
+            logger.debug(f"Match ratio: {ratio:.4f}")
+            return ratio
+
+        except Exception as e:
+            # If casting fails, skip this relationship
+            logger.error(f"Error in referential integrity check ({parent_key} -> {child_col}): {type(e).__name__}: {str(e)}")
+            logger.error(f"Child dtype: {child_values.dtype}, Parent dtype: {parent_values.dtype}")
+            import traceback
+            logger.error(f"Traceback: {traceback.format_exc()}")
+            return 0.0
+
+    def _find_anchor_table(self) -> str:
+        """
+        Find anchor table (most central in join graph).
+
+        DEPRECATED: Use _find_anchor_by_centrality() instead.
+
+        Strategy:
+        - Table with most relationships (parent or child)
+        - Prefer tables with "patient" or "subject" in name
+
+        Returns:
+            Anchor table name
+        """
+        logger.warning(
+            "_find_anchor_table() is deprecated, use _find_anchor_by_centrality() instead"
+        )
+
+        # Count relationships for each table
+        table_counts = {}
+
+        for rel in self.relationships:
+            table_counts[rel.parent_table] = table_counts.get(rel.parent_table, 0) + 1
+            table_counts[rel.child_table] = table_counts.get(rel.child_table, 0) + 1
+
+        # Prefer tables with "patient" or "subject" in name
+        patient_tables = [
+            t for t in self.tables.keys()
+            if 'patient' in t.lower() or 'subject' in t.lower()
+        ]
+
+        if patient_tables:
+            # Pick patient table with most relationships
+            return max(patient_tables, key=lambda t: table_counts.get(t, 0))
+
+        # Fallback: table with most relationships
+        if table_counts:
+            return max(table_counts, key=table_counts.get)
+
+        # Last resort: first table
+        return list(self.tables.keys())[0]
+
+    def _find_anchor_by_centrality(self) -> str:
+        """
+        Find anchor table using graph centrality with hard exclusions and tie-breakers.
+
+        Hard Exclusions (never anchor on):
+        - Classification in {event, fact, bridge}
+        - Tables without unique grain key
+        - Tables with >50% NULLs in grain key
+
+        Scoring Rules:
+        - +10 if has hadm_id or encounter_id column
+        - +5 if has patient_id or subject_id column
+        - +1 per relationship (incoming + outgoing)
+        - +3 if classified as dimension with patient grain
+
+        Tie-breakers (deterministic, in order):
+        1. Prefer fewer NULLs in grain key (lower null_rate)
+        2. Prefer unique grain key (is_unique_on_grain = True)
+        3. Prefer smaller estimated_bytes
+        4. Prefer patient grain over admission grain
+
+        Returns:
+            Anchor table name
+
+        Raises:
+            ValueError: If no suitable anchor table found
+        """
+        # Ensure classifications exist
+        if not self.classifications:
+            self.classify_tables()
+
+        # Hard exclusion: filter to only dimensions
+        dimension_tables = {
+            name: cls for name, cls in self.classifications.items()
+            if cls.classification == "dimension"
+        }
+
+        if not dimension_tables:
+            raise ValueError(
+                "No dimension tables found for anchor selection. "
+                "All tables are classified as fact/event/bridge/reference."
+            )
+
+        # Hard exclusion: filter out tables with >50% NULLs or non-unique grain
+        candidates = {}
+        for name, cls in dimension_tables.items():
+            if cls.null_rate_in_grain > 0.5:
+                logger.debug(f"Excluding {name} from anchor candidates: null_rate={cls.null_rate_in_grain:.2%}")
+                continue
+
+            if not cls.is_unique_on_grain:
+                logger.debug(f"Excluding {name} from anchor candidates: not unique on grain")
+                continue
+
+            candidates[name] = cls
+
+        if not candidates:
+            raise ValueError(
+                "No suitable anchor table found. All dimensions have >50% NULLs "
+                "or non-unique grain keys."
+            )
+
+        # Count relationships per table
+        relationship_counts = {}
+        for rel in self.relationships:
+            relationship_counts[rel.parent_table] = relationship_counts.get(rel.parent_table, 0) + 1
+            relationship_counts[rel.child_table] = relationship_counts.get(rel.child_table, 0) + 1
+
+        # Score each candidate
+        scores = {}
+        for name, cls in candidates.items():
+            score = 0
+
+            # Check for key columns
+            df = self.tables[name]
+            col_lower = [c.lower() for c in df.columns]
+
+            if any(p in col_lower for p in ['hadm_id', 'encounter_id']):
+                score += 10
+
+            if any(p in col_lower for p in ['patient_id', 'subject_id']):
+                score += 5
+
+            # Relationship count
+            score += relationship_counts.get(name, 0)
+
+            # Bonus for dimension with patient grain
+            if cls.grain == "patient":
+                score += 3
+
+            scores[name] = score
+
+        # Find max score
+        max_score = max(scores.values())
+        top_candidates = [name for name, score in scores.items() if score == max_score]
+
+        # If single winner, return it
+        if len(top_candidates) == 1:
+            winner = top_candidates[0]
+            logger.info(
+                f"Selected anchor table '{winner}' (score={max_score}, "
+                f"grain={candidates[winner].grain}, "
+                f"null_rate={candidates[winner].null_rate_in_grain:.2%})"
+            )
+            return winner
+
+        # Apply tie-breakers
+        logger.debug(f"Tie detected among {len(top_candidates)} tables, applying tie-breakers")
+
+        # Tie-breaker 1: Lower null rate
+        min_null_rate = min(candidates[name].null_rate_in_grain for name in top_candidates)
+        top_candidates = [
+            name for name in top_candidates
+            if candidates[name].null_rate_in_grain == min_null_rate
+        ]
+
+        if len(top_candidates) == 1:
+            winner = top_candidates[0]
+            logger.info(
+                f"Selected anchor table '{winner}' (tie-breaker: null_rate={min_null_rate:.2%})"
+            )
+            return winner
+
+        # Tie-breaker 2: Unique on grain (should all be True at this point, but check anyway)
+        unique_candidates = [
+            name for name in top_candidates
+            if candidates[name].is_unique_on_grain
+        ]
+
+        if unique_candidates:
+            top_candidates = unique_candidates
+
+        if len(top_candidates) == 1:
+            winner = top_candidates[0]
+            logger.info(f"Selected anchor table '{winner}' (tie-breaker: unique on grain)")
+            return winner
+
+        # Tie-breaker 3: Smaller estimated bytes
+        min_bytes = min(candidates[name].estimated_bytes for name in top_candidates)
+        top_candidates = [
+            name for name in top_candidates
+            if candidates[name].estimated_bytes == min_bytes
+        ]
+
+        if len(top_candidates) == 1:
+            winner = top_candidates[0]
+            logger.info(
+                f"Selected anchor table '{winner}' (tie-breaker: bytes={min_bytes:,})"
+            )
+            return winner
+
+        # Tie-breaker 4: Patient grain over admission grain
+        patient_grain_candidates = [
+            name for name in top_candidates
+            if candidates[name].grain == "patient"
+        ]
+
+        if patient_grain_candidates:
+            top_candidates = patient_grain_candidates
+
+        # Final tie-breaker: alphabetical order (deterministic)
+        winner = sorted(top_candidates)[0]
+        logger.info(
+            f"Selected anchor table '{winner}' (tie-breaker: alphabetical, "
+            f"grain={candidates[winner].grain})"
+        )
+
+        return winner
+
+    def get_relationship_summary(self) -> str:
+        """Generate human-readable summary of detected relationships."""
+        lines = ["=== Detected Table Relationships ==="]
+
+        if not self.relationships:
+            lines.append("No relationships detected")
+            return "\n".join(lines)
+
+        for rel in self.relationships:
+            lines.append(f"  {rel}")
+
+        lines.append(f"\nTotal: {len(self.relationships)} relationships")
+
+        return "\n".join(lines)
+
+    def close(self):
+        """Close DuckDB connection."""
+        if self.conn:
+            self.conn.close()
+
+    def __del__(self):
+        """Cleanup DuckDB connection on deletion."""
+        self.close()
diff --git a/src/clinical_analytics/core/registry.py b/src/clinical_analytics/core/registry.py
index f64f75c..4dbeee6 100644
--- a/src/clinical_analytics/core/registry.py
+++ b/src/clinical_analytics/core/registry.py
@@ -9,10 +9,12 @@ import importlib
 import inspect
 import pkgutil
 from pathlib import Path
-from typing import Dict, Type, Optional, List
+from typing import Dict, Type, Optional, List, Any
 import yaml
+import polars as pl
 
 from clinical_analytics.core.dataset import ClinicalDataset
+from clinical_analytics.core.schema_inference import SchemaInferenceEngine
 
 
 class DatasetRegistry:
@@ -25,6 +27,7 @@ class DatasetRegistry:
     _datasets: Dict[str, Type[ClinicalDataset]] = {}
     _configs: Dict[str, dict] = {}
     _config_loaded: bool = False
+    _auto_inferred: Dict[str, pl.DataFrame] = {}  # Store DataFrames for auto-inferred datasets
 
     @classmethod
     def discover_datasets(cls) -> Dict[str, Type[ClinicalDataset]]:
@@ -185,9 +188,89 @@ class DatasetRegistry:
 
         return {name: cls.get_dataset_info(name) for name in cls._datasets.keys()}
 
+    @classmethod
+    def register_from_dataframe(
+        cls,
+        dataset_name: str,
+        df: pl.DataFrame,
+        display_name: Optional[str] = None,
+        infer_schema: bool = True
+    ) -> Dict[str, Any]:
+        """
+        Register dataset from Polars DataFrame with automatic schema inference.
+
+        This eliminates the need for manual YAML configuration files.
+        Schema is automatically detected using SchemaInferenceEngine.
+
+        Args:
+            dataset_name: Unique identifier for this dataset
+            df: Polars DataFrame with raw data
+            display_name: Human-readable name (defaults to dataset_name)
+            infer_schema: Whether to auto-infer schema (default: True)
+
+        Returns:
+            Generated config dictionary
+
+        Example:
+            >>> df = pl.read_csv("patient_data.csv")
+            >>> config = DatasetRegistry.register_from_dataframe(
+            ...     "my_study",
+            ...     df,
+            ...     display_name="My Clinical Study"
+            ... )
+            >>> print(f"Detected patient ID: {config['column_mapping']}")
+
+        Note:
+            For uploaded datasets, this is called automatically by UserDatasetStorage.
+            For built-in datasets, YAML configs can be replaced with this method.
+        """
+        # Infer schema from DataFrame
+        if infer_schema:
+            engine = SchemaInferenceEngine()
+            schema = engine.infer_schema(df)
+            config = schema.to_dataset_config()
+        else:
+            config = {
+                'column_mapping': {},
+                'outcomes': {},
+                'time_zero': {}
+            }
+
+        # Add metadata
+        config['name'] = dataset_name
+        config['display_name'] = display_name or dataset_name
+        config['status'] = 'auto-inferred' if infer_schema else 'manual'
+        config['row_count'] = df.height
+        config['column_count'] = df.width
+
+        # Store config
+        cls._configs[dataset_name] = config
+
+        # Store DataFrame for later retrieval
+        cls._auto_inferred[dataset_name] = df
+
+        # Mark config as loaded
+        cls._config_loaded = True
+
+        return config
+
+    @classmethod
+    def get_auto_inferred_dataframe(cls, dataset_name: str) -> Optional[pl.DataFrame]:
+        """
+        Retrieve Polars DataFrame for an auto-inferred dataset.
+
+        Args:
+            dataset_name: Dataset identifier
+
+        Returns:
+            Polars DataFrame or None if not found
+        """
+        return cls._auto_inferred.get(dataset_name)
+
     @classmethod
     def reset(cls) -> None:
         """Reset registry (mainly for testing)."""
         cls._datasets = {}
         cls._configs = {}
         cls._config_loaded = False
+        cls._auto_inferred = {}
diff --git a/src/clinical_analytics/core/schema_inference.py b/src/clinical_analytics/core/schema_inference.py
new file mode 100644
index 0000000..0f55a39
--- /dev/null
+++ b/src/clinical_analytics/core/schema_inference.py
@@ -0,0 +1,590 @@
+"""
+Automatic Schema Inference Engine - Zero Manual Configuration
+
+This module implements intelligent schema detection for clinical datasets using
+only Polars and DuckDB operations (no pandas). Automatically detects:
+- Patient ID columns (unique identifiers)
+- Outcome variables (binary clinical endpoints)
+- Time variables (dates, survival time)
+- Event indicators (censoring status)
+- Categorical vs continuous variables
+
+Key Principles:
+- Use Polars for all DataFrame operations
+- Use DuckDB for SQL-based analysis
+- Privacy-preserving (all local, no API calls)
+- Fail gracefully with sensible defaults
+"""
+
+from dataclasses import dataclass, field
+from typing import Dict, Any, List, Optional, Set
+from pathlib import Path
+import polars as pl
+import duckdb
+import re
+
+
+@dataclass
+class DictionaryMetadata:
+    """
+    Metadata extracted from data dictionary PDF.
+
+    This provides ground truth context about column meanings,
+    valid values, and clinical significance.
+
+    Attributes:
+        column_descriptions: Dict mapping column names to descriptions
+        column_types: Dict mapping column names to expected data types
+        valid_values: Dict mapping column names to valid value sets
+        source_file: Path to the PDF dictionary
+    """
+    column_descriptions: Dict[str, str] = field(default_factory=dict)
+    column_types: Dict[str, str] = field(default_factory=dict)
+    valid_values: Dict[str, List[str]] = field(default_factory=dict)
+    source_file: Optional[Path] = None
+
+    def get_description(self, column: str) -> Optional[str]:
+        """Get description for a column, case-insensitive."""
+        col_lower = column.lower()
+        for key, value in self.column_descriptions.items():
+            if key.lower() == col_lower:
+                return value
+        return None
+
+
+@dataclass
+class InferredSchema:
+    """
+    Complete inferred schema for a clinical dataset.
+
+    This represents the automatically detected structure of a dataset,
+    eliminating the need for manual YAML configuration files.
+
+    Attributes:
+        patient_id_column: Unique identifier column (e.g., 'patient_id', 'subject_id')
+        time_zero: Reference time column or static value
+        outcome_columns: Binary outcome variables (mortality, ICU, etc.)
+        time_columns: Time-related columns (dates, survival time)
+        event_columns: Event indicator columns (censoring status)
+        categorical_columns: Categorical variables
+        continuous_columns: Continuous numeric variables
+        confidence_scores: Detection confidence for each field (0-1)
+    """
+    patient_id_column: Optional[str] = None
+    time_zero: Optional[str] = None
+    outcome_columns: List[str] = field(default_factory=list)
+    time_columns: List[str] = field(default_factory=list)
+    event_columns: List[str] = field(default_factory=list)
+    categorical_columns: List[str] = field(default_factory=list)
+    continuous_columns: List[str] = field(default_factory=list)
+    confidence_scores: Dict[str, float] = field(default_factory=dict)
+    dictionary_metadata: Optional[DictionaryMetadata] = None
+
+    def to_dataset_config(self) -> Dict[str, Any]:
+        """
+        Convert inferred schema to dataset config format.
+
+        This generates the config dict that would have been manually written
+        in YAML files, enabling seamless integration with existing systems.
+
+        Returns:
+            Config dictionary compatible with ClinicalDataset
+        """
+        config = {
+            'column_mapping': {},
+            'outcomes': {},
+            'time_zero': {}
+        }
+
+        # Map patient ID
+        if self.patient_id_column:
+            config['column_mapping'][self.patient_id_column] = 'patient_id'
+
+        # Map outcomes with descriptions from dictionary
+        for outcome_col in self.outcome_columns:
+            outcome_config = {
+                'source_column': outcome_col,
+                'type': 'binary',
+                'confidence': self.confidence_scores.get(f'outcome_{outcome_col}', 0.5)
+            }
+
+            # Add description from dictionary if available
+            if self.dictionary_metadata:
+                desc = self.dictionary_metadata.get_description(outcome_col)
+                if desc:
+                    outcome_config['description'] = desc
+
+            config['outcomes'][outcome_col] = outcome_config
+
+        # Time zero
+        if self.time_zero:
+            if self.time_zero in self.time_columns:
+                config['time_zero'] = {'source_column': self.time_zero}
+            else:
+                config['time_zero'] = {'value': self.time_zero}
+
+        return config
+
+    def summary(self) -> str:
+        """Generate human-readable summary of inferred schema."""
+        lines = ["=== Inferred Schema Summary ==="]
+
+        if self.patient_id_column:
+            conf = self.confidence_scores.get('patient_id', 0.0)
+            lines.append(f"Patient ID: {self.patient_id_column} (confidence: {conf:.2f})")
+
+        if self.outcome_columns:
+            lines.append(f"Outcomes ({len(self.outcome_columns)}): {', '.join(self.outcome_columns)}")
+
+        if self.time_columns:
+            lines.append(f"Time columns ({len(self.time_columns)}): {', '.join(self.time_columns)}")
+
+        if self.event_columns:
+            lines.append(f"Event columns ({len(self.event_columns)}): {', '.join(self.event_columns)}")
+
+        lines.append(f"Categorical: {len(self.categorical_columns)} columns")
+        lines.append(f"Continuous: {len(self.continuous_columns)} columns")
+
+        return "\n".join(lines)
+
+
+class SchemaInferenceEngine:
+    """
+    Automatic schema inference for any tabular clinical dataset.
+
+    Replaces hardcoded YAML configs with intelligent pattern recognition
+    using Polars and DuckDB operations only.
+
+    Detection Strategies:
+    1. Pattern matching on column names
+    2. Statistical analysis of value distributions
+    3. Cardinality-based classification
+    4. Data type analysis
+
+    Example:
+        >>> engine = SchemaInferenceEngine()
+        >>> df = pl.read_csv("patient_data.csv")
+        >>> schema = engine.infer_schema(df)
+        >>> print(schema.summary())
+    """
+
+    # Pattern keywords for different column types
+    PATIENT_ID_PATTERNS: Set[str] = {
+        'patient_id', 'patientid', 'patient', 'id', 'subject_id',
+        'subjectid', 'subject', 'mrn', 'study_id', 'studyid', 'participant_id'
+    }
+
+    OUTCOME_PATTERNS: Set[str] = {
+        'outcome', 'death', 'mortality', 'died', 'deceased', 'expired',
+        'icu', 'hospitalized', 'hospitalisation', 'readmit', 'complication',
+        'adverse', 'event', 'endpoint', 'status', 'relapse'
+    }
+
+    TIME_PATTERNS: Set[str] = {
+        'time', 'date', 'day', 'month', 'year', 'duration',
+        'survival', 'followup', 'follow_up', 'days_to', 'time_to'
+    }
+
+    EVENT_PATTERNS: Set[str] = {
+        'event', 'status', 'censor', 'indicator', 'flag', 'occurred'
+    }
+
+    def __init__(self):
+        """Initialize schema inference engine."""
+        pass
+
+    def infer_schema(self, df: pl.DataFrame) -> InferredSchema:
+        """
+        Infer complete schema from Polars DataFrame.
+
+        Args:
+            df: Raw Polars DataFrame to analyze
+
+        Returns:
+            InferredSchema with all detected columns and confidence scores
+
+        Example:
+            >>> df = pl.read_csv("data.csv")
+            >>> schema = engine.infer_schema(df)
+            >>> config = schema.to_dataset_config()
+        """
+        schema = InferredSchema()
+
+        # 1. Detect patient ID column (highest priority)
+        schema.patient_id_column, conf = self._detect_patient_id(df)
+        if schema.patient_id_column:
+            schema.confidence_scores['patient_id'] = conf
+
+        # 2. Detect outcome columns (binary variables with clinical names)
+        schema.outcome_columns = self._detect_outcomes(df)
+        for col in schema.outcome_columns:
+            schema.confidence_scores[f'outcome_{col}'] = 0.8  # Default confidence
+
+        # 3. Detect time columns (dates, numeric time values)
+        schema.time_columns = self._detect_time_columns(df)
+
+        # 4. Detect event columns (binary, often paired with time columns)
+        schema.event_columns = self._detect_event_columns(df)
+
+        # 5. Classify remaining columns as categorical or continuous
+        classified = {schema.patient_id_column} | set(schema.outcome_columns) | \
+                     set(schema.time_columns) | set(schema.event_columns)
+
+        for col in df.columns:
+            if col in classified:
+                continue
+
+            if self._is_categorical(df, col):
+                schema.categorical_columns.append(col)
+            else:
+                schema.continuous_columns.append(col)
+
+        # 6. Infer time_zero (earliest date column or static)
+        if schema.time_columns:
+            # Use earliest date as time_zero
+            schema.time_zero = schema.time_columns[0]
+        else:
+            # No time columns - use static time_zero
+            schema.time_zero = "2020-01-01"
+
+        return schema
+
+    def _detect_patient_id(self, df: pl.DataFrame) -> tuple[Optional[str], float]:
+        """
+        Detect patient ID column using Polars operations.
+
+        Strategy:
+        1. Check name patterns (patient_id, subject_id, etc.)
+        2. Verify high uniqueness (>95% unique values)
+        3. Check for null values (ID should have none)
+
+        Args:
+            df: Polars DataFrame
+
+        Returns:
+            Tuple of (column_name, confidence_score)
+        """
+        for col in df.columns:
+            col_lower = col.lower()
+
+            # Check name patterns
+            if any(pattern in col_lower for pattern in self.PATIENT_ID_PATTERNS):
+                # Verify high uniqueness
+                unique_count = df[col].n_unique()
+                total_count = df.height
+                uniqueness_ratio = unique_count / total_count if total_count > 0 else 0
+
+                # Check for nulls
+                null_count = df[col].null_count()
+
+                if uniqueness_ratio > 0.95 and null_count == 0:
+                    return col, 0.95
+                elif uniqueness_ratio > 0.9:
+                    return col, 0.85
+
+        # Fallback: highest cardinality column with no nulls
+        best_col = None
+        best_ratio = 0.0
+
+        for col in df.columns:
+            unique_count = df[col].n_unique()
+            total_count = df.height
+            null_count = df[col].null_count()
+
+            uniqueness_ratio = unique_count / total_count if total_count > 0 else 0
+
+            if null_count == 0 and uniqueness_ratio > best_ratio:
+                best_ratio = uniqueness_ratio
+                best_col = col
+
+        if best_ratio > 0.95:
+            return best_col, 0.7  # Lower confidence for fallback
+
+        return None, 0.0
+
+    def _detect_outcomes(self, df: pl.DataFrame) -> List[str]:
+        """
+        Detect outcome columns (binary with clinical names).
+
+        Strategy:
+        1. Must be binary (2 unique values)
+        2. Check name patterns (mortality, icu, etc.)
+        3. Prefer 0/1 or True/False encoding
+
+        Args:
+            df: Polars DataFrame
+
+        Returns:
+            List of outcome column names
+        """
+        outcome_cols = []
+
+        for col in df.columns:
+            col_lower = col.lower()
+
+            # Must be binary (2 unique non-null values)
+            unique_count = df[col].n_unique()
+
+            if unique_count != 2:
+                continue
+
+            # Check name patterns
+            if any(pattern in col_lower for pattern in self.OUTCOME_PATTERNS):
+                outcome_cols.append(col)
+
+        return outcome_cols
+
+    def _detect_time_columns(self, df: pl.DataFrame) -> List[str]:
+        """
+        Detect time-related columns using Polars dtype checking.
+
+        Strategy:
+        1. Check for datetime dtypes
+        2. Check name patterns (time, date, duration, etc.)
+        3. For numeric columns, check if values look like time (positive integers)
+
+        Args:
+            df: Polars DataFrame
+
+        Returns:
+            List of time column names
+        """
+        time_cols = []
+
+        for col in df.columns:
+            col_lower = col.lower()
+            dtype = df[col].dtype
+
+            # Check if datetime type
+            if dtype in [pl.Datetime, pl.Date]:
+                time_cols.append(col)
+                continue
+
+            # Check name patterns
+            if any(pattern in col_lower for pattern in self.TIME_PATTERNS):
+                time_cols.append(col)
+
+        return time_cols
+
+    def _detect_event_columns(self, df: pl.DataFrame) -> List[str]:
+        """
+        Detect event indicator columns (binary, often paired with time).
+
+        Strategy:
+        1. Must be binary
+        2. Check name patterns (event, status, censor, etc.)
+        3. Often appears near time columns
+
+        Args:
+            df: Polars DataFrame
+
+        Returns:
+            List of event column names
+        """
+        event_cols = []
+
+        for col in df.columns:
+            col_lower = col.lower()
+
+            # Must be binary
+            unique_count = df[col].n_unique()
+
+            if unique_count != 2:
+                continue
+
+            # Check name patterns
+            if any(pattern in col_lower for pattern in self.EVENT_PATTERNS):
+                event_cols.append(col)
+
+        return event_cols
+
+    def _is_categorical(self, df: pl.DataFrame, col: str) -> bool:
+        """
+        Classify column as categorical or continuous using Polars.
+
+        Strategy:
+        1. String columns are categorical
+        2. Numeric columns with ‚â§20 unique values are categorical
+        3. Boolean columns are categorical
+
+        Args:
+            df: Polars DataFrame
+            col: Column name
+
+        Returns:
+            True if categorical, False if continuous
+        """
+        dtype = df[col].dtype
+
+        # String columns are categorical
+        if dtype == pl.Utf8 or dtype == pl.Categorical:
+            return True
+
+        # Boolean columns are categorical
+        if dtype == pl.Boolean:
+            return True
+
+        # Numeric columns: check cardinality
+        if dtype in [pl.Int8, pl.Int16, pl.Int32, pl.Int64,
+                     pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64,
+                     pl.Float32, pl.Float64]:
+            unique_count = df[col].n_unique()
+            return unique_count <= 20
+
+        # Default to categorical for unknown types
+        return True
+
+    def parse_dictionary_pdf(self, pdf_path: Path) -> Optional[DictionaryMetadata]:
+        """
+        Parse data dictionary PDF to extract column metadata using LangChain.
+
+        Uses LangChain's PyPDFLoader for better text extraction and pattern matching to find:
+        - Column names
+        - Descriptions
+        - Data types
+        - Valid values
+
+        Args:
+            pdf_path: Path to data dictionary PDF
+
+        Returns:
+            DictionaryMetadata with extracted information, or None if parsing fails
+
+        Note:
+            This is a best-effort extraction. PDF structure varies widely,
+            so we use heuristics to identify column documentation.
+        """
+        try:
+            from langchain_community.document_loaders import PyPDFLoader
+        except ImportError:
+            print("Warning: LangChain not installed. Install with: uv add langchain langchain-community")
+            return None
+
+        if not pdf_path.exists():
+            return None
+
+        metadata = DictionaryMetadata(source_file=pdf_path)
+
+        try:
+            # Load PDF using LangChain
+            loader = PyPDFLoader(str(pdf_path))
+            pages = loader.load()
+
+            # Combine text from all pages
+            full_text = "\n".join([page.page_content for page in pages])
+
+            # Pattern 1: "ColumnName: Description" or "ColumnName - Description"
+            pattern1 = r'^([a-z_][a-z0-9_]*)\s*[:\-]\s*(.+)$'
+
+            # Pattern 2: "Variable Name: Description" format
+            pattern2 = r'(?:variable|column|field)\s+(?:name|id)?\s*[:\-]?\s*([a-z_][a-z0-9_]*)\s*(?:description|meaning)?[:\-]\s*(.+)$'
+
+            for line in full_text.split('\n'):
+                line = line.strip()
+
+                if not line:
+                    continue
+
+                # Try pattern 1
+                match = re.match(pattern1, line, re.IGNORECASE)
+                if match:
+                    col_name = match.group(1).lower()
+                    description = match.group(2).strip()
+
+                    # Skip if description is too short (likely not a real description)
+                    if len(description) > 10:
+                        metadata.column_descriptions[col_name] = description
+                    continue
+
+                # Try pattern 2
+                match = re.match(pattern2, line, re.IGNORECASE)
+                if match:
+                    col_name = match.group(1).lower()
+                    description = match.group(2).strip()
+
+                    if len(description) > 10:
+                        metadata.column_descriptions[col_name] = description
+
+            # Extract data types if mentioned
+            type_patterns = {
+                'integer': ['integer', 'int', 'numeric', 'number'],
+                'float': ['float', 'decimal', 'real', 'double'],
+                'string': ['string', 'text', 'varchar', 'char'],
+                'date': ['date', 'datetime', 'timestamp'],
+                'boolean': ['boolean', 'bool', 'binary', 'yes/no']
+            }
+
+            for col_name, desc in metadata.column_descriptions.items():
+                desc_lower = desc.lower()
+
+                for dtype, keywords in type_patterns.items():
+                    if any(kw in desc_lower for kw in keywords):
+                        metadata.column_types[col_name] = dtype
+                        break
+
+            return metadata if metadata.column_descriptions else None
+
+        except Exception as e:
+            print(f"Error parsing PDF dictionary: {e}")
+            return None
+
+    def infer_schema_with_dictionary(
+        self,
+        df: pl.DataFrame,
+        dictionary_path: Optional[Path] = None
+    ) -> InferredSchema:
+        """
+        Infer schema from DataFrame and merge with PDF dictionary metadata.
+
+        This is the enhanced version that provides richer context by combining:
+        1. DataFrame analysis (data-driven inference)
+        2. PDF dictionary (documentation-driven context)
+
+        Args:
+            df: Polars DataFrame to analyze
+            dictionary_path: Optional path to data dictionary PDF
+
+        Returns:
+            InferredSchema enriched with dictionary metadata
+
+        Example:
+            >>> df = pl.read_csv("patient_data.csv")
+            >>> dict_path = Path("data/dictionaries/patient_data_dict.pdf")
+            >>> schema = engine.infer_schema_with_dictionary(df, dict_path)
+            >>> print(schema.summary())  # Shows DataFrame + dictionary context
+        """
+        # First, do standard DataFrame-based inference
+        schema = self.infer_schema(df)
+
+        # Then, try to merge with dictionary metadata
+        if dictionary_path:
+            dict_metadata = self.parse_dictionary_pdf(dictionary_path)
+
+            if dict_metadata:
+                schema.dictionary_metadata = dict_metadata
+
+                # Use dictionary hints to improve detection confidence
+                for col in df.columns:
+                    col_desc = dict_metadata.get_description(col)
+
+                    if not col_desc:
+                        continue
+
+                    desc_lower = col_desc.lower()
+
+                    # Check if dictionary suggests this is an outcome
+                    outcome_keywords = ['outcome', 'mortality', 'death', 'died', 'icu', 'event']
+                    if any(kw in desc_lower for kw in outcome_keywords):
+                        if col not in schema.outcome_columns:
+                            # Dictionary suggests this is an outcome
+                            if df[col].n_unique() == 2:  # Verify it's binary
+                                schema.outcome_columns.append(col)
+                                schema.confidence_scores[f'outcome_{col}'] = 0.95  # High confidence from dictionary
+
+                    # Check if dictionary suggests this is a time variable
+                    time_keywords = ['time', 'date', 'duration', 'days', 'months', 'years']
+                    if any(kw in desc_lower for kw in time_keywords):
+                        if col not in schema.time_columns:
+                            schema.time_columns.append(col)
+
+        return schema
diff --git a/src/clinical_analytics/datasets/uploaded/definition.py b/src/clinical_analytics/datasets/uploaded/definition.py
index 662cec8..6cb6858 100644
--- a/src/clinical_analytics/datasets/uploaded/definition.py
+++ b/src/clinical_analytics/datasets/uploaded/definition.py
@@ -73,8 +73,9 @@ class UploadedDataset(ClinicalDataset):
         """
         Return analysis cohort mapped to UnifiedCohort schema.
 
-        Maps user columns to UnifiedCohort schema based on
-        the variable mapping from upload wizard.
+        Maps user columns to UnifiedCohort schema based on either:
+        - variable_mapping (from single-table upload wizard)
+        - inferred_schema (from multi-table ZIP upload)
 
         Args:
             **filters: Optional filters (not yet implemented)
@@ -85,11 +86,18 @@ class UploadedDataset(ClinicalDataset):
         if self.data is None:
             self.load()
 
-        # Get variable mapping from metadata
+        # Get variable mapping from metadata (single-table uploads)
         variable_mapping = self.metadata.get('variable_mapping', {})
 
+        # If no variable mapping, try to build it from inferred schema (ZIP uploads)
         if not variable_mapping:
-            raise ValueError("Variable mapping not found in upload metadata")
+            inferred_schema = self.metadata.get('inferred_schema', {})
+
+            if inferred_schema:
+                # Convert inferred_schema to variable_mapping format
+                variable_mapping = self._convert_inferred_schema_to_mapping(inferred_schema)
+            else:
+                raise ValueError("Neither variable_mapping nor inferred_schema found in upload metadata")
 
         # Extract mapping fields
         patient_id_col = variable_mapping.get('patient_id')
@@ -141,6 +149,53 @@ class UploadedDataset(ClinicalDataset):
 
         return cohort
 
+    def _convert_inferred_schema_to_mapping(self, inferred_schema: Dict[str, Any]) -> Dict[str, Any]:
+        """
+        Convert inferred_schema format (from ZIP uploads) to variable_mapping format.
+
+        Args:
+            inferred_schema: Schema from schema inference engine
+
+        Returns:
+            variable_mapping dictionary compatible with get_cohort()
+        """
+        variable_mapping = {
+            'patient_id': None,
+            'outcome': None,
+            'time_variables': {},
+            'predictors': []
+        }
+
+        # Extract patient ID from column_mapping
+        column_mapping = inferred_schema.get('column_mapping', {})
+        for col, role in column_mapping.items():
+            if role == 'patient_id':
+                variable_mapping['patient_id'] = col
+                break
+
+        # Extract first outcome
+        outcomes = inferred_schema.get('outcomes', {})
+        if outcomes:
+            # Use first outcome as primary
+            first_outcome = list(outcomes.keys())[0]
+            variable_mapping['outcome'] = first_outcome
+
+        # Extract time_zero
+        time_zero_config = inferred_schema.get('time_zero', {})
+        if 'source_column' in time_zero_config:
+            variable_mapping['time_variables']['time_zero'] = time_zero_config['source_column']
+
+        # Add all other columns as predictors (exclude patient_id and outcome)
+        if self.data is not None:
+            all_cols = set(self.data.columns)
+            excluded = {variable_mapping['patient_id'], variable_mapping['outcome']}
+            variable_mapping['predictors'] = [
+                col for col in all_cols
+                if col not in excluded and col not in {None}
+            ]
+
+        return variable_mapping
+
     def get_info(self) -> Dict[str, Any]:
         """
         Get dataset information.
diff --git "a/src/clinical_analytics/ui/pages/1_\360\237\223\244_Upload_Data.py" "b/src/clinical_analytics/ui/pages/1_\360\237\223\244_Upload_Data.py"
index 2e3577b..9167c49 100644
--- "a/src/clinical_analytics/ui/pages/1_\360\237\223\244_Upload_Data.py"
+++ "b/src/clinical_analytics/ui/pages/1_\360\237\223\244_Upload_Data.py"
@@ -5,23 +5,33 @@ Self-service data upload for clinicians.
 Upload CSV, Excel, or SPSS files without code or YAML configuration.
 """
 
-import streamlit as st
-import pandas as pd
+import logging
 import sys
 from pathlib import Path
-from typing import Optional
+
+import pandas as pd
+import streamlit as st
+
+# Configure logging for verbose output
+logging.basicConfig(
+    level=logging.INFO,
+    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
+    handlers=[
+        logging.StreamHandler(sys.stdout)
+    ]
+)
+
+# Set specific loggers to INFO/DEBUG for visibility
+logging.getLogger('clinical_analytics.core.multi_table_handler').setLevel(logging.INFO)
+logging.getLogger('clinical_analytics.ui.storage.user_datasets').setLevel(logging.INFO)
 
 # Add src to path
 sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))
 
-from clinical_analytics.ui.storage.user_datasets import (
-    UserDatasetStorage,
-    UploadSecurityValidator
-)
+from clinical_analytics.ui.components.data_validator import DataQualityValidator
 from clinical_analytics.ui.components.variable_detector import VariableTypeDetector
 from clinical_analytics.ui.components.variable_mapper import VariableMappingWizard
-from clinical_analytics.ui.components.data_validator import DataQualityValidator
-
+from clinical_analytics.ui.storage.user_datasets import UploadSecurityValidator, UserDatasetStorage
 
 # Page configuration
 st.set_page_config(
@@ -38,23 +48,29 @@ def render_upload_step():
     """Step 1: File Upload"""
     st.markdown("## üì§ Upload Your Data")
     st.markdown("""
-    Upload your clinical dataset in CSV, Excel, or SPSS format.
+    Upload your clinical dataset in CSV, Excel, SPSS, or ZIP format.
 
     **Supported formats:**
-    - CSV (`.csv`)
-    - Excel (`.xlsx`, `.xls`)
-    - SPSS (`.sav`)
+    - CSV (`.csv`) - Single table
+    - Excel (`.xlsx`, `.xls`) - Single table
+    - SPSS (`.sav`) - Single table
+    - **ZIP (`.zip`) - Multi-table datasets** ‚≠ê NEW!
 
     **Requirements:**
     - File size: 1KB - 100MB
     - Must include patient ID column
     - Must include outcome variable
+
+    **Multi-Table ZIP Format:**
+    - ZIP file containing multiple CSV files
+    - Tables will be automatically joined (e.g., MIMIC-IV demo)
+    - Relationships detected via foreign keys
     """)
 
     uploaded_file = st.file_uploader(
         "Choose a file",
-        type=['csv', 'xlsx', 'xls', 'sav'],
-        help="Maximum file size: 100MB",
+        type=['csv', 'xlsx', 'xls', 'sav', 'zip'],
+        help="Maximum file size: 100MB. ZIP files for multi-table datasets.",
         key="file_uploader"
     )
 
@@ -80,10 +96,30 @@ def render_upload_step():
 
         st.success("‚úÖ File validation passed")
 
-        # Try to load preview
-        try:
-            file_ext = Path(uploaded_file.name).suffix.lower()
+        # Check if ZIP file (multi-table)
+        file_ext = Path(uploaded_file.name).suffix.lower()
+
+        if file_ext == '.zip':
+            # Handle multi-table ZIP upload
+            st.info("üóÇÔ∏è **Multi-table dataset detected!** ZIP file validated.")
 
+            # Store ZIP upload details
+            st.session_state['is_zip_upload'] = True
+            st.session_state['uploaded_filename'] = uploaded_file.name
+            st.session_state['uploaded_bytes'] = file_bytes
+
+            st.success("‚úÖ ZIP file ready for processing")
+            st.info("üí° **Next:** Click 'Continue to Review' to process tables, detect relationships, and build unified cohort.")
+
+            # Button to proceed to review step
+            if st.button("Continue to Review ‚û°Ô∏è", type="primary"):
+                st.session_state['upload_step'] = 5
+                st.rerun()
+
+            return uploaded_file
+
+        # Try to load preview for single-table files
+        try:
             if file_ext == '.csv':
                 df = pd.read_csv(uploaded_file)
             elif file_ext in {'.xlsx', '.xls'}:
@@ -95,7 +131,8 @@ def render_upload_step():
                 st.error(f"Unsupported file type: {file_ext}")
                 return None
 
-            # Store in session state
+            # Store in session state (single-table)
+            st.session_state['is_zip_upload'] = False
             st.session_state['uploaded_df'] = df
             st.session_state['uploaded_filename'] = uploaded_file.name
             st.session_state['uploaded_bytes'] = file_bytes
@@ -309,10 +346,24 @@ def render_mapping_step(df: pd.DataFrame, variable_info: dict, suggestions: dict
                 st.rerun()
 
 
-def render_review_step(df: pd.DataFrame, mapping: dict, variable_info: dict):
+def render_review_step(df: pd.DataFrame = None, mapping: dict = None, variable_info: dict = None):
     """Step 5: Final Review & Save"""
     st.markdown("## ‚úÖ Review & Save Dataset")
 
+    # Check if this is a ZIP upload (multi-table)
+    is_zip = st.session_state.get('is_zip_upload', False)
+
+    if is_zip:
+        # Handle multi-table ZIP upload
+        return render_zip_review_step()
+
+    # Single-table workflow (existing logic)
+    if df is None or mapping is None or variable_info is None:
+        st.warning("Missing required data. Please start over.")
+        st.session_state['upload_step'] = 1
+        st.rerun()
+        return
+
     # Run final validation with mapping
     patient_id_col = mapping['patient_id']
     outcome_col = mapping['outcome']
@@ -416,6 +467,159 @@ def render_review_step(df: pd.DataFrame, mapping: dict, variable_info: dict):
                 st.error(f"‚ùå {message}")
 
 
+def render_zip_review_step():
+    """Step 5: Review & Save for Multi-Table ZIP Upload"""
+    st.markdown("### üóÇÔ∏è Multi-Table Dataset Processing")
+
+    # Dataset name input
+    st.markdown("### üìù Dataset Name")
+    default_name = Path(st.session_state.get('uploaded_filename', 'dataset')).stem
+    dataset_name = st.text_input(
+        "Enter a name for this dataset",
+        value=default_name,
+        help="This name will be used to identify your dataset in the analysis interface"
+    )
+
+    # Process ZIP file
+    if st.button("üöÄ Process & Save Multi-Table Dataset", type="primary", disabled=not dataset_name.strip()):
+        # Create progress tracking UI elements
+        progress_bar = st.progress(0)
+        status_text = st.empty()
+        log_expander = st.expander("üìã Processing Log", expanded=True)
+        log_container = log_expander.container()
+
+
+        def progress_callback(step, total_steps, message, details):
+            """Update progress UI with current step information."""
+            progress = step / total_steps if total_steps > 0 else 0
+            progress_bar.progress(progress)
+            status_text.info(f"üîÑ {message}")
+
+            # Add to log
+            with log_container:
+                if details:
+                    if 'table_name' in details:
+                        table_info = f"**{details['table_name']}**"
+                        if 'rows' in details:
+                            table_info += f" - {details['rows']:,} rows, {details['cols']} cols"
+                        if 'progress' in details:
+                            table_info += f" ({details['progress']})"
+                        st.text(f"‚úì {table_info}")
+                    elif 'tables_found' in details:
+                        st.text(f"üì¶ Found {details['tables_found']} tables in ZIP")
+                        if 'table_names' in details:
+                            st.text(f"   Tables: {', '.join(details['table_names'][:5])}" +
+                                   (f" ... and {len(details['table_names']) - 5} more"
+                                    if len(details['table_names']) > 5 else ""))
+                    elif 'relationships' in details:
+                        st.text(f"üîó Detected {len(details['relationships'])} relationships")
+                        for rel in details['relationships'][:3]:  # Show first 3
+                            st.text(f"   ‚Ä¢ {rel}")
+                        if len(details['relationships']) > 3:
+                            st.text(f"   ... and {len(details['relationships']) - 3} more")
+                    else:
+                        st.text(f"‚Üí {message}")
+                else:
+                    st.text(f"‚Üí {message}")
+
+        # Prepare metadata
+        metadata = {
+            'dataset_name': dataset_name
+        }
+
+        # Save ZIP upload (this processes everything)
+        try:
+            success, message, upload_id = storage.save_zip_upload(
+                file_bytes=st.session_state['uploaded_bytes'],
+                original_filename=st.session_state['uploaded_filename'],
+                metadata=metadata,
+                progress_callback=progress_callback
+            )
+        except Exception as e:
+            import traceback
+            with log_container:
+                st.error(f"‚ùå Error during processing: {str(e)}")
+                st.code(traceback.format_exc())
+            status_text.error(f"‚ùå Processing failed: {str(e)}")
+            success = False
+            message = str(e)
+            upload_id = None
+
+        if success:
+            progress_bar.progress(1.0)
+            status_text.success(f"‚úÖ {message}")
+            with log_container:
+                st.success("‚úÖ Processing complete!")
+            st.balloons()
+
+            # Load metadata to show details
+            upload_metadata = storage.get_upload_metadata(upload_id)
+            if upload_metadata:
+                st.markdown("### üìä Processing Summary")
+
+                col1, col2, col3 = st.columns(3)
+                with col1:
+                    st.metric("Tables Joined", upload_metadata.get('tables', []) and len(upload_metadata.get('tables', [])) or 0)
+                with col2:
+                    st.metric("Unified Rows", upload_metadata.get('row_count', 0))
+                with col3:
+                    st.metric("Total Columns", upload_metadata.get('column_count', 0))
+
+                # Show detected relationships
+                relationships = upload_metadata.get('relationships', [])
+                if relationships:
+                    with st.expander(f"üîó Detected Relationships ({len(relationships)})"):
+                        for rel in relationships:
+                            st.code(rel)
+
+                # Show tables
+                tables = upload_metadata.get('tables', [])
+                table_counts = upload_metadata.get('table_counts', {})
+                if tables:
+                    with st.expander(f"üìã Tables ({len(tables)})"):
+                        for table in tables:
+                            count = table_counts.get(table, 0)
+                            st.markdown(f"- **{table}**: {count:,} rows")
+
+                # Show inferred schema
+                inferred_schema = upload_metadata.get('inferred_schema', {})
+                if inferred_schema:
+                    with st.expander("üî¨ Inferred Schema"):
+                        if inferred_schema.get('column_mapping'):
+                            st.markdown("**Column Mappings:**")
+                            for col, role in inferred_schema['column_mapping'].items():
+                                st.markdown(f"- `{col}` ‚Üí {role}")
+
+                        if inferred_schema.get('outcomes'):
+                            st.markdown("**Outcomes:**")
+                            for outcome, config in inferred_schema['outcomes'].items():
+                                st.markdown(f"- `{outcome}` ({config.get('type', 'unknown')})")
+
+            st.markdown(f"""
+            **Dataset saved successfully!**
+
+            - **Upload ID:** `{upload_id}`
+            - **Name:** {dataset_name}
+            - **Format:** Multi-table (ZIP)
+
+            You can now use this dataset in the main analysis interface.
+            """)
+
+            # Clear session state
+            if st.button("Upload Another Dataset"):
+                for key in list(st.session_state.keys()):
+                    if key.startswith('upload') or key == 'is_zip_upload':
+                        del st.session_state[key]
+                st.rerun()
+        else:
+            st.error(f"‚ùå {message}")
+
+    # Back button
+    if st.button("‚¨ÖÔ∏è Back to Upload"):
+        st.session_state['upload_step'] = 1
+        st.rerun()
+
+
 def main():
     """Main upload page logic"""
     st.title("üì§ Upload Clinical Data")
@@ -474,7 +678,13 @@ def main():
             st.rerun()
 
     elif current_step == 5:
-        if all(k in st.session_state for k in ['uploaded_df', 'variable_mapping', 'variable_info']):
+        # Check if ZIP upload (skip to review directly)
+        is_zip = st.session_state.get('is_zip_upload', False)
+
+        if is_zip:
+            # For ZIP files, go directly to review (skip preview/detection/mapping)
+            render_review_step()
+        elif all(k in st.session_state for k in ['uploaded_df', 'variable_mapping', 'variable_info']):
             render_review_step(
                 st.session_state['uploaded_df'],
                 st.session_state['variable_mapping'],
diff --git "a/src/clinical_analytics/ui/pages/2_\360\237\223\212_Descriptive_Stats.py" "b/src/clinical_analytics/ui/pages/2_\360\237\223\212_Descriptive_Stats.py"
index 03f5297..dacbb14 100644
--- "a/src/clinical_analytics/ui/pages/2_\360\237\223\212_Descriptive_Stats.py"
+++ "b/src/clinical_analytics/ui/pages/2_\360\237\223\212_Descriptive_Stats.py"
@@ -155,6 +155,7 @@ def main():
 
     # Add uploaded datasets
     uploaded_datasets = {}
+    uploaded_ids = set()  # Track upload IDs for detection
     try:
         uploads = UploadedDatasetFactory.list_available_uploads()
         for upload in uploads:
@@ -163,6 +164,7 @@ def main():
             display_name = f"üì§ {dataset_name}"
             dataset_display_names[display_name] = upload_id
             uploaded_datasets[upload_id] = upload
+            uploaded_ids.add(upload_id)
     except Exception as e:
         st.sidebar.warning(f"Could not load uploads: {e}")
 
@@ -176,20 +178,47 @@ def main():
     )
 
     dataset_choice = dataset_display_names[dataset_choice_display]
-    is_uploaded = dataset_choice in uploaded_datasets
+    
+    # Check if this is an uploaded dataset
+    # Method 1: Check if in uploaded_datasets dict
+    # Method 2: Check if display name starts with üì§
+    # Method 3: Check if dataset_choice is an upload_id (UUID-like or matches upload pattern)
+    is_uploaded = (
+        dataset_choice in uploaded_datasets or
+        dataset_choice_display.startswith("üì§") or
+        dataset_choice in uploaded_ids
+    )
 
     # Load dataset
     with st.spinner(f"Loading {dataset_choice_display}..."):
         try:
             if is_uploaded:
-                dataset = UploadedDatasetFactory.create_dataset(dataset_choice)
-                dataset.load()
+                # For uploaded datasets, use the factory
+                upload_id = dataset_choice  # dataset_choice is the upload_id for uploaded datasets
+                try:
+                    dataset = UploadedDatasetFactory.create_dataset(upload_id)
+                    dataset.load()
+                except Exception as e:
+                    st.error(f"Error loading uploaded dataset: {str(e)}")
+                    st.exception(e)
+                    return
             else:
-                dataset = DatasetRegistry.get_dataset(dataset_choice)
-                if not dataset.validate():
-                    st.error("Dataset validation failed")
+                # For built-in datasets, use the registry
+                try:
+                    dataset = DatasetRegistry.get_dataset(dataset_choice)
+                    if not dataset.validate():
+                        st.error("Dataset validation failed")
+                        return
+                    dataset.load()
+                except KeyError as e:
+                    # Dataset not found in registry - might be an uploaded dataset that wasn't detected
+                    st.error(f"Dataset '{dataset_choice}' not found in registry.")
+                    st.info("üí° If this is an uploaded dataset, please refresh the page or check the upload status.")
+                    return
+                except Exception as e:
+                    st.error(f"Error loading dataset: {str(e)}")
+                    st.exception(e)
                     return
-                dataset.load()
 
             cohort = dataset.get_cohort()
 
diff --git a/src/clinical_analytics/ui/storage/user_datasets.py b/src/clinical_analytics/ui/storage/user_datasets.py
index 7f2e7d6..501550c 100644
--- a/src/clinical_analytics/ui/storage/user_datasets.py
+++ b/src/clinical_analytics/ui/storage/user_datasets.py
@@ -4,11 +4,13 @@ User Dataset Storage Manager
 Handles secure storage, metadata management, and persistence of uploaded datasets.
 """
 
-import json
 import hashlib
+import json
+from collections.abc import Callable
 from datetime import datetime
 from pathlib import Path
-from typing import Dict, List, Optional, Any
+from typing import Any
+
 import pandas as pd
 import polars as pl
 
@@ -25,7 +27,7 @@ class UploadSecurityValidator:
     """
 
     # Allowlisted file extensions
-    ALLOWED_EXTENSIONS = {'.csv', '.xlsx', '.xls', '.sav'}
+    ALLOWED_EXTENSIONS = {'.csv', '.xlsx', '.xls', '.sav', '.zip'}
 
     # Maximum file size (100MB as per Phase 0 spec)
     MAX_FILE_SIZE_BYTES = 100 * 1024 * 1024  # 100MB
@@ -137,7 +139,7 @@ class UserDatasetStorage:
     - Integration with existing registry system
     """
 
-    def __init__(self, upload_dir: Optional[Path] = None):
+    def __init__(self, upload_dir: Path | None = None):
         """
         Initialize storage manager.
 
@@ -177,8 +179,8 @@ class UserDatasetStorage:
         self,
         file_bytes: bytes,
         original_filename: str,
-        metadata: Dict[str, Any]
-    ) -> tuple[bool, str, Optional[str]]:
+        metadata: dict[str, Any]
+    ) -> tuple[bool, str, str | None]:
         """
         Save uploaded file with security validation.
 
@@ -221,6 +223,7 @@ class UserDatasetStorage:
             elif file_ext == '.sav':
                 # SPSS file - convert to CSV
                 import io
+
                 import pyreadstat
                 df, meta = pyreadstat.read_sav(io.BytesIO(file_bytes))
                 csv_path = self.raw_dir / f"{upload_id}.csv"
@@ -251,7 +254,7 @@ class UserDatasetStorage:
         except Exception as e:
             return False, f"Error saving upload: {str(e)}", None
 
-    def get_upload_metadata(self, upload_id: str) -> Optional[Dict[str, Any]]:
+    def get_upload_metadata(self, upload_id: str) -> dict[str, Any] | None:
         """
         Retrieve metadata for an upload.
 
@@ -266,10 +269,10 @@ class UserDatasetStorage:
         if not metadata_path.exists():
             return None
 
-        with open(metadata_path, 'r') as f:
+        with open(metadata_path) as f:
             return json.load(f)
 
-    def get_upload_data(self, upload_id: str) -> Optional[pd.DataFrame]:
+    def get_upload_data(self, upload_id: str) -> pd.DataFrame | None:
         """
         Load uploaded dataset.
 
@@ -286,7 +289,7 @@ class UserDatasetStorage:
 
         return pd.read_csv(csv_path)
 
-    def list_uploads(self) -> List[Dict[str, Any]]:
+    def list_uploads(self) -> list[dict[str, Any]]:
         """
         List all uploaded datasets.
 
@@ -296,7 +299,7 @@ class UserDatasetStorage:
         uploads = []
 
         for metadata_file in self.metadata_dir.glob("*.json"):
-            with open(metadata_file, 'r') as f:
+            with open(metadata_file) as f:
                 metadata = json.load(f)
                 uploads.append(metadata)
 
@@ -337,7 +340,7 @@ class UserDatasetStorage:
         except Exception as e:
             return False, f"Error deleting upload: {str(e)}"
 
-    def update_metadata(self, upload_id: str, metadata_updates: Dict[str, Any]) -> tuple[bool, str]:
+    def update_metadata(self, upload_id: str, metadata_updates: dict[str, Any]) -> tuple[bool, str]:
         """
         Update metadata for an upload.
 
@@ -355,7 +358,7 @@ class UserDatasetStorage:
 
         try:
             # Load existing metadata
-            with open(metadata_path, 'r') as f:
+            with open(metadata_path) as f:
                 metadata = json.load(f)
 
             # Update with new fields
@@ -369,3 +372,234 @@ class UserDatasetStorage:
 
         except Exception as e:
             return False, f"Error updating metadata: {str(e)}"
+
+    def save_zip_upload(
+        self,
+        file_bytes: bytes,
+        original_filename: str,
+        metadata: dict[str, Any],
+        progress_callback: Callable[[int, int, str, dict], None] | None = None
+    ) -> tuple[bool, str, str | None]:
+        """
+        Save uploaded ZIP file containing multiple CSV files.
+
+        This enables multi-table dataset support (e.g., MIMIC-IV).
+        Tables are extracted, relationships detected, and unified cohort created.
+
+        Args:
+            file_bytes: ZIP file content
+            original_filename: Original filename
+            metadata: Upload metadata
+            progress_callback: Optional callback function(step, total_steps, message, details)
+
+        Returns:
+            Tuple of (success, message, upload_id)
+        """
+        import io
+        import zipfile
+
+        from clinical_analytics.core.multi_table_handler import MultiTableHandler
+        from clinical_analytics.core.schema_inference import SchemaInferenceEngine
+
+        # Security validation
+        valid, error = UploadSecurityValidator.validate(original_filename, file_bytes)
+        if not valid:
+            return False, error, None
+
+        # Generate upload ID
+        upload_id = self.generate_upload_id(original_filename)
+
+        try:
+            import logging
+            logger = logging.getLogger(__name__)
+            logger.info(f"Starting ZIP upload processing: {original_filename}")
+
+            # Extract ZIP contents
+            zip_buffer = io.BytesIO(file_bytes)
+            tables: dict[str, pl.DataFrame] = {}
+
+            with zipfile.ZipFile(zip_buffer, 'r') as zip_file:
+                # Get list of CSV files in ZIP (including .csv.gz in subdirectories)
+                csv_files = [
+                    f for f in zip_file.namelist()
+                    if (f.endswith('.csv') or f.endswith('.csv.gz'))
+                    and not f.startswith('__MACOSX')
+                    and not f.endswith('/')  # Skip directory entries
+                ]
+
+                if not csv_files:
+                    return False, "No CSV files found in ZIP archive", None
+
+                logger.info(f"Found {len(csv_files)} CSV files in ZIP archive")
+
+                # Calculate total steps now that we know number of files
+                # 1 (found tables) + len(csv_files) (loading) + 4 (detect, build, save, infer)
+                total_steps = 1 + len(csv_files) + 4
+
+                if progress_callback:
+                    progress_callback(0, total_steps, "Initializing ZIP extraction...", {})
+                    progress_callback(1, total_steps, f"Found {len(csv_files)} tables to load", {
+                        'tables_found': len(csv_files),
+                        'table_names': [Path(f).stem for f in csv_files]
+                    })
+
+                # Load each CSV as a table
+                for idx, csv_filename in enumerate(csv_files, start=1):
+                    # Extract table name (without path and extension)
+                    table_name = Path(csv_filename).stem
+                    if table_name.endswith('.csv'):
+                        # Handle .csv.gz case where stem gives us "filename.csv"
+                        table_name = Path(table_name).stem
+
+                    logger.info(f"Loading table {idx}/{len(csv_files)}: {table_name} from {csv_filename}")
+
+                    if progress_callback:
+                        progress_callback(
+                            1 + idx,
+                            total_steps,
+                            f"Loading table: {table_name}",
+                            {
+                                'table_name': table_name,
+                                'file': csv_filename,
+                                'progress': f"{idx}/{len(csv_files)}"
+                            }
+                        )
+
+                    # Read file content
+                    csv_content = zip_file.read(csv_filename)
+
+                    # Handle gzip compression
+                    if csv_filename.endswith('.gz'):
+                        import gzip
+                        logger.debug(f"Decompressing gzip file: {csv_filename}")
+                        csv_content = gzip.decompress(csv_content)
+
+                    # Load as Polars DataFrame with robust schema inference
+                    # Use larger infer_schema_length to handle mixed-type columns (e.g., ICD codes)
+                    try:
+                        logger.debug(f"Reading CSV with schema inference for {table_name}")
+                        df = pl.read_csv(
+                            io.BytesIO(csv_content),
+                            infer_schema_length=10000,  # Scan more rows for better type inference
+                            try_parse_dates=True
+                        )
+                    except Exception as e:
+                        logger.warning(f"Schema inference failed for {table_name}, falling back to string types: {e}")
+                        # Fallback: read with all columns as strings, let DuckDB handle types
+                        df = pl.read_csv(
+                            io.BytesIO(csv_content),
+                            infer_schema_length=0  # Treat all as strings
+                        )
+
+                    tables[table_name] = df
+                    logger.info(f"Loaded table '{table_name}': {df.height:,} rows, {df.width} cols")
+                    logger.debug(f"Schema for {table_name}: {dict(df.schema)}")
+
+                    if progress_callback:
+                        progress_callback(
+                            1 + idx,
+                            total_steps,
+                            f"Loaded {table_name}: {df.height:,} rows, {df.width} cols",
+                            {
+                                'table_name': table_name,
+                                'rows': df.height,
+                                'cols': df.width,
+                                'status': 'loaded'
+                            }
+                        )
+
+            logger.info(f"Extracted {len(tables)} tables from ZIP: {list(tables.keys())}")
+
+            # Detect relationships between tables
+            # Step calculation: 1 (init) + len(csv_files) (loading) = current step
+            step_num = 1 + len(csv_files)
+            logger.info(f"Detecting relationships for {len(tables)} tables")
+
+            if progress_callback:
+                progress_callback(step_num, total_steps, "Detecting table relationships...", {
+                    'tables': list(tables.keys()),
+                    'table_counts': {name: df.height for name, df in tables.items()}
+                })
+
+            handler = MultiTableHandler(tables)
+            relationships = handler.detect_relationships()
+            logger.info(f"Detected {len(relationships)} relationships")
+
+            if relationships:
+                for rel in relationships:
+                    logger.info(f"Relationship: {rel}")
+
+            if progress_callback:
+                progress_callback(step_num + 1, total_steps, f"Detected {len(relationships)} relationships", {
+                    'relationships': [str(rel) for rel in relationships]
+                })
+
+            # Build unified cohort
+            logger.info("Building unified cohort from detected relationships")
+            if progress_callback:
+                progress_callback(step_num + 2, total_steps, "Building unified cohort...", {})
+
+            unified_df = handler.build_unified_cohort()
+            logger.info(f"Unified cohort created: {unified_df.height:,} rows, {unified_df.width} cols")
+
+            # Save unified cohort as CSV
+            csv_path = self.raw_dir / f"{upload_id}.csv"
+            logger.info(f"Saving unified cohort to {csv_path}")
+            if progress_callback:
+                progress_callback(
+                    step_num + 3,
+                    total_steps,
+                    f"Saving unified cohort ({unified_df.height:,} rows)...",
+                    {}
+                )
+            unified_df.write_csv(csv_path)
+
+            # Infer schema for unified cohort
+            logger.info("Inferring schema for unified cohort")
+            if progress_callback:
+                progress_callback(step_num + 4, total_steps, "Inferring schema...", {})
+
+            engine = SchemaInferenceEngine()
+            schema = engine.infer_schema(unified_df)
+            logger.info("Schema inference complete")
+
+            # Save metadata
+            full_metadata = {
+                "upload_id": upload_id,
+                "original_filename": UploadSecurityValidator.sanitize_filename(original_filename),
+                "upload_timestamp": datetime.now().isoformat(),
+                "file_size_bytes": len(file_bytes),
+                "file_format": "zip_multi_table",
+                "row_count": unified_df.height,
+                "column_count": unified_df.width,
+                "columns": list(unified_df.columns),
+                "tables": list(tables.keys()),
+                "table_counts": {name: df.height for name, df in tables.items()},
+                "relationships": [str(rel) for rel in relationships],
+                "inferred_schema": schema.to_dataset_config(),
+                **metadata
+            }
+
+            metadata_path = self.metadata_dir / f"{upload_id}.json"
+            with open(metadata_path, 'w') as f:
+                json.dump(full_metadata, f, indent=2)
+
+            handler.close()
+
+            if progress_callback:
+                progress_callback(step_num + 4, total_steps, "Processing complete!", {
+                    'tables': len(tables),
+                    'rows': unified_df.height,
+                    'cols': unified_df.width
+                })
+
+            logger.info(f"Multi-table upload successful: {len(tables)} tables joined into {unified_df.height:,} rows")
+            return True, f"Multi-table upload successful: {len(tables)} tables joined into {unified_df.height:,} rows", upload_id
+
+        except Exception as e:
+            import logging
+            import traceback
+            logger = logging.getLogger(__name__)
+            logger.error(f"Error processing ZIP upload: {type(e).__name__}: {str(e)}")
+            logger.error(f"Traceback: {traceback.format_exc()}")
+            return False, f"Error processing ZIP upload: {str(e)}", None
diff --git a/test_zip_upload.py b/test_zip_upload.py
new file mode 100644
index 0000000..211199a
--- /dev/null
+++ b/test_zip_upload.py
@@ -0,0 +1,72 @@
+#!/usr/bin/env python3
+"""
+Test script to debug ZIP upload with logging enabled.
+"""
+
+import logging
+import sys
+from pathlib import Path
+
+# Configure logging
+logging.basicConfig(
+    level=logging.DEBUG,
+    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
+    handlers=[
+        logging.StreamHandler(sys.stdout)
+    ]
+)
+
+# Add src to path
+sys.path.insert(0, str(Path(__file__).parent / 'src'))
+
+from clinical_analytics.ui.storage.user_datasets import UserDatasetStorage
+import tempfile
+
+def test_zip_upload():
+    """Test ZIP upload with MIMIC-IV demo file."""
+    zip_path = Path("data/raw/mimic/mimic-iv-clinical-database-demo-2.2.zip")
+    
+    if not zip_path.exists():
+        print(f"ERROR: ZIP file not found at {zip_path}")
+        print("Please ensure the MIMIC-IV demo ZIP file exists at that location")
+        return
+    
+    print(f"Testing ZIP upload with: {zip_path}")
+    print(f"File size: {zip_path.stat().st_size / (1024*1024):.2f} MB")
+    print("-" * 80)
+    
+    # Read ZIP file
+    with open(zip_path, 'rb') as f:
+        zip_bytes = f.read()
+    
+    # Create temporary directory for uploads
+    with tempfile.TemporaryDirectory() as tmp_dir:
+        storage = UserDatasetStorage(upload_dir=Path(tmp_dir))
+        
+        print(f"Upload directory: {tmp_dir}")
+        print("-" * 80)
+        
+        try:
+            success, message, upload_id = storage.save_zip_upload(
+                file_bytes=zip_bytes,
+                original_filename=zip_path.name,
+                metadata={'dataset_name': 'mimic_iv_demo'}
+            )
+            
+            print("-" * 80)
+            if success:
+                print(f"‚úÖ SUCCESS: {message}")
+                print(f"Upload ID: {upload_id}")
+            else:
+                print(f"‚ùå FAILED: {message}")
+                print(f"Upload ID: {upload_id}")
+                
+        except Exception as e:
+            print("-" * 80)
+            print(f"‚ùå EXCEPTION: {type(e).__name__}: {str(e)}")
+            import traceback
+            traceback.print_exc()
+
+if __name__ == "__main__":
+    test_zip_upload()
+
diff --git a/tests/README.md b/tests/README.md
new file mode 100644
index 0000000..d8c7ead
--- /dev/null
+++ b/tests/README.md
@@ -0,0 +1,79 @@
+# Test Organization
+
+Tests are organized by function/module for better maintainability and clarity.
+
+## Directory Structure
+
+```
+tests/
+‚îú‚îÄ‚îÄ core/              # Core functionality tests
+‚îÇ   ‚îú‚îÄ‚îÄ test_mapper.py      # Column mapping and transformations
+‚îÇ   ‚îú‚îÄ‚îÄ test_registry.py    # Dataset registry and discovery
+‚îÇ   ‚îú‚îÄ‚îÄ test_profiling.py   # Data profiling and quality metrics
+‚îÇ   ‚îî‚îÄ‚îÄ test_schema.py      # Unified cohort schema
+‚îÇ
+‚îú‚îÄ‚îÄ loader/            # Dataset loader tests
+‚îÇ   ‚îú‚îÄ‚îÄ test_covid_ms_loader.py   # COVID-MS dataset loader
+‚îÇ   ‚îú‚îÄ‚îÄ test_sepsis_loader.py     # Sepsis dataset loader
+‚îÇ   ‚îî‚îÄ‚îÄ test_mimic3_loader.py     # MIMIC-III dataset loader
+‚îÇ
+‚îú‚îÄ‚îÄ analysis/          # Statistical analysis tests
+‚îÇ   ‚îú‚îÄ‚îÄ test_stats.py        # Logistic regression and statistical tests
+‚îÇ   ‚îî‚îÄ‚îÄ test_survival.py     # Survival analysis (Kaplan-Meier, Cox regression)
+‚îÇ
+‚îú‚îÄ‚îÄ ui/                # UI component tests
+‚îÇ   ‚îú‚îÄ‚îÄ test_variable_detector.py  # Variable type detection
+‚îÇ   ‚îî‚îÄ‚îÄ test_user_datasets.py      # User dataset storage and security
+‚îÇ
+‚îú‚îÄ‚îÄ conftest.py        # Pytest configuration and shared fixtures
+‚îî‚îÄ‚îÄ README.md          # This file
+```
+
+## Running Tests
+
+Run all tests:
+```bash
+pytest tests/
+```
+
+Run tests for a specific module:
+```bash
+pytest tests/core/          # Core module tests
+pytest tests/loader/        # Loader tests
+pytest tests/analysis/      # Analysis tests
+pytest tests/ui/            # UI tests
+```
+
+Run a specific test file:
+```bash
+pytest tests/core/test_mapper.py
+```
+
+Run with coverage:
+```bash
+pytest tests/ --cov=src/clinical_analytics --cov-report=html
+```
+
+## Test Organization Principles
+
+1. **By Function**: Tests are grouped by the functional area they test (core, loader, analysis, ui)
+2. **Comprehensive Coverage**: Each module has tests for:
+   - Basic functionality
+   - Edge cases (nulls, empty data, invalid inputs)
+   - Error handling
+   - Integration scenarios
+3. **Isolated Tests**: Each test is independent and can run in any order
+4. **Fixtures**: Shared test fixtures are in `conftest.py`
+
+## Legacy Tests
+
+Some older test files remain in the root `tests/` directory:
+- `test_registry.py` - Can be removed (moved to `tests/core/test_registry.py`)
+- `test_ui.py` - UI integration tests
+- `test_ui_integration.py` - UI integration tests
+- `test_upload_security.py` - Security tests (functionality moved to `tests/ui/test_user_datasets.py`)
+- `test_upload_security_manual.py` - Manual security tests
+- `test_covid_ms_dataset.py` - Dataset-specific tests
+
+These can be gradually migrated to the new structure or removed if redundant.
+
diff --git a/tests/analysis/__init__.py b/tests/analysis/__init__.py
new file mode 100644
index 0000000..0d69074
--- /dev/null
+++ b/tests/analysis/__init__.py
@@ -0,0 +1,4 @@
+"""
+Analysis module tests.
+"""
+
diff --git a/tests/analysis/test_stats.py b/tests/analysis/test_stats.py
new file mode 100644
index 0000000..3d02054
--- /dev/null
+++ b/tests/analysis/test_stats.py
@@ -0,0 +1,128 @@
+"""
+Tests for statistical analysis module.
+"""
+
+import pytest
+import pandas as pd
+import numpy as np
+from clinical_analytics.analysis.stats import run_logistic_regression
+
+
+class TestStats:
+    """Test suite for statistical analysis functions."""
+
+    def test_run_logistic_regression(self):
+        """Test running logistic regression."""
+        # Create test data
+        np.random.seed(42)
+        n = 100
+        df = pd.DataFrame({
+            'outcome': np.random.binomial(1, 0.3, n),
+            'predictor1': np.random.normal(0, 1, n),
+            'predictor2': np.random.normal(0, 1, n)
+        })
+
+        model, summary_df = run_logistic_regression(
+            df,
+            outcome_col='outcome',
+            predictors=['predictor1', 'predictor2']
+        )
+
+        assert model is not None
+        assert isinstance(summary_df, pd.DataFrame)
+        assert 'Odds Ratio' in summary_df.columns
+        assert 'CI Lower' in summary_df.columns
+        assert 'CI Upper' in summary_df.columns
+        assert 'P-Value' in summary_df.columns
+
+    def test_run_logistic_regression_single_predictor(self):
+        """Test logistic regression with single predictor."""
+        df = pd.DataFrame({
+            'outcome': [0, 1, 0, 1, 0, 1],
+            'age': [20, 30, 40, 50, 60, 70]
+        })
+
+        model, summary_df = run_logistic_regression(
+            df,
+            outcome_col='outcome',
+            predictors=['age']
+        )
+
+        assert model is not None
+        assert len(summary_df) == 2  # Intercept + age
+
+    def test_run_logistic_regression_with_nulls(self):
+        """Test logistic regression handles nulls correctly."""
+        df = pd.DataFrame({
+            'outcome': [0, 1, 0, 1, None, 1],
+            'predictor1': [1, 2, None, 4, 5, 6],
+            'predictor2': [10, 20, 30, None, 50, 60]
+        })
+
+        # Should drop rows with nulls
+        model, summary_df = run_logistic_regression(
+            df,
+            outcome_col='outcome',
+            predictors=['predictor1', 'predictor2']
+        )
+
+        assert model is not None
+        assert isinstance(summary_df, pd.DataFrame)
+
+    def test_run_logistic_regression_all_nulls(self):
+        """Test logistic regression with all nulls raises error."""
+        df = pd.DataFrame({
+            'outcome': [None, None, None],
+            'predictor1': [1, 2, 3]
+        })
+
+        with pytest.raises(ValueError, match="No data remaining"):
+            run_logistic_regression(
+                df,
+                outcome_col='outcome',
+                predictors=['predictor1']
+            )
+
+    def test_run_logistic_regression_empty_dataframe(self):
+        """Test logistic regression with empty dataframe raises error."""
+        df = pd.DataFrame({
+            'outcome': [],
+            'predictor1': []
+        })
+
+        with pytest.raises(ValueError, match="No data remaining"):
+            run_logistic_regression(
+                df,
+                outcome_col='outcome',
+                predictors=['predictor1']
+            )
+
+    def test_run_logistic_regression_results_format(self):
+        """Test that results are properly formatted."""
+        df = pd.DataFrame({
+            'outcome': [0, 1, 0, 1, 0, 1, 0, 1],
+            'age': [20, 30, 40, 50, 60, 70, 80, 90],
+            'sex': [0, 1, 0, 1, 0, 1, 0, 1]
+        })
+
+        model, summary_df = run_logistic_regression(
+            df,
+            outcome_col='outcome',
+            predictors=['age', 'sex']
+        )
+
+        # Check all required columns exist
+        assert 'Odds Ratio' in summary_df.columns
+        assert 'CI Lower' in summary_df.columns
+        assert 'CI Upper' in summary_df.columns
+        assert 'P-Value' in summary_df.columns
+
+        # Check values are numeric
+        assert summary_df['Odds Ratio'].dtype in [np.float64, np.float32]
+        assert summary_df['P-Value'].dtype in [np.float64, np.float32]
+
+        # Check values are reasonable
+        assert (summary_df['Odds Ratio'] > 0).all()
+        assert (summary_df['P-Value'] >= 0).all()
+        assert (summary_df['P-Value'] <= 1).all()
+
diff --git a/tests/analysis/test_survival.py b/tests/analysis/test_survival.py
new file mode 100644
index 0000000..ab60d6a
--- /dev/null
+++ b/tests/analysis/test_survival.py
@@ -0,0 +1,263 @@
+"""
+Tests for survival analysis module.
+"""
+
+import pytest
+import pandas as pd
+import numpy as np
+from clinical_analytics.analysis.survival import (
+    run_kaplan_meier,
+    run_cox_regression,
+    run_logrank_test,
+    calculate_median_survival
+)
+
+
+class TestSurvivalAnalysis:
+    """Test suite for survival analysis functions."""
+
+    def test_run_kaplan_meier_single_cohort(self):
+        """Test Kaplan-Meier analysis for single cohort."""
+        # Create test survival data
+        df = pd.DataFrame({
+            'duration': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],
+            'event': [1, 1, 0, 1, 0, 1, 1, 0, 1, 1]
+        })
+
+        kmf, summary = run_kaplan_meier(
+            df,
+            duration_col='duration',
+            event_col='event'
+        )
+
+        assert kmf is not None
+        assert isinstance(summary, pd.DataFrame)
+        assert 'time' in summary.columns
+        assert 'survival_probability' in summary.columns
+        assert 'ci_lower' in summary.columns
+        assert 'ci_upper' in summary.columns
+
+    def test_run_kaplan_meier_stratified(self):
+        """Test Kaplan-Meier analysis with stratification."""
+        df = pd.DataFrame({
+            'duration': [10, 20, 30, 40, 50, 60, 70, 80],
+            'event': [1, 1, 0, 1, 0, 1, 1, 0],
+            'group': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B']
+        })
+
+        kmf, summary = run_kaplan_meier(
+            df,
+            duration_col='duration',
+            event_col='event',
+            group_col='group'
+        )
+
+        assert kmf is not None
+        assert isinstance(summary, pd.DataFrame)
+        assert 'group' in summary.columns
+        assert len(summary['group'].unique()) == 2
+
+    def test_run_kaplan_meier_with_nulls(self):
+        """Test Kaplan-Meier handles nulls correctly."""
+        df = pd.DataFrame({
+            'duration': [10, 20, None, 40, 50],
+            'event': [1, None, 0, 1, 1]
+        })
+
+        kmf, summary = run_kaplan_meier(
+            df,
+            duration_col='duration',
+            event_col='event'
+        )
+
+        assert kmf is not None
+        assert isinstance(summary, pd.DataFrame)
+
+    def test_run_kaplan_meier_all_nulls(self):
+        """Test Kaplan-Meier with all nulls raises error."""
+        df = pd.DataFrame({
+            'duration': [None, None, None],
+            'event': [None, None, None]
+        })
+
+        with pytest.raises(ValueError, match="No data remaining"):
+            run_kaplan_meier(
+                df,
+                duration_col='duration',
+                event_col='event'
+            )
+
+    def test_run_cox_regression(self):
+        """Test Cox regression analysis."""
+        df = pd.DataFrame({
+            'duration': [10, 20, 30, 40, 50, 60, 70, 80],
+            'event': [1, 1, 0, 1, 0, 1, 1, 0],
+            'age': [45, 50, 55, 60, 65, 70, 75, 80],
+            'treatment': [0, 1, 0, 1, 0, 1, 0, 1]
+        })
+
+        cph, summary_df = run_cox_regression(
+            df,
+            duration_col='duration',
+            event_col='event',
+            covariates=['age', 'treatment']
+        )
+
+        assert cph is not None
+        assert isinstance(summary_df, pd.DataFrame)
+        assert 'hazard_ratio' in summary_df.columns
+        assert 'hr_ci_lower' in summary_df.columns
+        assert 'hr_ci_upper' in summary_df.columns
+        assert 'p' in summary_df.columns
+
+    def test_run_cox_regression_categorical(self):
+        """Test Cox regression with categorical variables."""
+        df = pd.DataFrame({
+            'duration': [10, 20, 30, 40, 50],
+            'event': [1, 1, 0, 1, 0],
+            'treatment': ['A', 'B', 'A', 'B', 'A']
+        })
+
+        cph, summary_df = run_cox_regression(
+            df,
+            duration_col='duration',
+            event_col='event',
+            covariates=['treatment']
+        )
+
+        assert cph is not None
+        assert isinstance(summary_df, pd.DataFrame)
+
+    def test_run_cox_regression_with_nulls(self):
+        """Test Cox regression handles nulls correctly."""
+        df = pd.DataFrame({
+            'duration': [10, 20, None, 40, 50],
+            'event': [1, None, 0, 1, 1],
+            'age': [45, 50, 55, None, 60]
+        })
+
+        cph, summary_df = run_cox_regression(
+            df,
+            duration_col='duration',
+            event_col='event',
+            covariates=['age']
+        )
+
+        assert cph is not None
+        assert isinstance(summary_df, pd.DataFrame)
+
+    def test_run_cox_regression_all_nulls(self):
+        """Test Cox regression with all nulls raises error."""
+        df = pd.DataFrame({
+            'duration': [None, None, None],
+            'event': [None, None, None],
+            'age': [None, None, None]
+        })
+
+        with pytest.raises(ValueError, match="No data remaining"):
+            run_cox_regression(
+                df,
+                duration_col='duration',
+                event_col='event',
+                covariates=['age']
+            )
+
+    def test_run_logrank_test_two_groups(self):
+        """Test log-rank test with two groups."""
+        df = pd.DataFrame({
+            'duration': [10, 20, 30, 40, 50, 60, 70, 80],
+            'event': [1, 1, 0, 1, 0, 1, 1, 0],
+            'group': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B']
+        })
+
+        result = run_logrank_test(
+            df,
+            duration_col='duration',
+            event_col='event',
+            group_col='group'
+        )
+
+        assert isinstance(result, dict)
+        assert 'test_statistic' in result
+        assert 'p_value' in result
+        assert 'groups' in result
+        assert result['n_groups'] == 2
+        assert result['test_type'] == 'two_group_logrank'
+
+    def test_run_logrank_test_multiple_groups(self):
+        """Test log-rank test with multiple groups."""
+        df = pd.DataFrame({
+            'duration': [10, 20, 30, 40, 50, 60, 70, 80, 90],
+            'event': [1, 1, 0, 1, 0, 1, 1, 0, 1],
+            'group': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C']
+        })
+
+        result = run_logrank_test(
+            df,
+            duration_col='duration',
+            event_col='event',
+            group_col='group'
+        )
+
+        assert isinstance(result, dict)
+        assert 'test_statistic' in result
+        assert 'p_value' in result
+        assert result['n_groups'] == 3
+        assert result['test_type'] == 'multivariate_logrank'
+        assert 'degrees_of_freedom' in result
+
+    def test_run_logrank_test_single_group(self):
+        """Test log-rank test with single group raises error."""
+        df = pd.DataFrame({
+            'duration': [10, 20, 30],
+            'event': [1, 1, 0],
+            'group': ['A', 'A', 'A']
+        })
+
+        with pytest.raises(ValueError, match="Need at least 2 groups"):
+            run_logrank_test(
+                df,
+                duration_col='duration',
+                event_col='event',
+                group_col='group'
+            )
+
+    def test_calculate_median_survival_single_cohort(self):
+        """Test calculating median survival for single cohort."""
+        df = pd.DataFrame({
+            'duration': [10, 20, 30, 40, 50, 60, 70, 80],
+            'event': [1, 1, 0, 1, 0, 1, 1, 0]
+        })
+
+        result = calculate_median_survival(
+            df,
+            duration_col='duration',
+            event_col='event'
+        )
+
+        assert isinstance(result, pd.DataFrame)
+        assert 'group' in result.columns
+        assert 'median_survival' in result.columns
+        assert 'n' in result.columns
+        assert 'n_events' in result.columns
+
+    def test_calculate_median_survival_stratified(self):
+        """Test calculating median survival with stratification."""
+        df = pd.DataFrame({
+            'duration': [10, 20, 30, 40, 50, 60],
+            'event': [1, 1, 0, 1, 0, 1],
+            'group': ['A', 'A', 'A', 'B', 'B', 'B']
+        })
+
+        result = calculate_median_survival(
+            df,
+            duration_col='duration',
+            event_col='event',
+            group_col='group'
+        )
+
+        assert isinstance(result, pd.DataFrame)
+        assert len(result) == 2  # Two groups
+        assert 'group' in result.columns
+        assert 'median_survival' in result.columns
+
diff --git a/tests/core/__init__.py b/tests/core/__init__.py
new file mode 100644
index 0000000..282707a
--- /dev/null
+++ b/tests/core/__init__.py
@@ -0,0 +1,4 @@
+"""
+Core module tests.
+"""
+
diff --git a/tests/core/test_mapper.py b/tests/core/test_mapper.py
new file mode 100644
index 0000000..35bb3e6
--- /dev/null
+++ b/tests/core/test_mapper.py
@@ -0,0 +1,491 @@
+"""
+Tests for the column mapper module.
+"""
+
+import pytest
+import polars as pl
+from clinical_analytics.core.mapper import ColumnMapper, load_dataset_config, get_global_config
+from clinical_analytics.core.schema import UnifiedCohort
+
+
+class TestColumnMapper:
+    """Test suite for ColumnMapper."""
+
+    def test_mapper_initialization(self):
+        """Test mapper can be initialized with config."""
+        config = load_dataset_config('covid_ms')
+        mapper = ColumnMapper(config)
+
+        assert mapper.config is not None
+        assert mapper.column_mapping is not None
+
+    def test_get_default_predictors(self):
+        """Test getting default predictors from config."""
+        config = load_dataset_config('covid_ms')
+        mapper = ColumnMapper(config)
+
+        predictors = mapper.get_default_predictors()
+
+        assert isinstance(predictors, list)
+        assert len(predictors) > 0
+
+    def test_get_categorical_variables(self):
+        """Test getting categorical variables from config."""
+        config = load_dataset_config('covid_ms')
+        mapper = ColumnMapper(config)
+
+        categoricals = mapper.get_categorical_variables()
+
+        assert isinstance(categoricals, list)
+
+    def test_get_default_outcome(self):
+        """Test getting default outcome from config."""
+        config = load_dataset_config('covid_ms')
+        mapper = ColumnMapper(config)
+
+        outcome = mapper.get_default_outcome()
+
+        assert isinstance(outcome, str)
+        assert len(outcome) > 0
+
+    def test_get_default_filters(self):
+        """Test getting default filters from config."""
+        config = load_dataset_config('covid_ms')
+        mapper = ColumnMapper(config)
+
+        filters = mapper.get_default_filters()
+
+        assert isinstance(filters, dict)
+
+    def test_apply_outcome_transformations(self):
+        """Test outcome transformations."""
+        config = {
+            'outcomes': {
+                'test_outcome': {
+                    'source_column': 'source',
+                    'type': 'binary',
+                    'mapping': {'yes': 1, 'no': 0}
+                }
+            }
+        }
+        mapper = ColumnMapper(config)
+
+        # Create test dataframe
+        df = pl.DataFrame({
+            'source': ['yes', 'no', 'yes', 'No']
+        })
+
+        result = mapper.apply_outcome_transformations(df)
+
+        assert 'test_outcome' in result.columns
+        assert result['test_outcome'].to_list() == [1, 0, 1, 0]
+
+    def test_map_to_unified_cohort(self):
+        """Test mapping to UnifiedCohort schema."""
+        config = {
+            'column_mapping': {
+                'id': 'patient_id',
+                'age': 'age'
+            },
+            'analysis': {
+                'default_outcome': 'outcome'
+            }
+        }
+        mapper = ColumnMapper(config)
+
+        # Create test dataframe
+        df = pl.DataFrame({
+            'id': ['P001', 'P002', 'P003'],
+            'age': [45, 62, 38],
+            'outcome': [1, 0, 1]
+        })
+
+        result = mapper.map_to_unified_cohort(
+            df,
+            time_zero_value="2020-01-01",
+            outcome_col="outcome",
+            outcome_label="test_outcome"
+        )
+
+        assert UnifiedCohort.PATIENT_ID in result.columns
+        assert UnifiedCohort.TIME_ZERO in result.columns
+        assert UnifiedCohort.OUTCOME in result.columns
+        assert UnifiedCohort.OUTCOME_LABEL in result.columns
+
+    def test_apply_filters(self):
+        """Test filter application engine."""
+        config = {
+            'filters': {
+                'confirmed_only': {
+                    'type': 'equals',
+                    'column': 'status',
+                    'description': 'Filter confirmed cases'
+                }
+            }
+        }
+        mapper = ColumnMapper(config)
+
+        # Create test dataframe
+        df = pl.DataFrame({
+            'status': ['yes', 'no', 'yes', 'no'],
+            'age': [25, 30, 35, 40]
+        })
+
+        # Test equals filter
+        result = mapper.apply_filters(df, {'confirmed_only': True})
+        assert len(result) == 2
+        assert result['status'].to_list() == ['yes', 'yes']
+
+        # Test with False
+        result = mapper.apply_filters(df, {'confirmed_only': False})
+        assert len(result) == 2
+        assert result['status'].to_list() == ['no', 'no']
+
+    def test_apply_filters_with_different_types(self):
+        """Test boolean filters with different column types."""
+        # Test with integer column
+        config_int = {
+            'filters': {
+                'active_int': {
+                    'type': 'equals',
+                    'column': 'active'
+                }
+            }
+        }
+        mapper_int = ColumnMapper(config_int)
+        df_int = pl.DataFrame({
+            'active': [1, 0, 1, 0],
+            'id': ['A', 'B', 'C', 'D']
+        })
+
+        result = mapper_int.apply_filters(df_int, {'active_int': True})
+        assert len(result) == 2
+        assert result['active'].to_list() == [1, 1]
+
+        result = mapper_int.apply_filters(df_int, {'active_int': False})
+        assert len(result) == 2
+        assert result['active'].to_list() == [0, 0]
+
+        # Test with boolean column
+        config_bool = {
+            'filters': {
+                'active_bool': {
+                    'type': 'equals',
+                    'column': 'active'
+                }
+            }
+        }
+        mapper_bool = ColumnMapper(config_bool)
+        df_bool = pl.DataFrame({
+            'active': [True, False, True, False],
+            'id': ['A', 'B', 'C', 'D']
+        })
+
+        result = mapper_bool.apply_filters(df_bool, {'active_bool': True})
+        assert len(result) == 2
+        assert result['active'].to_list() == [True, True]
+
+        # Test with float column
+        config_float = {
+            'filters': {
+                'active_float': {
+                    'type': 'equals',
+                    'column': 'active'
+                }
+            }
+        }
+        mapper_float = ColumnMapper(config_float)
+        df_float = pl.DataFrame({
+            'active': [1.0, 0.0, 1.0, 0.0],
+            'id': ['A', 'B', 'C', 'D']
+        })
+
+        result = mapper_float.apply_filters(df_float, {'active_float': True})
+        assert len(result) == 2
+        assert result['active'].to_list() == [1.0, 1.0]
+
+    def test_apply_filters_range(self):
+        """Test range filter."""
+        config = {
+            'filters': {
+                'age_range': {
+                    'type': 'range',
+                    'column': 'age'
+                }
+            }
+        }
+        mapper = ColumnMapper(config)
+
+        df = pl.DataFrame({
+            'age': [20, 30, 40, 50, 60],
+            'id': ['A', 'B', 'C', 'D', 'E']
+        })
+
+        result = mapper.apply_filters(df, {'age_range': {'min': 25, 'max': 45}})
+        assert len(result) == 2
+        assert set(result['age'].to_list()) == {30, 40}
+
+    def test_apply_filters_in(self):
+        """Test 'in' filter."""
+        config = {
+            'filters': {
+                'status_filter': {
+                    'type': 'in',
+                    'column': 'status'
+                }
+            }
+        }
+        mapper = ColumnMapper(config)
+
+        df = pl.DataFrame({
+            'status': ['A', 'B', 'C', 'A', 'B'],
+            'id': ['1', '2', '3', '4', '5']
+        })
+
+        result = mapper.apply_filters(df, {'status_filter': ['A', 'C']})
+        assert len(result) == 3
+        assert set(result['status'].to_list()) == {'A', 'C'}
+
+    def test_apply_filters_exists(self):
+        """Test 'exists' filter."""
+        config = {
+            'filters': {
+                'has_value': {
+                    'type': 'exists',
+                    'column': 'value'
+                }
+            }
+        }
+        mapper = ColumnMapper(config)
+
+        df = pl.DataFrame({
+            'value': [1, None, 3, None, 5],
+            'id': ['A', 'B', 'C', 'D', 'E']
+        })
+
+        result = mapper.apply_filters(df, {'has_value': True})
+        assert len(result) == 3
+        assert result['value'].null_count() == 0
+
+        result = mapper.apply_filters(df, {'has_value': False})
+        assert len(result) == 2
+        assert result['value'].null_count() == 2
+
+    def test_apply_aggregations(self):
+        """Test aggregation engine."""
+        config = {
+            'aggregation': {
+                'static_features': [
+                    {
+                        'column': 'Age',
+                        'method': 'first',
+                        'target': 'age'
+                    },
+                    {
+                        'column': 'Gender',
+                        'method': 'first',
+                        'target': 'gender'
+                    }
+                ],
+                'outcome': {
+                    'column': 'SepsisLabel',
+                    'method': 'max',
+                    'target': 'sepsis_label'
+                }
+            }
+        }
+        mapper = ColumnMapper(config)
+
+        # Create time-series test dataframe
+        df = pl.DataFrame({
+            'patient_id': ['P001', 'P001', 'P001', 'P002', 'P002'],
+            'Age': [45, 45, 45, 62, 62],
+            'Gender': ['M', 'M', 'M', 'F', 'F'],
+            'SepsisLabel': [0, 0, 1, 0, 0]
+        })
+
+        result = mapper.apply_aggregations(df, group_by='patient_id')
+
+        assert len(result) == 2
+        assert 'age' in result.columns
+        assert 'gender' in result.columns
+        assert 'sepsis_label' in result.columns
+        assert 'num_hours' in result.columns
+
+        # Check aggregation results
+        p001 = result.filter(pl.col('patient_id') == 'P001')
+        assert p001['sepsis_label'].item() == 1  # max should be 1
+        assert p001['age'].item() == 45  # first should be 45
+
+    def test_apply_aggregations_all_methods(self):
+        """Test all aggregation methods."""
+        config = {
+            'aggregation': {
+                'static_features': [
+                    {'column': 'val1', 'method': 'mean', 'target': 'mean_val'},
+                    {'column': 'val2', 'method': 'sum', 'target': 'sum_val'},
+                    {'column': 'val3', 'method': 'min', 'target': 'min_val'},
+                    {'column': 'val4', 'method': 'max', 'target': 'max_val'},
+                    {'column': 'val5', 'method': 'last', 'target': 'last_val'},
+                    {'column': 'val6', 'method': 'count', 'target': 'count_val'},
+                ]
+            }
+        }
+        mapper = ColumnMapper(config)
+
+        df = pl.DataFrame({
+            'patient_id': ['P001', 'P001', 'P002', 'P002'],
+            'val1': [10, 20, 30, 40],
+            'val2': [1, 2, 3, 4],
+            'val3': [5, 3, 7, 2],
+            'val4': [1, 5, 2, 8],
+            'val5': [100, 200, 300, 400],
+            'val6': [1, 1, 1, 1],
+        })
+
+        result = mapper.apply_aggregations(df, group_by='patient_id')
+
+        assert 'mean_val' in result.columns
+        assert 'sum_val' in result.columns
+        assert 'min_val' in result.columns
+        assert 'max_val' in result.columns
+        assert 'last_val' in result.columns
+        assert 'count_val' in result.columns
+
+    def test_get_time_zero_value(self):
+        """Test getting time_zero value from config."""
+        # Test with string value
+        config = {
+            'time_zero': '2020-01-01'
+        }
+        mapper = ColumnMapper(config)
+        assert mapper.get_time_zero_value() == '2020-01-01'
+
+        # Test with dict value
+        config = {
+            'time_zero': {
+                'value': '2019-01-01'
+            }
+        }
+        mapper = ColumnMapper(config)
+        assert mapper.get_time_zero_value() == '2019-01-01'
+
+        # Test with missing config
+        config = {}
+        mapper = ColumnMapper(config)
+        assert mapper.get_time_zero_value() is None
+
+    def test_get_default_outcome_label(self):
+        """Test getting default outcome label from config."""
+        config = {
+            'outcome_labels': {
+                'outcome_hospitalized': 'hospitalization',
+                'outcome_icu': 'icu_admission'
+            },
+            'outcomes': {
+                'outcome_hospitalized': {
+                    'source_column': 'source',
+                    'type': 'binary',
+                    'label': 'custom_label'
+                }
+            }
+        }
+        mapper = ColumnMapper(config)
+
+        # Test with outcome_labels mapping
+        label = mapper.get_default_outcome_label('outcome_hospitalized')
+        assert label == 'hospitalization'
+
+        # Test with outcome definition label
+        config2 = {
+            'outcomes': {
+                'test_outcome': {
+                    'source_column': 'source',
+                    'type': 'binary',
+                    'label': 'test_label'
+                }
+            }
+        }
+        mapper2 = ColumnMapper(config2)
+        label = mapper2.get_default_outcome_label('test_outcome')
+        assert label == 'test_label'
+
+        # Test fallback to outcome column name
+        config3 = {}
+        mapper3 = ColumnMapper(config3)
+        label = mapper3.get_default_outcome_label('unknown_outcome')
+        assert label == 'unknown_outcome'
+
+    def test_idempotency(self):
+        """Test that same config produces same result (idempotency)."""
+        config = {
+            'outcomes': {
+                'test_outcome': {
+                    'source_column': 'source',
+                    'type': 'binary',
+                    'mapping': {'yes': 1, 'no': 0}
+                }
+            },
+            'column_mapping': {
+                'id': 'patient_id',
+                'source': 'source'
+            },
+            'analysis': {
+                'default_outcome': 'test_outcome'
+            }
+        }
+        mapper = ColumnMapper(config)
+
+        # Create test dataframe
+        df = pl.DataFrame({
+            'id': ['P001', 'P002', 'P003'],
+            'source': ['yes', 'no', 'yes']
+        })
+
+        # Apply transformations multiple times
+        result1 = mapper.apply_outcome_transformations(df)
+        result2 = mapper.apply_outcome_transformations(df)
+
+        # Results should be identical
+        assert result1.equals(result2)
+
+        # Map to unified cohort multiple times
+        cohort1 = mapper.map_to_unified_cohort(
+            result1,
+            time_zero_value='2020-01-01',
+            outcome_col='test_outcome',
+            outcome_label='test'
+        )
+        cohort2 = mapper.map_to_unified_cohort(
+            result2,
+            time_zero_value='2020-01-01',
+            outcome_col='test_outcome',
+            outcome_label='test'
+        )
+
+        assert cohort1.equals(cohort2)
+
+
+class TestConfigLoading:
+    """Test suite for config loading functions."""
+
+    def test_load_dataset_config(self):
+        """Test loading dataset configuration."""
+        config = load_dataset_config('covid_ms')
+
+        assert isinstance(config, dict)
+        assert 'name' in config or 'display_name' in config
+        assert 'init_params' in config
+
+    def test_load_nonexistent_config(self):
+        """Test loading nonexistent config raises error."""
+        with pytest.raises(KeyError):
+            load_dataset_config('nonexistent_dataset')
+
+    def test_get_global_config(self):
+        """Test loading global configuration."""
+        global_config = get_global_config()
+
+        assert isinstance(global_config, dict)
+        # May be empty if no global config defined
+
diff --git a/tests/core/test_multi_table_handler.py b/tests/core/test_multi_table_handler.py
new file mode 100644
index 0000000..d803e1d
--- /dev/null
+++ b/tests/core/test_multi_table_handler.py
@@ -0,0 +1,881 @@
+"""
+Tests for MultiTableHandler table classification and aggregate-before-join pipeline.
+
+Acceptance criteria for Milestone 1:
+1. Bridge detection on synthetic many-to-many fixture
+2. Byte estimates within sane range on sampled rows
+3. Grain key detection is deterministic
+"""
+
+import polars as pl
+import pytest
+from clinical_analytics.core.multi_table_handler import (
+    MultiTableHandler,
+    TableClassification,
+    TableRelationship
+)
+
+
+class TestTableClassification:
+    """Test suite for Milestone 1: Table Classification System."""
+
+    def test_bridge_detection_on_many_to_many_fixture(self):
+        """
+        M1 Acceptance Test 1: Bridge table identified in synthetic many-to-many fixture.
+
+        Setup:
+        - patients (dimension): unique patient_id
+        - medications (dimension): unique medication_id
+        - patient_medications (bridge): patient_id + medication_id composite unique
+
+        Expected:
+        - patient_medications classified as "bridge"
+        - patients classified as "dimension"
+        - medications classified as "dimension" or "reference"
+        """
+        # Arrange: Create synthetic many-to-many dataset
+        patients = pl.DataFrame({
+            "patient_id": ["P1", "P2", "P3"],
+            "name": ["Alice", "Bob", "Charlie"],
+            "age": [30, 45, 28]
+        })
+
+        medications = pl.DataFrame({
+            "medication_id": ["M1", "M2", "M3"],
+            "drug_name": ["Aspirin", "Metformin", "Lisinopril"],
+            "dosage": ["100mg", "500mg", "10mg"]
+        })
+
+        # Bridge table: many-to-many relationship
+        # - patient_id is NOT unique (P1 has 2 medications)
+        # - medication_id is NOT unique (M1 prescribed to 2 patients)
+        # - BUT composite (patient_id, medication_id) IS unique
+        patient_medications = pl.DataFrame({
+            "patient_id": ["P1", "P1", "P2", "P3"],
+            "medication_id": ["M1", "M2", "M1", "M3"],
+            "start_date": ["2024-01-01", "2024-01-15", "2024-02-01", "2024-03-01"],
+            "dosage_override": [None, "250mg", None, None]
+        })
+
+        tables = {
+            "patients": patients,
+            "medications": medications,
+            "patient_medications": patient_medications
+        }
+
+        # Act: Initialize handler and classify
+        handler = MultiTableHandler(tables)
+        handler.detect_relationships()
+        classifications = handler.classify_tables()
+
+        # Assert: Bridge detection
+        assert "patient_medications" in classifications, "Bridge table should be classified"
+
+        bridge_class = classifications["patient_medications"]
+        assert bridge_class.classification == "bridge", (
+            f"patient_medications should be classified as bridge, got {bridge_class.classification}"
+        )
+
+        # Verify bridge characteristics
+        assert bridge_class.relationship_degree >= 2, (
+            f"Bridge should have 2+ foreign keys, got {bridge_class.relationship_degree}"
+        )
+
+        # Verify patients is dimension
+        assert classifications["patients"].classification == "dimension", (
+            f"patients should be dimension, got {classifications['patients'].classification}"
+        )
+
+        # Verify medications is dimension or reference
+        assert classifications["medications"].classification in ["dimension", "reference"], (
+            f"medications should be dimension or reference, got {classifications['medications'].classification}"
+        )
+
+        handler.close()
+
+    def test_byte_estimates_within_sane_range(self):
+        """
+        M1 Acceptance Test 2: Byte estimates within sane range on sampled rows.
+
+        Expected ranges:
+        - Int64 column (1000 rows): ~8,000 bytes
+        - Utf8 column (1000 rows, avg 10 chars): ~10,000 bytes
+        - Total estimate should be within 2x of theoretical minimum
+        """
+        # Arrange: Create table with known characteristics
+        num_rows = 1000
+        df = (
+            pl.select(idx=pl.int_range(0, num_rows))
+            .with_columns([
+                pl.col("idx").alias("id"),  # Int64: 8 bytes/row
+                pl.concat_str([
+                    pl.lit("Name_"),
+                    pl.col("idx").cast(pl.Utf8).str.zfill(4)
+                ]).alias("name"),  # Utf8: ~9 chars = 9 bytes/row
+                (pl.col("idx") * 1.5).cast(pl.Float64).alias("value"),  # Float64: 8 bytes/row
+            ])
+            .drop("idx")
+        )
+
+        tables = {"test_table": df}
+        handler = MultiTableHandler(tables)
+
+        # Act: Estimate bytes
+        estimated_bytes = handler._estimate_table_bytes(df)
+
+        # Assert: Within reasonable range
+        # Theoretical minimum: (8 + 9 + 8) * 1000 = 25,000 bytes
+        theoretical_min = 25_000
+        theoretical_max = theoretical_min * 2  # Allow 2x overhead
+
+        assert theoretical_min <= estimated_bytes <= theoretical_max, (
+            f"Byte estimate {estimated_bytes:,} outside sane range "
+            f"[{theoretical_min:,}, {theoretical_max:,}]"
+        )
+
+        # Verify estimate is reasonable per row
+        bytes_per_row = estimated_bytes / num_rows
+        assert 20 <= bytes_per_row <= 50, (
+            f"Bytes per row {bytes_per_row:.1f} outside expected range [20, 50]"
+        )
+
+        handler.close()
+
+    def test_grain_key_detection_is_deterministic(self):
+        """
+        M1 Acceptance Test 3: Grain key detection is deterministic.
+
+        Same DataFrame should always return same grain key, regardless of:
+        - Column order
+        - Multiple runs
+        - Data order
+        """
+        # Arrange: Create DataFrame with multiple ID columns
+        df_original = pl.DataFrame({
+            "encounter_id": ["E1", "E2", "E3"],
+            "patient_id": ["P1", "P1", "P2"],
+            "visit_id": ["V1", "V2", "V3"],
+            "name": ["Alice", "Alice", "Bob"]
+        })
+
+        # Create reordered version (different column order)
+        df_reordered = df_original.select(["name", "patient_id", "visit_id", "encounter_id"])
+
+        tables_original = {"test": df_original}
+        tables_reordered = {"test": df_reordered}
+
+        handler_original = MultiTableHandler(tables_original)
+        handler_reordered = MultiTableHandler(tables_reordered)
+
+        # Act: Detect grain key multiple times
+        grain_key_1 = handler_original._detect_grain_key(df_original)
+        grain_key_2 = handler_original._detect_grain_key(df_original)  # Same handler
+        grain_key_3 = handler_reordered._detect_grain_key(df_reordered)  # Different order
+
+        # Assert: Deterministic grain key
+        assert grain_key_1 == grain_key_2, (
+            f"Same DataFrame should return same grain key: {grain_key_1} != {grain_key_2}"
+        )
+
+        assert grain_key_1 == grain_key_3, (
+            f"Column order should not affect grain key: {grain_key_1} != {grain_key_3}"
+        )
+
+        # Verify grain key follows priority rules (patient_id should be chosen)
+        assert grain_key_1 == "patient_id", (
+            f"patient_id should be prioritized, got {grain_key_1}"
+        )
+
+        handler_original.close()
+        handler_reordered.close()
+
+    def test_grain_level_detection(self):
+        """Test grain level detection from grain key names."""
+        # Arrange
+        tables = {"dummy": pl.DataFrame({"id": [1]})}
+        handler = MultiTableHandler(tables)
+
+        # Act & Assert: Patient grain
+        assert handler._detect_grain_level("patient_id") == "patient"
+        assert handler._detect_grain_level("subject_id") == "patient"
+
+        # Admission grain
+        assert handler._detect_grain_level("hadm_id") == "admission"
+        assert handler._detect_grain_level("encounter_id") == "admission"
+        assert handler._detect_grain_level("visit_id") == "admission"
+
+        # Event grain (fallback)
+        assert handler._detect_grain_level("row_id") == "event"
+        assert handler._detect_grain_level("charttime_id") == "event"
+
+        handler.close()
+
+    def test_time_column_detection(self):
+        """Test detection of time columns."""
+        # Arrange: DataFrame with time column
+        df_with_time = pl.DataFrame({
+            "id": [1, 2, 3],
+            "charttime": ["2024-01-01", "2024-01-02", "2024-01-03"],
+            "value": [100, 200, 300]
+        })
+
+        # DataFrame without time column
+        df_no_time = pl.DataFrame({
+            "id": [1, 2, 3],
+            "value": [100, 200, 300]
+        })
+
+        # DataFrame with constant time column (should not detect)
+        df_constant_time = pl.DataFrame({
+            "id": [1, 2, 3],
+            "timestamp": ["2024-01-01", "2024-01-01", "2024-01-01"]
+        })
+
+        tables = {"dummy": pl.DataFrame({"id": [1]})}
+        handler = MultiTableHandler(tables)
+
+        # Act & Assert
+        time_col, has_time = handler._detect_time_column(df_with_time)
+        assert has_time is True
+        assert time_col == "charttime"
+
+        time_col, has_time = handler._detect_time_column(df_no_time)
+        assert has_time is False
+        assert time_col is None
+
+        time_col, has_time = handler._detect_time_column(df_constant_time)
+        assert has_time is False, "Constant time columns should not be detected"
+
+        handler.close()
+
+    def test_classification_rules(self):
+        """Test classification rule priority."""
+        # Arrange
+        patients = pl.DataFrame({
+            "patient_id": ["P1", "P2", "P3"],
+            "age": [30, 45, 28]
+        })
+
+        # High cardinality fact table (make it larger to avoid reference classification)
+        # Need > 10 MB to avoid reference classification
+        # Create enough rows to exceed 10 MB threshold
+        num_vitals_rows = 100_000  # Ensure > 10 MB
+        vitals = (
+            pl.select(idx=pl.int_range(0, num_vitals_rows))
+            .with_columns([
+                pl.concat_str([
+                    pl.lit("P"),
+                    (pl.col("idx") % 100).cast(pl.Utf8)
+                ]).alias("patient_id"),
+                pl.concat_str([
+                    pl.lit("2024-01-"),
+                    ((pl.col("idx") % 30) + 1).cast(pl.Utf8).str.zfill(2)
+                ]).alias("charttime"),
+                (70 + (pl.col("idx") % 30)).alias("heart_rate"),
+            ])
+            .drop("idx")
+        )
+
+        tables = {
+            "patients": patients,
+            "vitals": vitals
+        }
+
+        # Act
+        handler = MultiTableHandler(tables)
+        handler.detect_relationships()
+        classifications = handler.classify_tables(anchor_table="patients")
+
+        # Assert
+        patients_class = classifications["patients"]
+        assert patients_class.classification == "dimension"
+        assert patients_class.is_unique_on_grain is True
+        assert patients_class.cardinality_ratio <= 1.1
+
+        vitals_class = classifications["vitals"]
+        # Small table with high cardinality ‚Üí reference
+        # Large table with high cardinality AND N-side ‚Üí event or fact
+        assert vitals_class.classification in ["event", "fact", "reference"]
+        assert vitals_class.cardinality_ratio > 1.1
+        assert vitals_class.has_time_column is True
+
+        # Verify estimated bytes is reasonable
+        assert vitals_class.estimated_bytes > 0
+
+        handler.close()
+
+    def test_null_rate_calculation(self):
+        """Test null rate calculation in grain key."""
+        # Arrange: DataFrame with NULLs in grain key
+        df_with_nulls = pl.DataFrame({
+            "patient_id": ["P1", "P2", None, "P3", None],
+            "value": [100, 200, 300, 400, 500]
+        })
+
+        tables = {"test": df_with_nulls}
+
+        # Act
+        handler = MultiTableHandler(tables)
+        classifications = handler.classify_tables()
+
+        # Assert
+        test_class = classifications["test"]
+        assert test_class.null_rate_in_grain == 0.4, (  # 2 out of 5 are NULL
+            f"Expected null_rate 0.4, got {test_class.null_rate_in_grain}"
+        )
+
+        handler.close()
+
+
+class TestTableClassificationEdgeCases:
+    """Edge cases for table classification."""
+
+    def test_empty_dataframe(self):
+        """Test classification with empty DataFrame."""
+        # Arrange
+        empty_df = pl.DataFrame({
+            "patient_id": pl.Series([], dtype=pl.Utf8),
+            "value": pl.Series([], dtype=pl.Int64)
+        })
+
+        tables = {"empty": empty_df}
+
+        # Act
+        handler = MultiTableHandler(tables)
+        classifications = handler.classify_tables()
+
+        # Assert: Should handle gracefully
+        if "empty" in classifications:
+            empty_class = classifications["empty"]
+            assert empty_class.estimated_bytes == 0
+
+        handler.close()
+
+    def test_single_row_dataframe(self):
+        """Test classification with single row."""
+        # Arrange
+        single_row = pl.DataFrame({
+            "patient_id": ["P1"],
+            "age": [30]
+        })
+
+        tables = {"single": single_row}
+
+        # Act
+        handler = MultiTableHandler(tables)
+        classifications = handler.classify_tables()
+
+        # Assert
+        single_class = classifications["single"]
+        assert single_class.is_unique_on_grain is True
+        assert single_class.cardinality_ratio == 1.0
+
+        handler.close()
+
+    def test_all_nulls_grain_key(self):
+        """Test classification when grain key is all NULLs."""
+        # Arrange
+        all_nulls = pl.DataFrame({
+            "patient_id": [None, None, None],
+            "value": [100, 200, 300]
+        })
+
+        tables = {"nulls": all_nulls}
+
+        # Act
+        handler = MultiTableHandler(tables)
+        classifications = handler.classify_tables()
+
+        # Assert
+        nulls_class = classifications["nulls"]
+        assert nulls_class.null_rate_in_grain == 1.0
+
+        handler.close()
+
+
+class TestPerformanceOptimizations:
+    """Test suite for performance optimizations (sampling, pattern matching)."""
+
+    def test_grain_key_fallback_prefers_patient_over_event(self):
+        """
+        Acceptance: Table with patient_id and event_id picks patient_id even if event_id is more unique.
+
+        This tests the explicit scoring formula that penalizes row-level IDs (event_id, row_id, uuid).
+        """
+        # Arrange: event_id is perfectly unique (row-level ID), patient_id has duplicates
+        df = pl.DataFrame({
+            "patient_id": ["P1", "P1", "P2", "P2"],  # 2 unique
+            "event_id": ["E1", "E2", "E3", "E4"],    # 4 unique (higher uniqueness!)
+            "value": [100, 200, 300, 400]
+        })
+
+        # Act
+        handler = MultiTableHandler({"test": df})
+        grain_key = handler._detect_grain_key(df)
+
+        # Assert: Should pick patient_id despite event_id being more unique
+        assert grain_key == "patient_id", (
+            f"Expected 'patient_id' (explicit key), got '{grain_key}'"
+        )
+
+        handler.close()
+
+    def test_id_pattern_does_not_match_false_positives(self):
+        """
+        Acceptance: endswith('_id') pattern does not match 'valid', 'fluid', 'paid'.
+
+        This tests the tightened ID pattern matching that requires exact 'id' or endswith('_id').
+        """
+        # Arrange: False positives that end with 'id'
+        df = pl.DataFrame({
+            "valid": [True, False, True],
+            "fluid": [100, 200, 300],
+            "paid": [10.5, 20.5, 30.5],
+            "patient_id": ["P1", "P2", "P3"]
+        })
+
+        handler = MultiTableHandler({"test": df})
+
+        # Act
+        grain_key = handler._detect_grain_key(df)
+
+        # Assert: Should pick patient_id, not any false positives
+        assert grain_key == "patient_id", (
+            f"Expected 'patient_id', got '{grain_key}'"
+        )
+
+        # Verify _is_probably_id_col() rejects false positives
+        assert not handler._is_probably_id_col("valid")
+        assert not handler._is_probably_id_col("fluid")
+        assert not handler._is_probably_id_col("paid")
+        assert handler._is_probably_id_col("patient_id")
+
+        handler.close()
+
+    def test_classification_uses_sampled_helpers_only(self, monkeypatch):
+        """
+        Performance guardrail: Enforce that classification uses _sample_df() and never
+        touches df[...] for uniqueness on original frame.
+
+        This test tracks calls to our own _sample_df() helper to verify sampling is used.
+        """
+        # Arrange: Create large DataFrame
+        n = 100_000
+        large_df = (
+            pl.select(idx=pl.int_range(0, n))
+            .with_columns([
+                pl.concat_str([
+                    pl.lit("P"),
+                    (pl.col("idx") % 1000).cast(pl.Utf8)
+                ]).alias("patient_id"),
+                pl.col("idx").alias("value"),
+            ])
+            .drop("idx")
+        )
+
+        # Track calls to _sample_df()
+        sample_df_calls = []
+        original_sample_df = MultiTableHandler._sample_df
+
+        def tracked_sample_df(self, df, n=10_000):
+            sample_df_calls.append((df.height, n))
+            return original_sample_df(self, df, n)
+
+        monkeypatch.setattr(MultiTableHandler, "_sample_df", tracked_sample_df)
+
+        # Act
+        handler = MultiTableHandler({"large": large_df})
+        handler.classify_tables()
+
+        # Assert: _sample_df() was called (proving we're using sampling)
+        assert len(sample_df_calls) > 0, (
+            "Classification should use _sample_df() for all uniqueness checks"
+        )
+
+        # Verify all samples are bounded
+        for df_height, sample_size in sample_df_calls:
+            assert sample_size <= 10_000, (
+                f"Sample size {sample_size} exceeds bound (df_height={df_height})"
+            )
+
+        handler.close()
+
+    def test_classification_1m_rows_completes_within_3_seconds(self):
+        """
+        Strict acceptance gate: Classifying a 1M-row table must complete within 3 seconds.
+
+        This is a hard performance requirement that ensures sampling is working correctly.
+        """
+        import time
+
+        # Arrange: Create 1M-row table
+        n = 1_000_000
+        large_df = (
+            pl.select(idx=pl.int_range(0, n))
+            .with_columns([
+                pl.concat_str([
+                    pl.lit("P"),
+                    (pl.col("idx") % 1000).cast(pl.Utf8)
+                ]).alias("patient_id"),  # 1000 unique patients
+                pl.concat_str([
+                    pl.lit("E"),
+                    pl.col("idx").cast(pl.Utf8)
+                ]).alias("event_id"),  # 1M unique events (row-level ID)
+                pl.col("idx").alias("value"),
+            ])
+            .drop("idx")
+        )
+
+        handler = MultiTableHandler({"large": large_df})
+
+        # Act: Time classification
+        start = time.perf_counter()
+        handler.classify_tables()
+        elapsed = time.perf_counter() - start
+
+        # Assert: Must complete within 3 seconds
+        assert elapsed < 3.0, (
+            f"Classification of 1M-row table took {elapsed:.3f}s, "
+            f"exceeds 3s bound (sampling may not be working)"
+        )
+
+        # Verify classification worked correctly
+        assert "large" in handler.classifications
+        classification = handler.classifications["large"]
+        assert classification.grain_key == "patient_id", (
+            f"Should pick patient_id over event_id (grain_key={classification.grain_key})"
+        )
+
+        handler.close()
+
+
+class TestAnchorSelection:
+    """Test suite for Milestone 2: Centrality-Based Anchor Selection."""
+
+    def test_never_anchors_on_event_fact_bridge(self):
+        """
+        M2 Acceptance Test 1: Never anchors on {event, fact, bridge} classifications.
+
+        Setup:
+        - patients (dimension): unique patient_id
+        - vitals (event): high cardinality with time column
+        - patient_medications (bridge): many-to-many relationship
+
+        Expected:
+        - Anchor is "patients" (dimension), never vitals or patient_medications
+        """
+        # Arrange
+        patients = pl.DataFrame({
+            "patient_id": ["P1", "P2", "P3"],
+            "age": [30, 45, 28]
+        })
+
+        # Large vitals table (event classification)
+        num_vitals = 100_000
+        vitals = (
+            pl.select(idx=pl.int_range(0, num_vitals))
+            .with_columns([
+                pl.concat_str([
+                    pl.lit("P"),
+                    ((pl.col("idx") % 3) + 1).cast(pl.Utf8)
+                ]).alias("patient_id"),
+                pl.concat_str([
+                    pl.lit("2024-01-"),
+                    ((pl.col("idx") % 30) + 1).cast(pl.Utf8).str.zfill(2)
+                ]).alias("charttime"),
+                (70 + (pl.col("idx") % 30)).alias("heart_rate"),
+            ])
+            .drop("idx")
+        )
+
+        medications = pl.DataFrame({
+            "medication_id": ["M1", "M2", "M3"],
+            "drug_name": ["Aspirin", "Metformin", "Lisinopril"]
+        })
+
+        # Bridge table
+        patient_medications = pl.DataFrame({
+            "patient_id": ["P1", "P1", "P2", "P3"],
+            "medication_id": ["M1", "M2", "M1", "M3"],
+            "start_date": ["2024-01-01", "2024-01-15", "2024-02-01", "2024-03-01"]
+        })
+
+        tables = {
+            "patients": patients,
+            "vitals": vitals,
+            "medications": medications,
+            "patient_medications": patient_medications
+        }
+
+        # Act
+        handler = MultiTableHandler(tables)
+        handler.detect_relationships()
+        anchor = handler._find_anchor_by_centrality()
+
+        # Assert: Never anchor on event, fact, or bridge
+        anchor_class = handler.classifications[anchor]
+        assert anchor_class.classification == "dimension", (
+            f"Anchor '{anchor}' must be dimension, got {anchor_class.classification}"
+        )
+
+        # Verify it's actually patients (the only dimension)
+        assert anchor in ["patients", "medications"], (
+            f"Anchor should be patients or medications (dimensions), got '{anchor}'"
+        )
+
+        handler.close()
+
+    def test_same_input_graph_yields_same_anchor(self):
+        """
+        M2 Acceptance Test 2: Same input graph yields same anchor (determinism).
+
+        Run anchor selection multiple times with same data, verify same result.
+        """
+        # Arrange
+        patients = pl.DataFrame({
+            "patient_id": ["P1", "P2", "P3", "P4"],
+            "age": [30, 45, 28, 55]
+        })
+
+        admissions = pl.DataFrame({
+            "hadm_id": ["H1", "H2", "H3", "H4", "H5"],
+            "patient_id": ["P1", "P1", "P2", "P3", "P4"],
+            "admit_date": ["2024-01-01", "2024-02-01", "2024-01-15", "2024-03-01", "2024-04-01"]
+        })
+
+        tables = {
+            "patients": patients,
+            "admissions": admissions
+        }
+
+        # Act: Run anchor selection multiple times
+        anchors = []
+        for _ in range(5):
+            handler = MultiTableHandler(tables.copy())
+            handler.detect_relationships()
+            anchor = handler._find_anchor_by_centrality()
+            anchors.append(anchor)
+            handler.close()
+
+        # Assert: All anchors should be the same
+        assert len(set(anchors)) == 1, (
+            f"Anchor selection not deterministic: got {set(anchors)}"
+        )
+
+        # Verify it picked patients (patient grain preferred)
+        assert anchors[0] == "patients", (
+            f"Expected 'patients' as anchor, got '{anchors[0]}'"
+        )
+
+    def test_prefers_lower_null_rate_and_smaller_bytes_on_ties(self):
+        """
+        M2 Acceptance Test 3: Prefers lower null-rate and smaller bytes on ties.
+
+        Setup:
+        - dim_a: dimension, 0% nulls, 100 bytes
+        - dim_b: dimension, 20% nulls, 50 bytes
+        - dim_c: dimension, 0% nulls, 200 bytes
+
+        Expected:
+        - Anchor is dim_a (0% nulls wins over dim_b, smaller bytes wins over dim_c)
+        """
+        # Arrange: Three dimension tables with different null rates and sizes
+        # dim_a: 0% nulls, small size
+        dim_a = pl.DataFrame({
+            "patient_id": ["P1", "P2", "P3"],
+            "value_a": [100, 200, 300]
+        })
+
+        # dim_b: 20% nulls (1 out of 5), smaller bytes
+        dim_b = pl.DataFrame({
+            "patient_id": ["P1", "P2", None, "P4", "P5"],
+            "value_b": [10, 20, 30, 40, 50]
+        })
+
+        # dim_c: 0% nulls, larger size (more columns and longer strings)
+        dim_c = pl.DataFrame({
+            "patient_id": ["P1", "P2", "P3"],
+            "col1": ["A" * 100, "B" * 100, "C" * 100],  # Long strings
+            "col2": ["D" * 100, "E" * 100, "F" * 100],
+            "col3": ["G" * 100, "H" * 100, "I" * 100],
+            "col4": ["J" * 100, "K" * 100, "L" * 100],
+            "col5": ["M" * 100, "N" * 100, "O" * 100]
+        })
+
+        tables = {
+            "dim_a": dim_a,
+            "dim_b": dim_b,
+            "dim_c": dim_c
+        }
+
+        # Act
+        handler = MultiTableHandler(tables)
+        # No relationships, so all will have same relationship count
+        handler.detect_relationships()
+        anchor = handler._find_anchor_by_centrality()
+
+        # Assert: Should pick dim_a (0% nulls and smallest among 0% null tables)
+        assert anchor == "dim_a", (
+            f"Expected 'dim_a' as anchor (0% nulls, smallest), got '{anchor}'"
+        )
+
+        # Verify classifications
+        dim_a_class = handler.classifications["dim_a"]
+        dim_b_class = handler.classifications["dim_b"]
+        dim_c_class = handler.classifications["dim_c"]
+
+        # dim_a should have lower null rate than dim_b
+        assert dim_a_class.null_rate_in_grain < dim_b_class.null_rate_in_grain
+
+        # dim_a should have smaller bytes than dim_c
+        assert dim_a_class.estimated_bytes < dim_c_class.estimated_bytes
+
+        handler.close()
+
+    def test_hard_exclusions_no_unique_grain(self):
+        """Test that tables without unique grain keys are excluded from anchor selection."""
+        # Arrange: Only non-unique tables
+        non_unique = pl.DataFrame({
+            "patient_id": ["P1", "P1", "P2", "P2"],  # Not unique
+            "value": [100, 200, 300, 400]
+        })
+
+        tables = {"non_unique": non_unique}
+
+        # Act & Assert: Should raise ValueError
+        handler = MultiTableHandler(tables)
+        handler.detect_relationships()
+
+        with pytest.raises(ValueError, match="No dimension tables found"):
+            handler._find_anchor_by_centrality()
+
+        handler.close()
+
+    def test_hard_exclusions_high_null_rate(self):
+        """Test that tables with >50% NULL rate are excluded."""
+        # Arrange: Table with >50% NULLs
+        high_nulls = pl.DataFrame({
+            "patient_id": ["P1", None, None, None, "P5"],  # 60% nulls
+            "value": [100, 200, 300, 400, 500]
+        })
+
+        tables = {"high_nulls": high_nulls}
+
+        # Act & Assert
+        handler = MultiTableHandler(tables)
+        handler.detect_relationships()
+
+        with pytest.raises(ValueError, match="No suitable anchor table found"):
+            handler._find_anchor_by_centrality()
+
+        handler.close()
+
+
+if __name__ == "__main__":
+    pytest.main([__file__, "-v"])
+
+
+class TestDimensionMart:
+    """Test suite for Milestone 3: Dimension Mart Builder."""
+
+    def test_mart_rowcount_equals_anchor_unique_grain_count(self):
+        """
+        M3 Acceptance Test 1: Mart rowcount equals anchor unique grain count.
+
+        Critical invariant: Joining dimensions should preserve anchor cardinality.
+        """
+        # Arrange: Anchor with 3 unique patients + dimension with patient attributes
+        patients = pl.DataFrame({
+            "patient_id": ["P1", "P2", "P3"],
+            "name": ["Alice", "Bob", "Charlie"],
+            "age": [30, 45, 28]
+        })
+
+        # Dimension table (1:1 relationship with patients)
+        demographics = pl.DataFrame({
+            "patient_id": ["P1", "P2", "P3"],
+            "gender": ["F", "M", "M"],
+            "ethnicity": ["Asian", "White", "Hispanic"]
+        })
+
+        tables = {
+            "patients": patients,
+            "demographics": demographics
+        }
+
+        # Act
+        handler = MultiTableHandler(tables)
+        handler.detect_relationships()
+        mart_lazy = handler._build_dimension_mart(anchor_table="patients")
+        mart = mart_lazy.collect()
+
+        # Assert: Mart should have same row count as anchor
+        assert mart.height == patients.height, (
+            f"Mart rowcount {mart.height} != anchor rowcount {patients.height}"
+        )
+
+        assert mart.height == 3, "Mart should have 3 rows (one per unique patient)"
+
+        # Verify all anchor rows preserved
+        assert set(mart["patient_id"]) == {"P1", "P2", "P3"}
+
+        handler.close()
+
+    def test_no_joins_where_rhs_key_is_non_unique(self):
+        """
+        M3 Acceptance Test 2: No joins where RHS key is non-unique.
+
+        Critical invariant: Dimension mart should reject tables with non-unique join keys
+        to prevent row explosion.
+        """
+        # Arrange
+        patients = pl.DataFrame({
+            "patient_id": ["P1", "P2", "P3"],
+            "age": [30, 45, 28]
+        })
+
+        # Valid dimension (unique patient_id)
+        demographics = pl.DataFrame({
+            "patient_id": ["P1", "P2", "P3"],
+            "gender": ["F", "M", "M"]
+        })
+
+        # Invalid "dimension" (non-unique patient_id - actually a fact table)
+        # This would cause row explosion if joined
+        vitals = pl.DataFrame({
+            "patient_id": ["P1", "P1", "P2", "P2", "P3"],  # Non-unique!
+            "charttime": ["2024-01-01", "2024-01-02", "2024-01-01", "2024-01-03", "2024-01-01"],
+            "heart_rate": [70, 72, 68, 71, 75]
+        })
+
+        tables = {
+            "patients": patients,
+            "demographics": demographics,
+            "vitals": vitals
+        }
+
+        # Act
+        handler = MultiTableHandler(tables)
+        handler.detect_relationships()
+        mart_lazy = handler._build_dimension_mart(anchor_table="patients")
+        mart = mart_lazy.collect()
+
+        # Assert: Mart should have same row count (vitals should be excluded)
+        assert mart.height == patients.height, (
+            f"Mart rowcount {mart.height} != anchor rowcount {patients.height}. "
+            f"Non-unique join key may have caused row explosion!"
+        )
+
+        # Verify vitals was not joined (would appear as vitals_heart_rate column)
+        vitals_columns = [col for col in mart.columns if "heart_rate" in col]
+        assert len(vitals_columns) == 0, (
+            f"Vitals table (non-unique key) should not be joined, "
+            f"found columns: {vitals_columns}"
+        )
+
+        # Verify demographics WAS joined (unique key)
+        assert "gender" in mart.columns or "demographics_gender" in mart.columns, (
+            "Demographics (unique key) should be joined"
+        )
+
+        handler.close()
diff --git a/tests/core/test_profiling.py b/tests/core/test_profiling.py
new file mode 100644
index 0000000..64dad00
--- /dev/null
+++ b/tests/core/test_profiling.py
@@ -0,0 +1,233 @@
+"""
+Tests for data profiling module.
+"""
+
+import pytest
+import pandas as pd
+import polars as pl
+import numpy as np
+from clinical_analytics.core.profiling import DataProfiler
+
+
+class TestDataProfiler:
+    """Test suite for DataProfiler."""
+
+    def test_profiler_initialization_pandas(self):
+        """Test profiler initialization with Pandas DataFrame."""
+        df = pd.DataFrame({
+            'col1': [1, 2, 3],
+            'col2': ['a', 'b', 'c']
+        })
+        profiler = DataProfiler(df)
+
+        assert profiler.data is not None
+        assert isinstance(profiler.data, pd.DataFrame)
+
+    def test_profiler_initialization_polars(self):
+        """Test profiler initialization with Polars DataFrame."""
+        df = pl.DataFrame({
+            'col1': [1, 2, 3],
+            'col2': ['a', 'b', 'c']
+        })
+        profiler = DataProfiler(df)
+
+        assert profiler.data is not None
+        assert isinstance(profiler.data, pd.DataFrame)  # Should convert to pandas
+
+    def test_generate_profile(self):
+        """Test generating complete profile."""
+        df = pd.DataFrame({
+            'numeric': [1, 2, 3, 4, 5],
+            'categorical': ['A', 'B', 'A', 'B', 'A'],
+            'with_nulls': [1, None, 3, None, 5]
+        })
+        profiler = DataProfiler(df)
+        profile = profiler.generate_profile()
+
+        assert 'overview' in profile
+        assert 'missing_data' in profile
+        assert 'numeric_features' in profile
+        assert 'categorical_features' in profile
+        assert 'data_quality' in profile
+
+    def test_profile_overview(self):
+        """Test overview statistics."""
+        df = pd.DataFrame({
+            'col1': [1, 2, 3],
+            'col2': ['a', 'b', 'c']
+        })
+        profiler = DataProfiler(df)
+        overview = profiler._profile_overview()
+
+        assert overview['n_rows'] == 3
+        assert overview['n_columns'] == 2
+        assert 'col1' in overview['column_names']
+        assert 'col2' in overview['column_names']
+        assert 'memory_usage_mb' in overview
+
+    def test_profile_missing_data(self):
+        """Test missing data analysis."""
+        df = pd.DataFrame({
+            'complete': [1, 2, 3],
+            'some_missing': [1, None, 3],
+            'all_missing': [None, None, None]
+        })
+        profiler = DataProfiler(df)
+        missing = profiler._profile_missing_data()
+
+        assert missing['total_missing_cells'] == 4  # 1 + 3
+        assert 'some_missing' in missing['columns_with_missing']
+        assert 'all_missing' in missing['columns_with_missing']
+        assert missing['pct_missing_overall'] > 0
+
+    def test_profile_missing_data_no_missing(self):
+        """Test missing data analysis with no missing values."""
+        df = pd.DataFrame({
+            'col1': [1, 2, 3],
+            'col2': ['a', 'b', 'c']
+        })
+        profiler = DataProfiler(df)
+        missing = profiler._profile_missing_data()
+
+        assert missing['total_missing_cells'] == 0
+        assert missing['pct_missing_overall'] == 0.0
+        assert len(missing['columns_with_missing']) == 0
+
+    def test_profile_numeric_features(self):
+        """Test numeric feature profiling."""
+        df = pd.DataFrame({
+            'numeric': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
+            'with_zeros': [0, 1, 2, 0, 3, 0, 4, 5, 0, 6]
+        })
+        profiler = DataProfiler(df)
+        numeric = profiler._profile_numeric_features()
+
+        assert 'numeric' in numeric
+        assert 'with_zeros' in numeric
+        assert numeric['numeric']['mean'] == 5.5
+        assert numeric['numeric']['min'] == 1
+        assert numeric['numeric']['max'] == 10
+        assert numeric['numeric']['median'] == 5.5
+        assert numeric['with_zeros']['n_zeros'] == 3
+
+    def test_profile_numeric_features_empty(self):
+        """Test numeric profiling with no numeric columns."""
+        df = pd.DataFrame({
+            'text': ['a', 'b', 'c']
+        })
+        profiler = DataProfiler(df)
+        numeric = profiler._profile_numeric_features()
+
+        assert numeric == {}
+
+    def test_profile_categorical_features(self):
+        """Test categorical feature profiling."""
+        df = pd.DataFrame({
+            'category': ['A', 'B', 'A', 'B', 'A', 'C', 'A']
+        })
+        profiler = DataProfiler(df)
+        categorical = profiler._profile_categorical_features()
+
+        assert 'category' in categorical
+        assert categorical['category']['n_unique'] == 3
+        assert categorical['category']['mode'] == 'A'
+        assert categorical['category']['pct_mode'] > 0
+        assert len(categorical['category']['top_values']) > 0
+
+    def test_profile_categorical_features_empty(self):
+        """Test categorical profiling with no categorical columns."""
+        df = pd.DataFrame({
+            'numeric': [1, 2, 3]
+        })
+        profiler = DataProfiler(df)
+        categorical = profiler._profile_categorical_features()
+
+        assert categorical == {}
+
+    def test_profile_data_quality(self):
+        """Test data quality assessment."""
+        df = pd.DataFrame({
+            'good': [1, 2, 3, 4, 5],
+            'high_missing': [1, None, None, None, None],
+            'constant': [1, 1, 1, 1, 1]
+        })
+        profiler = DataProfiler(df)
+        quality = profiler._profile_data_quality()
+
+        assert 'quality_score' in quality
+        assert 'issues' in quality
+        assert 'n_issues' in quality
+        assert 0 <= quality['quality_score'] <= 100
+        assert len(quality['issues']) > 0
+
+    def test_calculate_quality_score(self):
+        """Test quality score calculation."""
+        # Perfect data
+        df_perfect = pd.DataFrame({
+            'col1': [1, 2, 3],
+            'col2': ['a', 'b', 'c']
+        })
+        profiler_perfect = DataProfiler(df_perfect)
+        score_perfect = profiler_perfect._calculate_quality_score()
+
+        # Data with issues
+        df_issues = pd.DataFrame({
+            'col1': [1, None, 3],
+            'col2': ['a', 'a', 'a']
+        })
+        profiler_issues = DataProfiler(df_issues)
+        score_issues = profiler_issues._calculate_quality_score()
+
+        assert score_perfect >= score_issues
+        assert 0 <= score_perfect <= 100
+        assert 0 <= score_issues <= 100
+
+    def test_to_dict(self):
+        """Test converting profile to dictionary."""
+        df = pd.DataFrame({'col': [1, 2, 3]})
+        profiler = DataProfiler(df)
+        profile_dict = profiler.to_dict()
+
+        assert isinstance(profile_dict, dict)
+        assert 'overview' in profile_dict
+
+    def test_to_html(self):
+        """Test generating HTML report."""
+        df = pd.DataFrame({
+            'numeric': [1, 2, 3],
+            'category': ['A', 'B', 'C']
+        })
+        profiler = DataProfiler(df)
+        html = profiler.to_html()
+
+        assert isinstance(html, str)
+        assert '<html>' in html
+        assert 'Data Profile Report' in html
+        assert 'Overview' in html
+
+    def test_profile_with_duplicates(self):
+        """Test profiling with duplicate rows."""
+        df = pd.DataFrame({
+            'col1': [1, 2, 1, 2, 3],
+            'col2': ['a', 'b', 'a', 'b', 'c']
+        })
+        profiler = DataProfiler(df)
+        quality = profiler._profile_data_quality()
+
+        # Should detect duplicates
+        duplicate_issues = [issue for issue in quality['issues'] if issue['type'] == 'duplicate_rows']
+        assert len(duplicate_issues) > 0
+
+    def test_profile_with_constant_columns(self):
+        """Test profiling with constant columns."""
+        df = pd.DataFrame({
+            'constant': [1, 1, 1, 1],
+            'variable': [1, 2, 3, 4]
+        })
+        profiler = DataProfiler(df)
+        quality = profiler._profile_data_quality()
+
+        constant_issues = [issue for issue in quality['issues'] if issue['type'] == 'constant_columns']
+        assert len(constant_issues) > 0
+        assert 'constant' in constant_issues[0]['columns']
+
diff --git a/tests/core/test_registry.py b/tests/core/test_registry.py
new file mode 100644
index 0000000..bd40e10
--- /dev/null
+++ b/tests/core/test_registry.py
@@ -0,0 +1,168 @@
+"""
+Tests for the dataset registry module.
+"""
+
+import pytest
+import polars as pl
+from pathlib import Path
+from clinical_analytics.core.registry import DatasetRegistry
+from clinical_analytics.core.dataset import ClinicalDataset
+
+
+class TestDatasetRegistry:
+    """Test suite for DatasetRegistry."""
+
+    def test_discover_datasets(self):
+        """Test that registry discovers datasets."""
+        # Reset registry to ensure clean state
+        DatasetRegistry.reset()
+
+        datasets = DatasetRegistry.discover_datasets()
+
+        assert isinstance(datasets, dict)
+        assert len(datasets) > 0
+        assert 'covid_ms' in datasets or 'sepsis' in datasets
+
+    def test_list_datasets(self):
+        """Test listing available datasets."""
+        DatasetRegistry.reset()
+
+        dataset_list = DatasetRegistry.list_datasets()
+
+        assert isinstance(dataset_list, list)
+        assert len(dataset_list) > 0
+
+    def test_get_dataset_info(self):
+        """Test getting dataset information."""
+        DatasetRegistry.reset()
+        DatasetRegistry.discover_datasets()
+        DatasetRegistry.load_config()
+
+        datasets = DatasetRegistry.list_datasets()
+        if datasets:
+            info = DatasetRegistry.get_dataset_info(datasets[0])
+
+            assert isinstance(info, dict)
+            assert 'name' in info
+            assert 'available' in info
+            assert 'config' in info
+
+    def test_get_dataset_factory(self):
+        """Test factory method for creating datasets."""
+        DatasetRegistry.reset()
+
+        dataset = DatasetRegistry.get_dataset('covid_ms')
+
+        assert isinstance(dataset, ClinicalDataset)
+        assert dataset.name == 'covid_ms'
+
+    def test_get_all_dataset_info(self):
+        """Test getting info for all datasets."""
+        DatasetRegistry.reset()
+
+        all_info = DatasetRegistry.get_all_dataset_info()
+
+        assert isinstance(all_info, dict)
+        assert len(all_info) > 0
+
+        for name, info in all_info.items():
+            assert 'name' in info
+            assert 'config' in info
+
+    def test_reset(self):
+        """Test registry reset functionality."""
+        DatasetRegistry.discover_datasets()
+        DatasetRegistry.load_config()
+
+        assert len(DatasetRegistry._datasets) > 0
+        assert DatasetRegistry._config_loaded
+
+        DatasetRegistry.reset()
+
+        assert len(DatasetRegistry._datasets) == 0
+        assert not DatasetRegistry._config_loaded
+
+    def test_get_nonexistent_dataset(self):
+        """Test that getting nonexistent dataset raises KeyError."""
+        DatasetRegistry.reset()
+        DatasetRegistry.discover_datasets()
+
+        with pytest.raises(KeyError):
+            DatasetRegistry.get_dataset('nonexistent_dataset')
+
+    def test_load_config(self):
+        """Test loading configuration from YAML."""
+        DatasetRegistry.reset()
+        DatasetRegistry.load_config()
+
+        assert DatasetRegistry._config_loaded
+        assert isinstance(DatasetRegistry._configs, dict)
+
+    def test_load_config_nonexistent_file(self):
+        """Test loading config with nonexistent file path."""
+        DatasetRegistry.reset()
+        
+        # Should not raise error, just set empty config
+        DatasetRegistry.load_config(Path('/nonexistent/path/config.yaml'))
+        
+        assert DatasetRegistry._config_loaded
+        assert DatasetRegistry._configs == {}
+
+    def test_register_from_dataframe(self):
+        """Test registering dataset from DataFrame."""
+        DatasetRegistry.reset()
+
+        # Create test DataFrame
+        df = pl.DataFrame({
+            'patient_id': ['P001', 'P002', 'P003'],
+            'age': [45, 62, 38],
+            'outcome': [1, 0, 1]
+        })
+
+        config = DatasetRegistry.register_from_dataframe(
+            'test_dataset',
+            df,
+            display_name='Test Dataset'
+        )
+
+        assert isinstance(config, dict)
+        assert config['name'] == 'test_dataset'
+        assert config['display_name'] == 'Test Dataset'
+        assert config['status'] == 'auto-inferred'
+        assert config['row_count'] == 3
+        assert config['column_count'] == 3
+
+        # Check DataFrame is stored
+        stored_df = DatasetRegistry.get_auto_inferred_dataframe('test_dataset')
+        assert stored_df is not None
+        assert stored_df.height == 3
+
+    def test_get_auto_inferred_dataframe(self):
+        """Test retrieving auto-inferred DataFrame."""
+        DatasetRegistry.reset()
+
+        df = pl.DataFrame({'col1': [1, 2, 3]})
+        DatasetRegistry.register_from_dataframe('test_df', df)
+
+        retrieved = DatasetRegistry.get_auto_inferred_dataframe('test_df')
+        assert retrieved is not None
+        assert retrieved.equals(df)
+
+        # Test nonexistent dataset
+        assert DatasetRegistry.get_auto_inferred_dataframe('nonexistent') is None
+
+    def test_get_dataset_with_override_params(self):
+        """Test getting dataset with override parameters."""
+        DatasetRegistry.reset()
+
+        # Get dataset with override params
+        dataset = DatasetRegistry.get_dataset(
+            'covid_ms',
+            source_path='/custom/path'
+        )
+
+        assert isinstance(dataset, ClinicalDataset)
+        # Override params should be applied
+        if dataset.source_path:
+            assert str(dataset.source_path) == '/custom/path'
+
diff --git a/tests/core/test_schema.py b/tests/core/test_schema.py
new file mode 100644
index 0000000..d9219f4
--- /dev/null
+++ b/tests/core/test_schema.py
@@ -0,0 +1,31 @@
+"""
+Tests for schema module.
+"""
+
+import pytest
+from clinical_analytics.core.schema import UnifiedCohort
+
+
+class TestUnifiedCohort:
+    """Test suite for UnifiedCohort schema."""
+
+    def test_required_columns(self):
+        """Test that required columns are defined."""
+        assert UnifiedCohort.PATIENT_ID in UnifiedCohort.REQUIRED_COLUMNS
+        assert UnifiedCohort.TIME_ZERO in UnifiedCohort.REQUIRED_COLUMNS
+        assert UnifiedCohort.OUTCOME in UnifiedCohort.REQUIRED_COLUMNS
+        assert UnifiedCohort.OUTCOME_LABEL in UnifiedCohort.REQUIRED_COLUMNS
+
+    def test_column_names(self):
+        """Test column name constants."""
+        assert UnifiedCohort.PATIENT_ID == "patient_id"
+        assert UnifiedCohort.TIME_ZERO == "time_zero"
+        assert UnifiedCohort.OUTCOME == "outcome"
+        assert UnifiedCohort.OUTCOME_LABEL == "outcome_label"
+        assert UnifiedCohort.FEATURES_JSON == "features_json"
+
+    def test_required_columns_list(self):
+        """Test that REQUIRED_COLUMNS is a list with all required columns."""
+        assert isinstance(UnifiedCohort.REQUIRED_COLUMNS, list)
+        assert len(UnifiedCohort.REQUIRED_COLUMNS) == 4
+
diff --git a/tests/loader/__init__.py b/tests/loader/__init__.py
new file mode 100644
index 0000000..e551a83
--- /dev/null
+++ b/tests/loader/__init__.py
@@ -0,0 +1,4 @@
+"""
+Loader module tests.
+"""
+
diff --git a/tests/loader/test_covid_ms_loader.py b/tests/loader/test_covid_ms_loader.py
new file mode 100644
index 0000000..770f83c
--- /dev/null
+++ b/tests/loader/test_covid_ms_loader.py
@@ -0,0 +1,85 @@
+"""
+Tests for COVID-MS dataset loader.
+"""
+
+import pytest
+import polars as pl
+from pathlib import Path
+from clinical_analytics.datasets.covid_ms.loader import load_raw_data, clean_data
+from clinical_analytics.core.mapper import ColumnMapper
+
+
+class TestCOVIDMSLoader:
+    """Test suite for COVID-MS loader."""
+
+    def test_load_raw_data(self, tmp_path):
+        """Test loading raw CSV data."""
+        # Create test CSV file
+        test_file = tmp_path / "test_data.csv"
+        test_data = pl.DataFrame({
+            'patient_id': ['P001', 'P002', 'P003'],
+            'age': [45, 62, 38],
+            'sex': ['M', 'F', 'M']
+        })
+        test_data.write_csv(test_file)
+
+        df = load_raw_data(test_file)
+
+        assert isinstance(df, pl.DataFrame)
+        assert len(df) == 3
+        assert 'patient_id' in df.columns
+
+    def test_load_raw_data_nonexistent(self):
+        """Test loading nonexistent file raises error."""
+        with pytest.raises(FileNotFoundError):
+            load_raw_data(Path('/nonexistent/file.csv'))
+
+    def test_clean_data_basic(self):
+        """Test basic data cleaning."""
+        df = pl.DataFrame({
+            'sex': ['M', 'F', None, 'M'],
+            'age': [45, 62, 38, 50]
+        })
+
+        result = clean_data(df)
+
+        assert isinstance(result, pl.DataFrame)
+        # Nulls should be filled
+        assert result['sex'].null_count() == 0
+        assert 'Unknown' in result['sex'].to_list()
+
+    def test_clean_data_with_mapper(self):
+        """Test data cleaning with mapper for outcome transformations."""
+        df = pl.DataFrame({
+            'sex': ['M', 'F', 'M'],
+            'outcome_source': ['yes', 'no', 'yes']
+        })
+
+        config = {
+            'outcomes': {
+                'outcome': {
+                    'source_column': 'outcome_source',
+                    'type': 'binary',
+                    'mapping': {'yes': 1, 'no': 0}
+                }
+            }
+        }
+        mapper = ColumnMapper(config)
+
+        result = clean_data(df, mapper=mapper)
+
+        assert 'outcome' in result.columns
+        assert result['outcome'].to_list() == [1, 0, 1]
+
+    def test_clean_data_without_mapper(self):
+        """Test data cleaning without mapper."""
+        df = pl.DataFrame({
+            'sex': ['M', 'F', None],
+            'age': [45, 62, 38]
+        })
+
+        result = clean_data(df, mapper=None)
+
+        assert isinstance(result, pl.DataFrame)
+        assert result['sex'].null_count() == 0
+
diff --git a/tests/loader/test_mimic3_loader.py b/tests/loader/test_mimic3_loader.py
new file mode 100644
index 0000000..4bdeade
--- /dev/null
+++ b/tests/loader/test_mimic3_loader.py
@@ -0,0 +1,164 @@
+"""
+Tests for MIMIC-III dataset loader.
+"""
+
+import pytest
+import polars as pl
+from pathlib import Path
+import duckdb
+from clinical_analytics.datasets.mimic3.loader import MIMIC3Loader, load_mimic3_from_duckdb
+
+
+class TestMIMIC3Loader:
+    """Test suite for MIMIC-III loader."""
+
+    def test_loader_initialization(self, tmp_path):
+        """Test loader initialization."""
+        db_path = tmp_path / "test.db"
+        loader = MIMIC3Loader(db_path=db_path)
+
+        assert loader.db_path == db_path
+        assert loader.db_connection is None
+        assert loader.conn is None
+
+    def test_loader_initialization_with_connection(self):
+        """Test loader initialization with existing connection."""
+        conn = duckdb.connect(':memory:')
+        loader = MIMIC3Loader(db_connection=conn)
+
+        assert loader.db_connection == conn
+        assert loader.db_path is None
+
+    def test_connect_file(self, tmp_path):
+        """Test connecting to DuckDB file."""
+        db_path = tmp_path / "test.db"
+        conn = duckdb.connect(str(db_path))
+        conn.execute("CREATE TABLE test (id INTEGER)")
+        conn.close()
+
+        loader = MIMIC3Loader(db_path=db_path)
+        loader.connect()
+
+        assert loader.conn is not None
+        loader.disconnect()
+
+    def test_connect_nonexistent_file(self, tmp_path):
+        """Test connecting to nonexistent file raises error."""
+        db_path = tmp_path / "nonexistent.db"
+        loader = MIMIC3Loader(db_path=db_path)
+
+        with pytest.raises(FileNotFoundError):
+            loader.connect()
+
+    def test_connect_no_params(self):
+        """Test connecting without params raises error."""
+        loader = MIMIC3Loader()
+
+        with pytest.raises(ValueError):
+            loader.connect()
+
+    def test_disconnect(self, tmp_path):
+        """Test disconnecting from database."""
+        db_path = tmp_path / "test.db"
+        conn = duckdb.connect(str(db_path))
+        conn.close()
+
+        loader = MIMIC3Loader(db_path=db_path)
+        loader.connect()
+        loader.disconnect()
+
+        assert loader.conn is None
+
+    def test_execute_query(self, tmp_path):
+        """Test executing SQL query."""
+        db_path = tmp_path / "test.db"
+        conn = duckdb.connect(str(db_path))
+        conn.execute("CREATE TABLE test (id INTEGER, name VARCHAR)")
+        conn.execute("INSERT INTO test VALUES (1, 'Alice'), (2, 'Bob')")
+        conn.close()
+
+        loader = MIMIC3Loader(db_path=db_path)
+        result = loader.execute_query("SELECT * FROM test")
+
+        assert isinstance(result, pl.DataFrame)
+        assert len(result) == 2
+        assert 'id' in result.columns
+        assert 'name' in result.columns
+        loader.disconnect()
+
+    def test_load_cohort(self, tmp_path):
+        """Test loading cohort data."""
+        db_path = tmp_path / "test.db"
+        conn = duckdb.connect(str(db_path))
+        conn.execute("CREATE TABLE patients (subject_id INTEGER, age INTEGER)")
+        conn.execute("INSERT INTO patients VALUES (1, 45), (2, 62)")
+        conn.close()
+
+        loader = MIMIC3Loader(db_path=db_path)
+        result = loader.load_cohort("SELECT * FROM patients")
+
+        assert isinstance(result, pl.DataFrame)
+        assert len(result) == 2
+        loader.disconnect()
+
+    def test_load_cohort_empty_query(self, tmp_path):
+        """Test loading cohort with empty query raises error."""
+        db_path = tmp_path / "test.db"
+        conn = duckdb.connect(str(db_path))
+        conn.close()
+
+        loader = MIMIC3Loader(db_path=db_path)
+        loader.connect()
+
+        with pytest.raises(ValueError):
+            loader.load_cohort("")
+
+        with pytest.raises(ValueError):
+            loader.load_cohort(None)
+
+        loader.disconnect()
+
+    def test_check_tables_exist(self, tmp_path):
+        """Test checking table existence."""
+        db_path = tmp_path / "test.db"
+        conn = duckdb.connect(str(db_path))
+        conn.execute("CREATE TABLE patients (id INTEGER)")
+        conn.execute("CREATE TABLE admissions (id INTEGER)")
+        conn.close()
+
+        loader = MIMIC3Loader(db_path=db_path)
+        table_status = loader.check_tables_exist()
+
+        assert isinstance(table_status, dict)
+        assert table_status['patients'] is True
+        assert table_status['admissions'] is False  # Not created
+        loader.disconnect()
+
+    def test_context_manager(self, tmp_path):
+        """Test using loader as context manager."""
+        db_path = tmp_path / "test.db"
+        conn = duckdb.connect(str(db_path))
+        conn.execute("CREATE TABLE test (id INTEGER)")
+        conn.close()
+
+        with MIMIC3Loader(db_path=db_path) as loader:
+            assert loader.conn is not None
+            result = loader.execute_query("SELECT * FROM test")
+            assert isinstance(result, pl.DataFrame)
+
+        # Connection should be closed
+        assert loader.conn is None
+
+    def test_load_mimic3_from_duckdb(self, tmp_path):
+        """Test convenience function for loading from DuckDB."""
+        db_path = tmp_path / "test.db"
+        conn = duckdb.connect(str(db_path))
+        conn.execute("CREATE TABLE test (id INTEGER, value INTEGER)")
+        conn.execute("INSERT INTO test VALUES (1, 10), (2, 20)")
+        conn.close()
+
+        result = load_mimic3_from_duckdb(db_path, "SELECT * FROM test")
+
+        assert isinstance(result, pl.DataFrame)
+        assert len(result) == 2
+
diff --git a/tests/loader/test_sepsis_loader.py b/tests/loader/test_sepsis_loader.py
new file mode 100644
index 0000000..b7d3bfa
--- /dev/null
+++ b/tests/loader/test_sepsis_loader.py
@@ -0,0 +1,107 @@
+"""
+Tests for Sepsis dataset loader.
+"""
+
+import pytest
+import polars as pl
+from pathlib import Path
+from clinical_analytics.datasets.sepsis.loader import (
+    find_psv_files,
+    load_patient_file,
+    load_and_aggregate
+)
+from clinical_analytics.core.mapper import ColumnMapper
+
+
+class TestSepsisLoader:
+    """Test suite for Sepsis loader."""
+
+    def test_find_psv_files(self, tmp_path):
+        """Test finding PSV files recursively."""
+        # Create test directory structure
+        subdir = tmp_path / "subdir"
+        subdir.mkdir()
+
+        # Create test PSV files
+        (tmp_path / "p00001.psv").write_text("col1|col2\n1|2\n")
+        (subdir / "p00002.psv").write_text("col1|col2\n3|4\n")
+
+        files = list(find_psv_files(tmp_path))
+
+        assert len(files) == 2
+        assert all(f.suffix == '.psv' for f in files)
+
+    def test_find_psv_files_none(self, tmp_path):
+        """Test finding PSV files when none exist."""
+        files = list(find_psv_files(tmp_path))
+        assert len(files) == 0
+
+    def test_load_patient_file(self, tmp_path):
+        """Test loading a single PSV file."""
+        test_file = tmp_path / "test.psv"
+        test_file.write_text("Age|Gender|SepsisLabel\n45|M|0\n")
+
+        df = load_patient_file(test_file)
+
+        assert isinstance(df, pl.DataFrame)
+        assert len(df) == 1
+        assert 'Age' in df.columns
+        assert 'Gender' in df.columns
+        assert 'SepsisLabel' in df.columns
+
+    def test_load_and_aggregate(self, tmp_path):
+        """Test loading and aggregating multiple PSV files."""
+        # Create test PSV files
+        (tmp_path / "p00001.psv").write_text("Age|Gender|SepsisLabel\n45|M|0\n45|M|1\n")
+        (tmp_path / "p00002.psv").write_text("Age|Gender|SepsisLabel\n62|F|0\n62|F|0\n")
+
+        config = {
+            'aggregation': {
+                'static_features': [
+                    {'column': 'Age', 'method': 'first', 'target': 'age'},
+                    {'column': 'Gender', 'method': 'first', 'target': 'gender'}
+                ],
+                'outcome': {
+                    'column': 'SepsisLabel',
+                    'method': 'max',
+                    'target': 'sepsis_label'
+                }
+            }
+        }
+        mapper = ColumnMapper(config)
+
+        result = load_and_aggregate(tmp_path, mapper=mapper)
+
+        assert isinstance(result, pl.DataFrame)
+        assert len(result) == 2  # Two patients
+        assert 'patient_id' in result.columns
+        assert 'age' in result.columns
+        assert 'gender' in result.columns
+        assert 'sepsis_label' in result.columns
+
+    def test_load_and_aggregate_no_files(self, tmp_path):
+        """Test loading when no PSV files exist."""
+        with pytest.raises(FileNotFoundError):
+            load_and_aggregate(tmp_path)
+
+    def test_load_and_aggregate_with_limit(self, tmp_path):
+        """Test loading with file limit."""
+        # Create multiple test files
+        for i in range(5):
+            (tmp_path / f"p{i:05d}.psv").write_text("Age|Gender|SepsisLabel\n45|M|0\n")
+
+        result = load_and_aggregate(tmp_path, limit=2)
+
+        # Should only process 2 files
+        assert len(result) == 2
+
+    def test_load_and_aggregate_without_mapper(self, tmp_path):
+        """Test loading without mapper (fallback aggregation)."""
+        (tmp_path / "p00001.psv").write_text("Age|Gender|SepsisLabel\n45|M|0\n45|M|1\n")
+
+        result = load_and_aggregate(tmp_path, mapper=None)
+
+        assert isinstance(result, pl.DataFrame)
+        assert len(result) == 1
+        assert 'patient_id' in result.columns
+
diff --git a/tests/loader/test_zip_extraction.py b/tests/loader/test_zip_extraction.py
new file mode 100644
index 0000000..c869154
--- /dev/null
+++ b/tests/loader/test_zip_extraction.py
@@ -0,0 +1,318 @@
+"""
+Tests for ZIP file extraction and multi-table processing.
+"""
+
+import pytest
+import polars as pl
+import zipfile
+import io
+from pathlib import Path
+from clinical_analytics.ui.storage.user_datasets import UserDatasetStorage, UploadSecurityValidator
+from clinical_analytics.core.multi_table_handler import MultiTableHandler
+
+
+class TestZipExtraction:
+    """Test suite for ZIP file extraction and processing."""
+
+    def test_extract_zip_with_csv_files(self, tmp_path):
+        """Test extracting ZIP file containing multiple CSV files."""
+        # Create test ZIP file
+        zip_buffer = io.BytesIO()
+        
+        with zipfile.ZipFile(zip_buffer, 'w') as zip_file:
+            # Add first CSV
+            zip_file.writestr('patients.csv', 'patient_id,age,sex\nP001,45,M\nP002,62,F\n')
+            # Add second CSV
+            zip_file.writestr('admissions.csv', 'patient_id,admission_date,discharge_date\nP001,2020-01-01,2020-01-05\nP002,2020-02-01,2020-02-10\n')
+            # Add third CSV
+            zip_file.writestr('diagnoses.csv', 'patient_id,icd_code,diagnosis\nP001,E11.9,Diabetes\nP002,I10,Hypertension\n')
+        
+        zip_buffer.seek(0)
+        zip_bytes = zip_buffer.getvalue()
+
+        # Test extraction
+        storage = UserDatasetStorage(upload_dir=tmp_path)
+        success, message, upload_id = storage.save_zip_upload(
+            file_bytes=zip_bytes,
+            original_filename='test_dataset.zip',
+            metadata={'dataset_name': 'test_dataset'}
+        )
+
+        assert success is True
+        assert upload_id is not None
+        assert 'tables' in message.lower() or 'joined' in message.lower()
+
+    def test_extract_zip_with_csv_gz_files(self, tmp_path):
+        """Test extracting ZIP file containing compressed CSV files."""
+        import gzip
+        
+        zip_buffer = io.BytesIO()
+        
+        with zipfile.ZipFile(zip_buffer, 'w') as zip_file:
+            # Create compressed CSV
+            csv_content = 'patient_id,age\nP001,45\nP002,62\n'.encode('utf-8')
+            compressed = gzip.compress(csv_content)
+            zip_file.writestr('patients.csv.gz', compressed)
+            
+            # Add regular CSV
+            zip_file.writestr('admissions.csv', 'patient_id,date\nP001,2020-01-01\n')
+        
+        zip_buffer.seek(0)
+        zip_bytes = zip_buffer.getvalue()
+
+        storage = UserDatasetStorage(base_dir=tmp_path)
+        success, message, upload_id = storage.save_zip_upload(
+            file_bytes=zip_bytes,
+            original_filename='compressed.zip',
+            metadata={'dataset_name': 'compressed'}
+        )
+
+        assert success is True
+        assert upload_id is not None
+
+    def test_extract_zip_with_subdirectories(self, tmp_path):
+        """Test extracting ZIP file with CSV files in subdirectories."""
+        zip_buffer = io.BytesIO()
+        
+        with zipfile.ZipFile(zip_buffer, 'w') as zip_file:
+            zip_file.writestr('mimic-iv/patients.csv', 'patient_id,age\nP001,45\n')
+            zip_file.writestr('mimic-iv/admissions.csv', 'patient_id,date\nP001,2020-01-01\n')
+            zip_file.writestr('mimic-iv/diagnoses.csv', 'patient_id,code\nP001,E11.9\n')
+        
+        zip_buffer.seek(0)
+        zip_bytes = zip_buffer.getvalue()
+
+        storage = UserDatasetStorage(base_dir=tmp_path)
+        success, message, upload_id = storage.save_zip_upload(
+            file_bytes=zip_bytes,
+            original_filename='mimic-iv-clinical-database-demo-2.2.zip',
+            metadata={'dataset_name': 'mimic_iv'}
+        )
+
+        assert success is True
+        assert upload_id is not None
+
+    def test_extract_zip_ignores_macosx(self, tmp_path):
+        """Test that ZIP extraction ignores __MACOSX files."""
+        zip_buffer = io.BytesIO()
+        
+        with zipfile.ZipFile(zip_buffer, 'w') as zip_file:
+            zip_file.writestr('patients.csv', 'patient_id,age\nP001,45\n')
+            zip_file.writestr('__MACOSX/._patients.csv', 'metadata')
+            zip_file.writestr('__MACOSX/.DS_Store', 'metadata')
+        
+        zip_buffer.seek(0)
+        zip_bytes = zip_buffer.getvalue()
+
+        storage = UserDatasetStorage(base_dir=tmp_path)
+        success, message, upload_id = storage.save_zip_upload(
+            file_bytes=zip_bytes,
+            original_filename='test.zip',
+            metadata={'dataset_name': 'test'}
+        )
+
+        assert success is True
+        # Should only process patients.csv, not __MACOSX files
+
+    def test_extract_zip_no_csv_files(self, tmp_path):
+        """Test ZIP file with no CSV files raises error."""
+        zip_buffer = io.BytesIO()
+        
+        with zipfile.ZipFile(zip_buffer, 'w') as zip_file:
+            zip_file.writestr('readme.txt', 'This is a readme file')
+            zip_file.writestr('data.json', '{"key": "value"}')
+        
+        zip_buffer.seek(0)
+        zip_bytes = zip_buffer.getvalue()
+
+        storage = UserDatasetStorage(base_dir=tmp_path)
+        success, message, upload_id = storage.save_zip_upload(
+            file_bytes=zip_bytes,
+            original_filename='no_csv.zip',
+            metadata={'dataset_name': 'test'}
+        )
+
+        assert success is False
+        assert 'no csv files' in message.lower()
+
+    def test_extract_zip_invalid_file(self, tmp_path):
+        """Test handling invalid ZIP file."""
+        invalid_bytes = b'This is not a ZIP file'
+
+        storage = UserDatasetStorage(base_dir=tmp_path)
+        success, message, upload_id = storage.save_zip_upload(
+            file_bytes=invalid_bytes,
+            original_filename='invalid.zip',
+            metadata={'dataset_name': 'test'}
+        )
+
+        assert success is False
+        assert 'error' in message.lower() or 'invalid' in message.lower()
+
+    def test_extract_zip_with_mixed_types(self, tmp_path):
+        """Test ZIP extraction with tables having different key column types (int vs string)."""
+        zip_buffer = io.BytesIO()
+        
+        with zipfile.ZipFile(zip_buffer, 'w') as zip_file:
+            # Table 1: patient_id as integer
+            zip_file.writestr('patients.csv', 'patient_id,age\n1,45\n2,62\n')
+            # Table 2: patient_id as string (should be normalized)
+            zip_file.writestr('admissions.csv', 'patient_id,date\n1,2020-01-01\n2,2020-02-01\n')
+        
+        zip_buffer.seek(0)
+        zip_bytes = zip_buffer.getvalue()
+
+        storage = UserDatasetStorage(base_dir=tmp_path)
+        success, message, upload_id = storage.save_zip_upload(
+            file_bytes=zip_bytes,
+            original_filename='mixed_types.zip',
+            metadata={'dataset_name': 'mixed_types'}
+        )
+
+        # Should succeed despite type differences (normalization should handle it)
+        assert success is True
+
+    def test_extract_zip_large_dataset(self, tmp_path):
+        """Test extracting ZIP with larger dataset (multiple tables, many rows)."""
+        zip_buffer = io.BytesIO()
+        
+        # Create larger dataset
+        patients_data = 'patient_id,age,sex\n' + '\n'.join([f'P{i:03d},{20+i},{["M","F"][i%2]}' for i in range(100)])
+        admissions_data = 'patient_id,admission_date\n' + '\n'.join([f'P{i:03d},2020-01-{1+i%30:02d}' for i in range(100)])
+        
+        with zipfile.ZipFile(zip_buffer, 'w') as zip_file:
+            zip_file.writestr('patients.csv', patients_data)
+            zip_file.writestr('admissions.csv', admissions_data)
+        
+        zip_buffer.seek(0)
+        zip_bytes = zip_buffer.getvalue()
+
+        storage = UserDatasetStorage(base_dir=tmp_path)
+        success, message, upload_id = storage.save_zip_upload(
+            file_bytes=zip_bytes,
+            original_filename='large_dataset.zip',
+            metadata={'dataset_name': 'large'}
+        )
+
+        assert success is True
+        assert upload_id is not None
+
+    def test_extract_zip_creates_unified_cohort(self, tmp_path):
+        """Test that ZIP extraction creates unified cohort with joined tables."""
+        zip_buffer = io.BytesIO()
+        
+        with zipfile.ZipFile(zip_buffer, 'w') as zip_file:
+            zip_file.writestr('patients.csv', 'patient_id,age,sex\nP001,45,M\nP002,62,F\n')
+            zip_file.writestr('admissions.csv', 'patient_id,admission_date\nP001,2020-01-01\nP002,2020-02-01\n')
+        
+        zip_buffer.seek(0)
+        zip_bytes = zip_buffer.getvalue()
+
+        storage = UserDatasetStorage(base_dir=tmp_path)
+        success, message, upload_id = storage.save_zip_upload(
+            file_bytes=zip_bytes,
+            original_filename='test.zip',
+            metadata={'dataset_name': 'test'}
+        )
+
+        assert success is True
+        
+        # Check that unified cohort CSV was created
+        csv_path = tmp_path / 'raw' / f'{upload_id}.csv'
+        assert csv_path.exists()
+        
+        # Load and verify unified cohort
+        unified_df = pl.read_csv(csv_path)
+        assert 'patient_id' in unified_df.columns
+        assert 'age' in unified_df.columns
+        assert 'admission_date' in unified_df.columns
+
+    def test_extract_zip_saves_metadata(self, tmp_path):
+        """Test that ZIP extraction saves proper metadata."""
+        zip_buffer = io.BytesIO()
+        
+        with zipfile.ZipFile(zip_buffer, 'w') as zip_file:
+            zip_file.writestr('patients.csv', 'patient_id,age\nP001,45\nP002,62\n')
+            zip_file.writestr('admissions.csv', 'patient_id,date\nP001,2020-01-01\n')
+        
+        zip_buffer.seek(0)
+        zip_bytes = zip_buffer.getvalue()
+
+        storage = UserDatasetStorage(base_dir=tmp_path)
+        success, message, upload_id = storage.save_zip_upload(
+            file_bytes=zip_bytes,
+            original_filename='test.zip',
+            metadata={'dataset_name': 'test_dataset'}
+        )
+
+        assert success is True
+        
+        # Check metadata file
+        import json
+        metadata_path = tmp_path / 'metadata' / f'{upload_id}.json'
+        assert metadata_path.exists()
+        
+        with open(metadata_path, 'r') as f:
+            metadata = json.load(f)
+        
+        assert metadata['file_format'] == 'zip_multi_table'
+        assert 'tables' in metadata
+        assert 'relationships' in metadata
+        assert 'inferred_schema' in metadata
+        assert len(metadata['tables']) == 2  # patients and admissions
+
+
+class TestMultiTableHandler:
+    """Test suite for MultiTableHandler used in ZIP extraction."""
+
+    def test_detect_relationships_with_type_mismatch(self):
+        """Test relationship detection handles type mismatches (int vs string keys)."""
+        # Create tables with different key types
+        patients = pl.DataFrame({
+            'patient_id': [1, 2, 3],  # Integer
+            'age': [45, 62, 38]
+        })
+        
+        admissions = pl.DataFrame({
+            'patient_id': ['1', '2', '3'],  # String (should be normalized)
+            'admission_date': ['2020-01-01', '2020-02-01', '2020-03-01']
+        })
+        
+        tables = {
+            'patients': patients,
+            'admissions': admissions
+        }
+        
+        handler = MultiTableHandler(tables)
+        relationships = handler.detect_relationships()
+        
+        # Should detect relationship despite type mismatch
+        assert len(relationships) > 0
+        handler.close()
+
+    def test_build_unified_cohort_with_type_mismatch(self):
+        """Test building unified cohort with type mismatches."""
+        patients = pl.DataFrame({
+            'patient_id': [1, 2, 3],
+            'age': [45, 62, 38]
+        })
+        
+        admissions = pl.DataFrame({
+            'patient_id': ['1', '2', '3'],  # String
+            'date': ['2020-01-01', '2020-02-01', '2020-03-01']
+        })
+        
+        tables = {
+            'patients': patients,
+            'admissions': admissions
+        }
+        
+        handler = MultiTableHandler(tables)
+        cohort = handler.build_unified_cohort()
+        
+        assert cohort.height > 0
+        assert 'patient_id' in cohort.columns
+        assert 'age' in cohort.columns
+        assert 'date' in cohort.columns
+        handler.close()
+
diff --git a/tests/ui/__init__.py b/tests/ui/__init__.py
new file mode 100644
index 0000000..f141586
--- /dev/null
+++ b/tests/ui/__init__.py
@@ -0,0 +1,4 @@
+"""
+UI module tests.
+"""
+
diff --git a/tests/ui/test_user_datasets.py b/tests/ui/test_user_datasets.py
new file mode 100644
index 0000000..d9b64c1
--- /dev/null
+++ b/tests/ui/test_user_datasets.py
@@ -0,0 +1,163 @@
+"""
+Tests for user dataset storage.
+"""
+
+import pytest
+import pandas as pd
+import polars as pl
+from pathlib import Path
+from clinical_analytics.ui.storage.user_datasets import (
+    UploadSecurityValidator,
+    UserDatasetStorage
+)
+
+
+class TestUploadSecurityValidator:
+    """Test suite for UploadSecurityValidator."""
+
+    def test_validate_file_type_csv(self):
+        """Test validating CSV file type."""
+        is_valid, error = UploadSecurityValidator.validate_file_type('test.csv')
+        assert is_valid is True
+        assert error == ""
+
+    def test_validate_file_type_xlsx(self):
+        """Test validating XLSX file type."""
+        is_valid, error = UploadSecurityValidator.validate_file_type('test.xlsx')
+        assert is_valid is True
+
+    def test_validate_file_type_invalid(self):
+        """Test validating invalid file type."""
+        is_valid, error = UploadSecurityValidator.validate_file_type('test.exe')
+        assert is_valid is False
+        assert 'not allowed' in error
+
+    def test_validate_file_type_no_extension(self):
+        """Test validating file with no extension."""
+        is_valid, error = UploadSecurityValidator.validate_file_type('test')
+        assert is_valid is False
+        assert 'no extension' in error
+
+    def test_validate_file_size_valid(self):
+        """Test validating file size within limits."""
+        file_bytes = b'x' * (10 * 1024)  # 10KB
+        is_valid, error = UploadSecurityValidator.validate_file_size(file_bytes)
+        assert is_valid is True
+        assert error == ""
+
+    def test_validate_file_size_too_small(self):
+        """Test validating file that's too small."""
+        file_bytes = b'x' * 100  # Less than 1KB
+        is_valid, error = UploadSecurityValidator.validate_file_size(file_bytes)
+        assert is_valid is False
+        assert 'too small' in error
+
+    def test_validate_file_size_too_large(self):
+        """Test validating file that's too large."""
+        file_bytes = b'x' * (101 * 1024 * 1024)  # 101MB
+        is_valid, error = UploadSecurityValidator.validate_file_size(file_bytes)
+        assert is_valid is False
+        assert 'too large' in error
+
+    def test_sanitize_filename(self):
+        """Test filename sanitization."""
+        safe = UploadSecurityValidator.sanitize_filename('test_file.csv')
+        assert safe == 'test_file.csv'
+
+    def test_sanitize_filename_path_traversal(self):
+        """Test sanitizing filename with path traversal."""
+        safe = UploadSecurityValidator.sanitize_filename('../../../etc/passwd')
+        assert '..' not in safe
+        assert '/' not in safe
+
+    def test_sanitize_filename_special_chars(self):
+        """Test sanitizing filename with special characters."""
+        safe = UploadSecurityValidator.sanitize_filename('test@file#name$.csv')
+        assert '@' not in safe
+        assert '#' not in safe
+        assert '$' not in safe
+
+
+class TestUserDatasetStorage:
+    """Test suite for UserDatasetStorage."""
+
+    def test_storage_initialization(self, tmp_path):
+        """Test storage initialization."""
+        storage = UserDatasetStorage(base_dir=tmp_path)
+        assert storage.base_dir == tmp_path
+
+    def test_save_dataset(self, tmp_path):
+        """Test saving a dataset."""
+        storage = UserDatasetStorage(base_dir=tmp_path)
+        
+        df = pd.DataFrame({
+            'patient_id': ['P001', 'P002', 'P003'],
+            'age': [45, 62, 38],
+            'outcome': [1, 0, 1]
+        })
+
+        dataset_id = storage.save_dataset(
+            df,
+            dataset_name='test_dataset',
+            metadata={'description': 'Test dataset'}
+        )
+
+        assert dataset_id is not None
+        assert (tmp_path / 'raw' / f'{dataset_id}.csv').exists()
+
+    def test_load_dataset(self, tmp_path):
+        """Test loading a dataset."""
+        storage = UserDatasetStorage(base_dir=tmp_path)
+        
+        df = pd.DataFrame({
+            'patient_id': ['P001', 'P002'],
+            'age': [45, 62]
+        })
+
+        dataset_id = storage.save_dataset(df, dataset_name='test')
+        loaded_df = storage.load_dataset(dataset_id)
+
+        assert isinstance(loaded_df, pd.DataFrame)
+        assert len(loaded_df) == 2
+        assert 'patient_id' in loaded_df.columns
+
+    def test_list_datasets(self, tmp_path):
+        """Test listing all datasets."""
+        storage = UserDatasetStorage(base_dir=tmp_path)
+        
+        df = pd.DataFrame({'col': [1, 2, 3]})
+        storage.save_dataset(df, dataset_name='dataset1')
+        storage.save_dataset(df, dataset_name='dataset2')
+
+        datasets = storage.list_datasets()
+
+        assert len(datasets) == 2
+        assert all('id' in d for d in datasets)
+        assert all('name' in d for d in datasets)
+
+    def test_get_dataset_metadata(self, tmp_path):
+        """Test getting dataset metadata."""
+        storage = UserDatasetStorage(base_dir=tmp_path)
+        
+        df = pd.DataFrame({'col': [1, 2, 3]})
+        metadata = {'description': 'Test', 'source': 'Manual upload'}
+        dataset_id = storage.save_dataset(df, dataset_name='test', metadata=metadata)
+
+        retrieved_metadata = storage.get_dataset_metadata(dataset_id)
+
+        assert retrieved_metadata['description'] == 'Test'
+        assert retrieved_metadata['source'] == 'Manual upload'
+
+    def test_delete_dataset(self, tmp_path):
+        """Test deleting a dataset."""
+        storage = UserDatasetStorage(base_dir=tmp_path)
+        
+        df = pd.DataFrame({'col': [1, 2, 3]})
+        dataset_id = storage.save_dataset(df, dataset_name='test')
+
+        storage.delete_dataset(dataset_id)
+
+        # Dataset should no longer exist
+        with pytest.raises(FileNotFoundError):
+            storage.load_dataset(dataset_id)
+
diff --git a/tests/ui/test_variable_detector.py b/tests/ui/test_variable_detector.py
new file mode 100644
index 0000000..6f1227e
--- /dev/null
+++ b/tests/ui/test_variable_detector.py
@@ -0,0 +1,121 @@
+"""
+Tests for variable type detector component.
+"""
+
+import pytest
+import pandas as pd
+import numpy as np
+from datetime import datetime
+from clinical_analytics.ui.components.variable_detector import VariableTypeDetector
+
+
+class TestVariableTypeDetector:
+    """Test suite for VariableTypeDetector."""
+
+    def test_detect_binary_yes_no(self):
+        """Test detecting binary variable with yes/no values."""
+        series = pd.Series(['yes', 'no', 'yes', 'no', 'yes'])
+        var_type, metadata = VariableTypeDetector.detect_variable_type(series, 'outcome')
+
+        assert var_type == 'binary'
+        assert 'unique_count' in metadata
+
+    def test_detect_binary_1_0(self):
+        """Test detecting binary variable with 1/0 values."""
+        series = pd.Series([1, 0, 1, 0, 1])
+        var_type, metadata = VariableTypeDetector.detect_variable_type(series, 'status')
+
+        assert var_type == 'binary'
+
+    def test_detect_binary_true_false(self):
+        """Test detecting binary variable with True/False values."""
+        series = pd.Series([True, False, True, False])
+        var_type, metadata = VariableTypeDetector.detect_variable_type(series, 'active')
+
+        assert var_type == 'binary'
+
+    def test_detect_categorical(self):
+        """Test detecting categorical variable."""
+        series = pd.Series(['A', 'B', 'C', 'A', 'B', 'C', 'A'])
+        var_type, metadata = VariableTypeDetector.detect_variable_type(series, 'category')
+
+        assert var_type == 'categorical'
+        assert metadata['unique_count'] == 3
+
+    def test_detect_continuous(self):
+        """Test detecting continuous variable."""
+        series = pd.Series([1.5, 2.3, 3.7, 4.2, 5.1, 6.8, 7.3, 8.9, 9.2, 10.5, 11.3, 12.7])
+        var_type, metadata = VariableTypeDetector.detect_variable_type(series, 'age')
+
+        assert var_type == 'continuous'
+
+    def test_detect_datetime(self):
+        """Test detecting datetime variable."""
+        series = pd.Series([
+            '2020-01-01',
+            '2020-02-01',
+            '2020-03-01',
+            '2020-04-01'
+        ])
+        var_type, metadata = VariableTypeDetector.detect_variable_type(series, 'date')
+
+        assert var_type == 'datetime'
+
+    def test_detect_datetime_parsed(self):
+        """Test detecting datetime with parsed datetime type."""
+        series = pd.Series([
+            datetime(2020, 1, 1),
+            datetime(2020, 2, 1),
+            datetime(2020, 3, 1)
+        ])
+        var_type, metadata = VariableTypeDetector.detect_variable_type(series, 'timestamp')
+
+        assert var_type == 'datetime'
+
+    def test_detect_id_column(self):
+        """Test detecting ID column."""
+        series = pd.Series([f'P{i:05d}' for i in range(100)])
+        var_type, metadata = VariableTypeDetector.detect_variable_type(series, 'patient_id')
+
+        assert var_type == 'id'
+        assert metadata['suggested_as_patient_id'] is True
+
+    def test_detect_with_nulls(self):
+        """Test detection with null values."""
+        series = pd.Series([1, 2, None, 3, None, 4])
+        var_type, metadata = VariableTypeDetector.detect_variable_type(series, 'value')
+
+        # Should still detect correctly despite nulls
+        assert var_type in ['continuous', 'categorical']
+
+    def test_detect_all_nulls(self):
+        """Test detection with all null values."""
+        series = pd.Series([None, None, None])
+        var_type, metadata = VariableTypeDetector.detect_variable_type(series, 'empty')
+
+        # Should handle gracefully
+        assert var_type is not None
+
+    def test_detect_outcome_pattern(self):
+        """Test detecting outcome variable by name pattern."""
+        series = pd.Series([1, 0, 1, 0])
+        var_type, metadata = VariableTypeDetector.detect_variable_type(series, 'outcome_death')
+
+        assert var_type == 'binary'
+
+    def test_detect_time_pattern(self):
+        """Test detecting time variable by name pattern."""
+        series = pd.Series(['2020-01-01', '2020-02-01', '2020-03-01'])
+        var_type, metadata = VariableTypeDetector.detect_variable_type(series, 'admission_date')
+
+        assert var_type == 'datetime'
+
+    def test_detect_categorical_threshold(self):
+        """Test categorical threshold logic."""
+        # Create series with exactly threshold + 1 unique values
+        series = pd.Series([f'cat_{i}' for i in range(21)])  # 21 unique values
+        var_type, metadata = VariableTypeDetector.detect_variable_type(series, 'category')
+
+        # Should be continuous (above threshold) or categorical depending on implementation
+        assert var_type in ['categorical', 'continuous']
+
diff --git a/tests/ui/test_zip_upload_progress.py b/tests/ui/test_zip_upload_progress.py
new file mode 100644
index 0000000..1f430e2
--- /dev/null
+++ b/tests/ui/test_zip_upload_progress.py
@@ -0,0 +1,234 @@
+"""
+Tests for ZIP upload progress callback functionality.
+"""
+
+import io
+import zipfile
+
+from clinical_analytics.ui.storage.user_datasets import UserDatasetStorage
+
+
+class TestZipUploadProgress:
+    """Test suite for progress callback in ZIP upload processing."""
+
+    def test_save_zip_upload_calls_progress_callback(self, tmp_path):
+        """Test that progress callback is called during ZIP upload."""
+        # Create test ZIP file (must be >= 1KB)
+        zip_buffer = io.BytesIO()
+        with zipfile.ZipFile(zip_buffer, 'w') as zip_file:
+            # Create larger CSV files to meet 1KB minimum
+            patients_data = 'patient_id,age\n' + '\n'.join([f'P{i:03d},{20+i}' for i in range(50)])
+            admissions_data = 'patient_id,date\n' + '\n'.join([f'P{i:03d},2020-01-{1+i%30:02d}' for i in range(50)])
+            zip_file.writestr('patients.csv', patients_data)
+            zip_file.writestr('admissions.csv', admissions_data)
+
+        zip_buffer.seek(0)
+        zip_bytes = zip_buffer.getvalue()
+
+        # Track progress calls
+        progress_calls = []
+
+        def progress_callback(step: int, total_steps: int, message: str, details: dict):
+            progress_calls.append({
+                'step': step,
+                'total_steps': total_steps,
+                'message': message,
+                'details': details
+            })
+
+        storage = UserDatasetStorage(upload_dir=tmp_path)
+        success, message, upload_id = storage.save_zip_upload(
+            file_bytes=zip_bytes,
+            original_filename='test.zip',
+            metadata={'dataset_name': 'test'},
+            progress_callback=progress_callback
+        )
+
+        if not success:
+            print(f"Upload failed: {message}")
+        assert success is True, f"Upload failed: {message}"
+        assert len(progress_calls) > 0
+
+        # Verify initial progress call
+        assert any('Initializing' in call['message'] for call in progress_calls)
+
+    def test_progress_callback_receives_table_loading_updates(self, tmp_path):
+        """Test that progress callback receives updates for each table being loaded."""
+        zip_buffer = io.BytesIO()
+        with zipfile.ZipFile(zip_buffer, 'w') as zip_file:
+            # Create larger CSV files to meet 1KB minimum
+            patients_data = 'patient_id,age\n' + '\n'.join([f'P{i:03d},{20+i}' for i in range(50)])
+            admissions_data = 'patient_id,date\n' + '\n'.join([f'P{i:03d},2020-01-{1+i%30:02d}' for i in range(50)])
+            diagnoses_data = 'patient_id,code\n' + '\n'.join([f'P{i:03d},E11.9' for i in range(50)])
+            zip_file.writestr('patients.csv', patients_data)
+            zip_file.writestr('admissions.csv', admissions_data)
+            zip_file.writestr('diagnoses.csv', diagnoses_data)
+
+        zip_buffer.seek(0)
+        zip_bytes = zip_buffer.getvalue()
+
+        table_loading_calls = []
+
+        def progress_callback(step: int, total_steps: int, message: str, details: dict):
+            if 'table_name' in details:
+                table_loading_calls.append({
+                    'table_name': details['table_name'],
+                    'rows': details.get('rows'),
+                    'cols': details.get('cols'),
+                    'message': message
+                })
+
+        storage = UserDatasetStorage(upload_dir=tmp_path)
+        success, _, _ = storage.save_zip_upload(
+            file_bytes=zip_bytes,
+            original_filename='test.zip',
+            metadata={'dataset_name': 'test'},
+            progress_callback=progress_callback
+        )
+
+        assert success is True
+        # Should have received updates for each table
+        assert len(table_loading_calls) >= 3  # At least one per table
+
+        # Verify table names are in the calls
+        table_names = {call['table_name'] for call in table_loading_calls}
+        assert 'patients' in table_names
+        assert 'admissions' in table_names
+        assert 'diagnoses' in table_names
+
+    def test_progress_callback_receives_relationship_detection_update(self, tmp_path):
+        """Test that progress callback receives relationship detection updates."""
+        zip_buffer = io.BytesIO()
+        with zipfile.ZipFile(zip_buffer, 'w') as zip_file:
+            # Create larger CSV files to meet 1KB minimum
+            patients_data = 'patient_id,age\n' + '\n'.join([f'P{i:03d},{20+i}' for i in range(50)])
+            admissions_data = 'patient_id,date\n' + '\n'.join([f'P{i:03d},2020-01-{1+i%30:02d}' for i in range(50)])
+            zip_file.writestr('patients.csv', patients_data)
+            zip_file.writestr('admissions.csv', admissions_data)
+
+        zip_buffer.seek(0)
+        zip_bytes = zip_buffer.getvalue()
+
+        relationship_calls = []
+
+        def progress_callback(step: int, total_steps: int, message: str, details: dict):
+            if 'relationships' in details:
+                relationship_calls.append({
+                    'relationships': details['relationships'],
+                    'message': message
+                })
+
+        storage = UserDatasetStorage(upload_dir=tmp_path)
+        success, _, _ = storage.save_zip_upload(
+            file_bytes=zip_bytes,
+            original_filename='test.zip',
+            metadata={'dataset_name': 'test'},
+            progress_callback=progress_callback
+        )
+
+        assert success is True
+        # Should have received relationship detection update
+        assert len(relationship_calls) > 0
+        assert any('relationships' in call for call in relationship_calls)
+
+    def test_progress_callback_without_callback_works(self, tmp_path):
+        """Test that save_zip_upload works without progress callback."""
+        zip_buffer = io.BytesIO()
+        with zipfile.ZipFile(zip_buffer, 'w') as zip_file:
+            # Create larger CSV file to meet 1KB minimum (need more rows)
+            patients_data = 'patient_id,age\n' + '\n'.join([f'P{i:03d},{20+i}' for i in range(150)])
+            zip_file.writestr('patients.csv', patients_data)
+
+        zip_buffer.seek(0)
+        zip_bytes = zip_buffer.getvalue()
+
+        storage = UserDatasetStorage(upload_dir=tmp_path)
+        success, message, upload_id = storage.save_zip_upload(
+            file_bytes=zip_bytes,
+            original_filename='test.zip',
+            metadata={'dataset_name': 'test'}
+            # No progress_callback provided
+        )
+
+        if not success:
+            print(f"Upload failed: {message}")
+        assert success is True, f"Upload failed: {message}"
+        assert upload_id is not None
+
+    def test_progress_callback_receives_correct_step_counts(self, tmp_path):
+        """Test that progress callback receives correct step and total_steps values."""
+        zip_buffer = io.BytesIO()
+        with zipfile.ZipFile(zip_buffer, 'w') as zip_file:
+            # Create larger CSV files to meet 1KB minimum
+            patients_data = 'patient_id,age\n' + '\n'.join([f'P{i:03d},{20+i}' for i in range(50)])
+            admissions_data = 'patient_id,date\n' + '\n'.join([f'P{i:03d},2020-01-{1+i%30:02d}' for i in range(50)])
+            zip_file.writestr('patients.csv', patients_data)
+            zip_file.writestr('admissions.csv', admissions_data)
+
+        zip_buffer.seek(0)
+        zip_bytes = zip_buffer.getvalue()
+
+        step_values = []
+
+        def progress_callback(step: int, total_steps: int, message: str, details: dict):
+            step_values.append((step, total_steps))
+            # Verify step is always <= total_steps
+            assert step <= total_steps, f"Step {step} exceeds total_steps {total_steps}"
+
+        storage = UserDatasetStorage(upload_dir=tmp_path)
+        success, _, _ = storage.save_zip_upload(
+            file_bytes=zip_bytes,
+            original_filename='test.zip',
+            metadata={'dataset_name': 'test'},
+            progress_callback=progress_callback
+        )
+
+        assert success is True
+        assert len(step_values) > 0
+
+        # Verify steps are monotonically increasing
+        steps = [s[0] for s in step_values]
+        assert steps == sorted(steps), "Steps should be monotonically increasing"
+
+        # Verify all calls use same total_steps
+        total_steps_values = {s[1] for s in step_values}
+        assert len(total_steps_values) == 1, "All calls should use same total_steps"
+
+    def test_progress_callback_receives_table_details(self, tmp_path):
+        """Test that progress callback receives detailed table information."""
+        zip_buffer = io.BytesIO()
+        with zipfile.ZipFile(zip_buffer, 'w') as zip_file:
+            # Create larger CSV file to meet 1KB minimum (need more rows)
+            patients_data = 'patient_id,age,sex\n' + '\n'.join([f'P{i:03d},{20+i},{["M","F"][i%2]}' for i in range(100)])
+            zip_file.writestr('patients.csv', patients_data)
+
+        zip_buffer.seek(0)
+        zip_bytes = zip_buffer.getvalue()
+
+        table_details = []
+
+        def progress_callback(step: int, total_steps: int, message: str, details: dict):
+            if 'table_name' in details and 'rows' in details:
+                table_details.append(details)
+
+        storage = UserDatasetStorage(upload_dir=tmp_path)
+        success, _, _ = storage.save_zip_upload(
+            file_bytes=zip_bytes,
+            original_filename='test.zip',
+            metadata={'dataset_name': 'test'},
+            progress_callback=progress_callback
+        )
+
+        assert success is True
+        assert len(table_details) > 0
+
+        # Verify table details structure
+        for details in table_details:
+            assert 'table_name' in details
+            assert 'rows' in details
+            assert 'cols' in details
+            assert isinstance(details['rows'], int)
+            assert isinstance(details['cols'], int)
+            assert details['rows'] > 0
+            assert details['cols'] > 0
+
diff --git a/uv.lock b/uv.lock
index 8a6374d..83032d4 100644
--- a/uv.lock
+++ b/uv.lock
@@ -13,6 +13,130 @@ members = [
     "docs",
 ]
 
+[[package]]
+name = "aiohappyeyeballs"
+version = "2.6.1"
+source = { registry = "https://pypi.org/simple" }
+sdist = { url = "https://files.pythonhosted.org/packages/26/30/f84a107a9c4331c14b2b586036f40965c128aa4fee4dda5d3d51cb14ad54/aiohappyeyeballs-2.6.1.tar.gz", hash = "sha256:c3f9d0113123803ccadfdf3f0faa505bc78e6a72d1cc4806cbd719826e943558", size = 22760, upload-time = "2025-03-12T01:42:48.764Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/0f/15/5bf3b99495fb160b63f95972b81750f18f7f4e02ad051373b669d17d44f2/aiohappyeyeballs-2.6.1-py3-none-any.whl", hash = "sha256:f349ba8f4b75cb25c99c5c2d84e997e485204d2902a9597802b0371f09331fb8", size = 15265, upload-time = "2025-03-12T01:42:47.083Z" },
+]
+
+[[package]]
+name = "aiohttp"
+version = "3.13.2"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "aiohappyeyeballs" },
+    { name = "aiosignal" },
+    { name = "attrs" },
+    { name = "frozenlist" },
+    { name = "multidict" },
+    { name = "propcache" },
+    { name = "yarl" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/1c/ce/3b83ebba6b3207a7135e5fcaba49706f8a4b6008153b4e30540c982fae26/aiohttp-3.13.2.tar.gz", hash = "sha256:40176a52c186aefef6eb3cad2cdd30cd06e3afbe88fe8ab2af9c0b90f228daca", size = 7837994, upload-time = "2025-10-28T20:59:39.937Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/35/74/b321e7d7ca762638cdf8cdeceb39755d9c745aff7a64c8789be96ddf6e96/aiohttp-3.13.2-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:4647d02df098f6434bafd7f32ad14942f05a9caa06c7016fdcc816f343997dd0", size = 743409, upload-time = "2025-10-28T20:56:00.354Z" },
+    { url = "https://files.pythonhosted.org/packages/99/3d/91524b905ec473beaf35158d17f82ef5a38033e5809fe8742e3657cdbb97/aiohttp-3.13.2-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:e3403f24bcb9c3b29113611c3c16a2a447c3953ecf86b79775e7be06f7ae7ccb", size = 497006, upload-time = "2025-10-28T20:56:01.85Z" },
+    { url = "https://files.pythonhosted.org/packages/eb/d3/7f68bc02a67716fe80f063e19adbd80a642e30682ce74071269e17d2dba1/aiohttp-3.13.2-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:43dff14e35aba17e3d6d5ba628858fb8cb51e30f44724a2d2f0c75be492c55e9", size = 493195, upload-time = "2025-10-28T20:56:03.314Z" },
+    { url = "https://files.pythonhosted.org/packages/98/31/913f774a4708775433b7375c4f867d58ba58ead833af96c8af3621a0d243/aiohttp-3.13.2-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:e2a9ea08e8c58bb17655630198833109227dea914cd20be660f52215f6de5613", size = 1747759, upload-time = "2025-10-28T20:56:04.904Z" },
+    { url = "https://files.pythonhosted.org/packages/e8/63/04efe156f4326f31c7c4a97144f82132c3bb21859b7bb84748d452ccc17c/aiohttp-3.13.2-cp311-cp311-manylinux2014_armv7l.manylinux_2_17_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:53b07472f235eb80e826ad038c9d106c2f653584753f3ddab907c83f49eedead", size = 1704456, upload-time = "2025-10-28T20:56:06.986Z" },
+    { url = "https://files.pythonhosted.org/packages/8e/02/4e16154d8e0a9cf4ae76f692941fd52543bbb148f02f098ca73cab9b1c1b/aiohttp-3.13.2-cp311-cp311-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:e736c93e9c274fce6419af4aac199984d866e55f8a4cec9114671d0ea9688780", size = 1807572, upload-time = "2025-10-28T20:56:08.558Z" },
+    { url = "https://files.pythonhosted.org/packages/34/58/b0583defb38689e7f06798f0285b1ffb3a6fb371f38363ce5fd772112724/aiohttp-3.13.2-cp311-cp311-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:ff5e771f5dcbc81c64898c597a434f7682f2259e0cd666932a913d53d1341d1a", size = 1895954, upload-time = "2025-10-28T20:56:10.545Z" },
+    { url = "https://files.pythonhosted.org/packages/6b/f3/083907ee3437425b4e376aa58b2c915eb1a33703ec0dc30040f7ae3368c6/aiohttp-3.13.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:a3b6fb0c207cc661fa0bf8c66d8d9b657331ccc814f4719468af61034b478592", size = 1747092, upload-time = "2025-10-28T20:56:12.118Z" },
+    { url = "https://files.pythonhosted.org/packages/ac/61/98a47319b4e425cc134e05e5f3fc512bf9a04bf65aafd9fdcda5d57ec693/aiohttp-3.13.2-cp311-cp311-manylinux_2_31_riscv64.manylinux_2_39_riscv64.whl", hash = "sha256:97a0895a8e840ab3520e2288db7cace3a1981300d48babeb50e7425609e2e0ab", size = 1606815, upload-time = "2025-10-28T20:56:14.191Z" },
+    { url = "https://files.pythonhosted.org/packages/97/4b/e78b854d82f66bb974189135d31fce265dee0f5344f64dd0d345158a5973/aiohttp-3.13.2-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:9e8f8afb552297aca127c90cb840e9a1d4bfd6a10d7d8f2d9176e1acc69bad30", size = 1723789, upload-time = "2025-10-28T20:56:16.101Z" },
+    { url = "https://files.pythonhosted.org/packages/ed/fc/9d2ccc794fc9b9acd1379d625c3a8c64a45508b5091c546dea273a41929e/aiohttp-3.13.2-cp311-cp311-musllinux_1_2_armv7l.whl", hash = "sha256:ed2f9c7216e53c3df02264f25d824b079cc5914f9e2deba94155190ef648ee40", size = 1718104, upload-time = "2025-10-28T20:56:17.655Z" },
+    { url = "https://files.pythonhosted.org/packages/66/65/34564b8765ea5c7d79d23c9113135d1dd3609173da13084830f1507d56cf/aiohttp-3.13.2-cp311-cp311-musllinux_1_2_ppc64le.whl", hash = "sha256:99c5280a329d5fa18ef30fd10c793a190d996567667908bef8a7f81f8202b948", size = 1785584, upload-time = "2025-10-28T20:56:19.238Z" },
+    { url = "https://files.pythonhosted.org/packages/30/be/f6a7a426e02fc82781afd62016417b3948e2207426d90a0e478790d1c8a4/aiohttp-3.13.2-cp311-cp311-musllinux_1_2_riscv64.whl", hash = "sha256:2ca6ffef405fc9c09a746cb5d019c1672cd7f402542e379afc66b370833170cf", size = 1595126, upload-time = "2025-10-28T20:56:20.836Z" },
+    { url = "https://files.pythonhosted.org/packages/e5/c7/8e22d5d28f94f67d2af496f14a83b3c155d915d1fe53d94b66d425ec5b42/aiohttp-3.13.2-cp311-cp311-musllinux_1_2_s390x.whl", hash = "sha256:47f438b1a28e926c37632bff3c44df7d27c9b57aaf4e34b1def3c07111fdb782", size = 1800665, upload-time = "2025-10-28T20:56:22.922Z" },
+    { url = "https://files.pythonhosted.org/packages/d1/11/91133c8b68b1da9fc16555706aa7276fdf781ae2bb0876c838dd86b8116e/aiohttp-3.13.2-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:9acda8604a57bb60544e4646a4615c1866ee6c04a8edef9b8ee6fd1d8fa2ddc8", size = 1739532, upload-time = "2025-10-28T20:56:25.924Z" },
+    { url = "https://files.pythonhosted.org/packages/17/6b/3747644d26a998774b21a616016620293ddefa4d63af6286f389aedac844/aiohttp-3.13.2-cp311-cp311-win32.whl", hash = "sha256:868e195e39b24aaa930b063c08bb0c17924899c16c672a28a65afded9c46c6ec", size = 431876, upload-time = "2025-10-28T20:56:27.524Z" },
+    { url = "https://files.pythonhosted.org/packages/c3/63/688462108c1a00eb9f05765331c107f95ae86f6b197b865d29e930b7e462/aiohttp-3.13.2-cp311-cp311-win_amd64.whl", hash = "sha256:7fd19df530c292542636c2a9a85854fab93474396a52f1695e799186bbd7f24c", size = 456205, upload-time = "2025-10-28T20:56:29.062Z" },
+    { url = "https://files.pythonhosted.org/packages/29/9b/01f00e9856d0a73260e86dd8ed0c2234a466c5c1712ce1c281548df39777/aiohttp-3.13.2-cp312-cp312-macosx_10_13_universal2.whl", hash = "sha256:b1e56bab2e12b2b9ed300218c351ee2a3d8c8fdab5b1ec6193e11a817767e47b", size = 737623, upload-time = "2025-10-28T20:56:30.797Z" },
+    { url = "https://files.pythonhosted.org/packages/5a/1b/4be39c445e2b2bd0aab4ba736deb649fabf14f6757f405f0c9685019b9e9/aiohttp-3.13.2-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:364e25edaabd3d37b1db1f0cbcee8c73c9a3727bfa262b83e5e4cf3489a2a9dc", size = 492664, upload-time = "2025-10-28T20:56:32.708Z" },
+    { url = "https://files.pythonhosted.org/packages/28/66/d35dcfea8050e131cdd731dff36434390479b4045a8d0b9d7111b0a968f1/aiohttp-3.13.2-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:c5c94825f744694c4b8db20b71dba9a257cd2ba8e010a803042123f3a25d50d7", size = 491808, upload-time = "2025-10-28T20:56:34.57Z" },
+    { url = "https://files.pythonhosted.org/packages/00/29/8e4609b93e10a853b65f8291e64985de66d4f5848c5637cddc70e98f01f8/aiohttp-3.13.2-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:ba2715d842ffa787be87cbfce150d5e88c87a98e0b62e0f5aa489169a393dbbb", size = 1738863, upload-time = "2025-10-28T20:56:36.377Z" },
+    { url = "https://files.pythonhosted.org/packages/9d/fa/4ebdf4adcc0def75ced1a0d2d227577cd7b1b85beb7edad85fcc87693c75/aiohttp-3.13.2-cp312-cp312-manylinux2014_armv7l.manylinux_2_17_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:585542825c4bc662221fb257889e011a5aa00f1ae4d75d1d246a5225289183e3", size = 1700586, upload-time = "2025-10-28T20:56:38.034Z" },
+    { url = "https://files.pythonhosted.org/packages/da/04/73f5f02ff348a3558763ff6abe99c223381b0bace05cd4530a0258e52597/aiohttp-3.13.2-cp312-cp312-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:39d02cb6025fe1aabca329c5632f48c9532a3dabccd859e7e2f110668972331f", size = 1768625, upload-time = "2025-10-28T20:56:39.75Z" },
+    { url = "https://files.pythonhosted.org/packages/f8/49/a825b79ffec124317265ca7d2344a86bcffeb960743487cb11988ffb3494/aiohttp-3.13.2-cp312-cp312-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:e67446b19e014d37342f7195f592a2a948141d15a312fe0e700c2fd2f03124f6", size = 1867281, upload-time = "2025-10-28T20:56:41.471Z" },
+    { url = "https://files.pythonhosted.org/packages/b9/48/adf56e05f81eac31edcfae45c90928f4ad50ef2e3ea72cb8376162a368f8/aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:4356474ad6333e41ccefd39eae869ba15a6c5299c9c01dfdcfdd5c107be4363e", size = 1752431, upload-time = "2025-10-28T20:56:43.162Z" },
+    { url = "https://files.pythonhosted.org/packages/30/ab/593855356eead019a74e862f21523db09c27f12fd24af72dbc3555b9bfd9/aiohttp-3.13.2-cp312-cp312-manylinux_2_31_riscv64.manylinux_2_39_riscv64.whl", hash = "sha256:eeacf451c99b4525f700f078becff32c32ec327b10dcf31306a8a52d78166de7", size = 1562846, upload-time = "2025-10-28T20:56:44.85Z" },
+    { url = "https://files.pythonhosted.org/packages/39/0f/9f3d32271aa8dc35036e9668e31870a9d3b9542dd6b3e2c8a30931cb27ae/aiohttp-3.13.2-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:d8a9b889aeabd7a4e9af0b7f4ab5ad94d42e7ff679aaec6d0db21e3b639ad58d", size = 1699606, upload-time = "2025-10-28T20:56:46.519Z" },
+    { url = "https://files.pythonhosted.org/packages/2c/3c/52d2658c5699b6ef7692a3f7128b2d2d4d9775f2a68093f74bca06cf01e1/aiohttp-3.13.2-cp312-cp312-musllinux_1_2_armv7l.whl", hash = "sha256:fa89cb11bc71a63b69568d5b8a25c3ca25b6d54c15f907ca1c130d72f320b76b", size = 1720663, upload-time = "2025-10-28T20:56:48.528Z" },
+    { url = "https://files.pythonhosted.org/packages/9b/d4/8f8f3ff1fb7fb9e3f04fcad4e89d8a1cd8fc7d05de67e3de5b15b33008ff/aiohttp-3.13.2-cp312-cp312-musllinux_1_2_ppc64le.whl", hash = "sha256:8aa7c807df234f693fed0ecd507192fc97692e61fee5702cdc11155d2e5cadc8", size = 1737939, upload-time = "2025-10-28T20:56:50.77Z" },
+    { url = "https://files.pythonhosted.org/packages/03/d3/ddd348f8a27a634daae39a1b8e291ff19c77867af438af844bf8b7e3231b/aiohttp-3.13.2-cp312-cp312-musllinux_1_2_riscv64.whl", hash = "sha256:9eb3e33fdbe43f88c3c75fa608c25e7c47bbd80f48d012763cb67c47f39a7e16", size = 1555132, upload-time = "2025-10-28T20:56:52.568Z" },
+    { url = "https://files.pythonhosted.org/packages/39/b8/46790692dc46218406f94374903ba47552f2f9f90dad554eed61bfb7b64c/aiohttp-3.13.2-cp312-cp312-musllinux_1_2_s390x.whl", hash = "sha256:9434bc0d80076138ea986833156c5a48c9c7a8abb0c96039ddbb4afc93184169", size = 1764802, upload-time = "2025-10-28T20:56:54.292Z" },
+    { url = "https://files.pythonhosted.org/packages/ba/e4/19ce547b58ab2a385e5f0b8aa3db38674785085abcf79b6e0edd1632b12f/aiohttp-3.13.2-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:ff15c147b2ad66da1f2cbb0622313f2242d8e6e8f9b79b5206c84523a4473248", size = 1719512, upload-time = "2025-10-28T20:56:56.428Z" },
+    { url = "https://files.pythonhosted.org/packages/70/30/6355a737fed29dcb6dfdd48682d5790cb5eab050f7b4e01f49b121d3acad/aiohttp-3.13.2-cp312-cp312-win32.whl", hash = "sha256:27e569eb9d9e95dbd55c0fc3ec3a9335defbf1d8bc1d20171a49f3c4c607b93e", size = 426690, upload-time = "2025-10-28T20:56:58.736Z" },
+    { url = "https://files.pythonhosted.org/packages/0a/0d/b10ac09069973d112de6ef980c1f6bb31cb7dcd0bc363acbdad58f927873/aiohttp-3.13.2-cp312-cp312-win_amd64.whl", hash = "sha256:8709a0f05d59a71f33fd05c17fc11fcb8c30140506e13c2f5e8ee1b8964e1b45", size = 453465, upload-time = "2025-10-28T20:57:00.795Z" },
+    { url = "https://files.pythonhosted.org/packages/bf/78/7e90ca79e5aa39f9694dcfd74f4720782d3c6828113bb1f3197f7e7c4a56/aiohttp-3.13.2-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:7519bdc7dfc1940d201651b52bf5e03f5503bda45ad6eacf64dda98be5b2b6be", size = 732139, upload-time = "2025-10-28T20:57:02.455Z" },
+    { url = "https://files.pythonhosted.org/packages/db/ed/1f59215ab6853fbaa5c8495fa6cbc39edfc93553426152b75d82a5f32b76/aiohttp-3.13.2-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:088912a78b4d4f547a1f19c099d5a506df17eacec3c6f4375e2831ec1d995742", size = 490082, upload-time = "2025-10-28T20:57:04.784Z" },
+    { url = "https://files.pythonhosted.org/packages/68/7b/fe0fe0f5e05e13629d893c760465173a15ad0039c0a5b0d0040995c8075e/aiohttp-3.13.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:5276807b9de9092af38ed23ce120539ab0ac955547b38563a9ba4f5b07b95293", size = 489035, upload-time = "2025-10-28T20:57:06.894Z" },
+    { url = "https://files.pythonhosted.org/packages/d2/04/db5279e38471b7ac801d7d36a57d1230feeee130bbe2a74f72731b23c2b1/aiohttp-3.13.2-cp313-cp313-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:1237c1375eaef0db4dcd7c2559f42e8af7b87ea7d295b118c60c36a6e61cb811", size = 1720387, upload-time = "2025-10-28T20:57:08.685Z" },
+    { url = "https://files.pythonhosted.org/packages/31/07/8ea4326bd7dae2bd59828f69d7fdc6e04523caa55e4a70f4a8725a7e4ed2/aiohttp-3.13.2-cp313-cp313-manylinux2014_armv7l.manylinux_2_17_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:96581619c57419c3d7d78703d5b78c1e5e5fc0172d60f555bdebaced82ded19a", size = 1688314, upload-time = "2025-10-28T20:57:10.693Z" },
+    { url = "https://files.pythonhosted.org/packages/48/ab/3d98007b5b87ffd519d065225438cc3b668b2f245572a8cb53da5dd2b1bc/aiohttp-3.13.2-cp313-cp313-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:a2713a95b47374169409d18103366de1050fe0ea73db358fc7a7acb2880422d4", size = 1756317, upload-time = "2025-10-28T20:57:12.563Z" },
+    { url = "https://files.pythonhosted.org/packages/97/3d/801ca172b3d857fafb7b50c7c03f91b72b867a13abca982ed6b3081774ef/aiohttp-3.13.2-cp313-cp313-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:228a1cd556b3caca590e9511a89444925da87d35219a49ab5da0c36d2d943a6a", size = 1858539, upload-time = "2025-10-28T20:57:14.623Z" },
+    { url = "https://files.pythonhosted.org/packages/f7/0d/4764669bdf47bd472899b3d3db91fffbe925c8e3038ec591a2fd2ad6a14d/aiohttp-3.13.2-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:ac6cde5fba8d7d8c6ac963dbb0256a9854e9fafff52fbcc58fdf819357892c3e", size = 1739597, upload-time = "2025-10-28T20:57:16.399Z" },
+    { url = "https://files.pythonhosted.org/packages/c4/52/7bd3c6693da58ba16e657eb904a5b6decfc48ecd06e9ac098591653b1566/aiohttp-3.13.2-cp313-cp313-manylinux_2_31_riscv64.manylinux_2_39_riscv64.whl", hash = "sha256:f2bef8237544f4e42878c61cef4e2839fee6346dc60f5739f876a9c50be7fcdb", size = 1555006, upload-time = "2025-10-28T20:57:18.288Z" },
+    { url = "https://files.pythonhosted.org/packages/48/30/9586667acec5993b6f41d2ebcf96e97a1255a85f62f3c653110a5de4d346/aiohttp-3.13.2-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:16f15a4eac3bc2d76c45f7ebdd48a65d41b242eb6c31c2245463b40b34584ded", size = 1683220, upload-time = "2025-10-28T20:57:20.241Z" },
+    { url = "https://files.pythonhosted.org/packages/71/01/3afe4c96854cfd7b30d78333852e8e851dceaec1c40fd00fec90c6402dd2/aiohttp-3.13.2-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:bb7fb776645af5cc58ab804c58d7eba545a97e047254a52ce89c157b5af6cd0b", size = 1712570, upload-time = "2025-10-28T20:57:22.253Z" },
+    { url = "https://files.pythonhosted.org/packages/11/2c/22799d8e720f4697a9e66fd9c02479e40a49de3de2f0bbe7f9f78a987808/aiohttp-3.13.2-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:e1b4951125ec10c70802f2cb09736c895861cd39fd9dcb35107b4dc8ae6220b8", size = 1733407, upload-time = "2025-10-28T20:57:24.37Z" },
+    { url = "https://files.pythonhosted.org/packages/34/cb/90f15dd029f07cebbd91f8238a8b363978b530cd128488085b5703683594/aiohttp-3.13.2-cp313-cp313-musllinux_1_2_riscv64.whl", hash = "sha256:550bf765101ae721ee1d37d8095f47b1f220650f85fe1af37a90ce75bab89d04", size = 1550093, upload-time = "2025-10-28T20:57:26.257Z" },
+    { url = "https://files.pythonhosted.org/packages/69/46/12dce9be9d3303ecbf4d30ad45a7683dc63d90733c2d9fe512be6716cd40/aiohttp-3.13.2-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:fe91b87fc295973096251e2d25a811388e7d8adf3bd2b97ef6ae78bc4ac6c476", size = 1758084, upload-time = "2025-10-28T20:57:28.349Z" },
+    { url = "https://files.pythonhosted.org/packages/f9/c8/0932b558da0c302ffd639fc6362a313b98fdf235dc417bc2493da8394df7/aiohttp-3.13.2-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:e0c8e31cfcc4592cb200160344b2fb6ae0f9e4effe06c644b5a125d4ae5ebe23", size = 1716987, upload-time = "2025-10-28T20:57:30.233Z" },
+    { url = "https://files.pythonhosted.org/packages/5d/8b/f5bd1a75003daed099baec373aed678f2e9b34f2ad40d85baa1368556396/aiohttp-3.13.2-cp313-cp313-win32.whl", hash = "sha256:0740f31a60848d6edb296a0df827473eede90c689b8f9f2a4cdde74889eb2254", size = 425859, upload-time = "2025-10-28T20:57:32.105Z" },
+    { url = "https://files.pythonhosted.org/packages/5d/28/a8a9fc6957b2cee8902414e41816b5ab5536ecf43c3b1843c10e82c559b2/aiohttp-3.13.2-cp313-cp313-win_amd64.whl", hash = "sha256:a88d13e7ca367394908f8a276b89d04a3652044612b9a408a0bb22a5ed976a1a", size = 452192, upload-time = "2025-10-28T20:57:34.166Z" },
+    { url = "https://files.pythonhosted.org/packages/9b/36/e2abae1bd815f01c957cbf7be817b3043304e1c87bad526292a0410fdcf9/aiohttp-3.13.2-cp314-cp314-macosx_10_13_universal2.whl", hash = "sha256:2475391c29230e063ef53a66669b7b691c9bfc3f1426a0f7bcdf1216bdbac38b", size = 735234, upload-time = "2025-10-28T20:57:36.415Z" },
+    { url = "https://files.pythonhosted.org/packages/ca/e3/1ee62dde9b335e4ed41db6bba02613295a0d5b41f74a783c142745a12763/aiohttp-3.13.2-cp314-cp314-macosx_10_13_x86_64.whl", hash = "sha256:f33c8748abef4d8717bb20e8fb1b3e07c6adacb7fd6beaae971a764cf5f30d61", size = 490733, upload-time = "2025-10-28T20:57:38.205Z" },
+    { url = "https://files.pythonhosted.org/packages/1a/aa/7a451b1d6a04e8d15a362af3e9b897de71d86feac3babf8894545d08d537/aiohttp-3.13.2-cp314-cp314-macosx_11_0_arm64.whl", hash = "sha256:ae32f24bbfb7dbb485a24b30b1149e2f200be94777232aeadba3eecece4d0aa4", size = 491303, upload-time = "2025-10-28T20:57:40.122Z" },
+    { url = "https://files.pythonhosted.org/packages/57/1e/209958dbb9b01174870f6a7538cd1f3f28274fdbc88a750c238e2c456295/aiohttp-3.13.2-cp314-cp314-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:5d7f02042c1f009ffb70067326ef183a047425bb2ff3bc434ead4dd4a4a66a2b", size = 1717965, upload-time = "2025-10-28T20:57:42.28Z" },
+    { url = "https://files.pythonhosted.org/packages/08/aa/6a01848d6432f241416bc4866cae8dc03f05a5a884d2311280f6a09c73d6/aiohttp-3.13.2-cp314-cp314-manylinux2014_armv7l.manylinux_2_17_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:93655083005d71cd6c072cdab54c886e6570ad2c4592139c3fb967bfc19e4694", size = 1667221, upload-time = "2025-10-28T20:57:44.869Z" },
+    { url = "https://files.pythonhosted.org/packages/87/4f/36c1992432d31bbc789fa0b93c768d2e9047ec8c7177e5cd84ea85155f36/aiohttp-3.13.2-cp314-cp314-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:0db1e24b852f5f664cd728db140cf11ea0e82450471232a394b3d1a540b0f906", size = 1757178, upload-time = "2025-10-28T20:57:47.216Z" },
+    { url = "https://files.pythonhosted.org/packages/ac/b4/8e940dfb03b7e0f68a82b88fd182b9be0a65cb3f35612fe38c038c3112cf/aiohttp-3.13.2-cp314-cp314-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:b009194665bcd128e23eaddef362e745601afa4641930848af4c8559e88f18f9", size = 1838001, upload-time = "2025-10-28T20:57:49.337Z" },
+    { url = "https://files.pythonhosted.org/packages/d7/ef/39f3448795499c440ab66084a9db7d20ca7662e94305f175a80f5b7e0072/aiohttp-3.13.2-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:c038a8fdc8103cd51dbd986ecdce141473ffd9775a7a8057a6ed9c3653478011", size = 1716325, upload-time = "2025-10-28T20:57:51.327Z" },
+    { url = "https://files.pythonhosted.org/packages/d7/51/b311500ffc860b181c05d91c59a1313bdd05c82960fdd4035a15740d431e/aiohttp-3.13.2-cp314-cp314-manylinux_2_31_riscv64.manylinux_2_39_riscv64.whl", hash = "sha256:66bac29b95a00db411cd758fea0e4b9bdba6d549dfe333f9a945430f5f2cc5a6", size = 1547978, upload-time = "2025-10-28T20:57:53.554Z" },
+    { url = "https://files.pythonhosted.org/packages/31/64/b9d733296ef79815226dab8c586ff9e3df41c6aff2e16c06697b2d2e6775/aiohttp-3.13.2-cp314-cp314-musllinux_1_2_aarch64.whl", hash = "sha256:4ebf9cfc9ba24a74cf0718f04aac2a3bbe745902cc7c5ebc55c0f3b5777ef213", size = 1682042, upload-time = "2025-10-28T20:57:55.617Z" },
+    { url = "https://files.pythonhosted.org/packages/3f/30/43d3e0f9d6473a6db7d472104c4eff4417b1e9df01774cb930338806d36b/aiohttp-3.13.2-cp314-cp314-musllinux_1_2_armv7l.whl", hash = "sha256:a4b88ebe35ce54205c7074f7302bd08a4cb83256a3e0870c72d6f68a3aaf8e49", size = 1680085, upload-time = "2025-10-28T20:57:57.59Z" },
+    { url = "https://files.pythonhosted.org/packages/16/51/c709f352c911b1864cfd1087577760ced64b3e5bee2aa88b8c0c8e2e4972/aiohttp-3.13.2-cp314-cp314-musllinux_1_2_ppc64le.whl", hash = "sha256:98c4fb90bb82b70a4ed79ca35f656f4281885be076f3f970ce315402b53099ae", size = 1728238, upload-time = "2025-10-28T20:57:59.525Z" },
+    { url = "https://files.pythonhosted.org/packages/19/e2/19bd4c547092b773caeb48ff5ae4b1ae86756a0ee76c16727fcfd281404b/aiohttp-3.13.2-cp314-cp314-musllinux_1_2_riscv64.whl", hash = "sha256:ec7534e63ae0f3759df3a1ed4fa6bc8f75082a924b590619c0dd2f76d7043caa", size = 1544395, upload-time = "2025-10-28T20:58:01.914Z" },
+    { url = "https://files.pythonhosted.org/packages/cf/87/860f2803b27dfc5ed7be532832a3498e4919da61299b4a1f8eb89b8ff44d/aiohttp-3.13.2-cp314-cp314-musllinux_1_2_s390x.whl", hash = "sha256:5b927cf9b935a13e33644cbed6c8c4b2d0f25b713d838743f8fe7191b33829c4", size = 1742965, upload-time = "2025-10-28T20:58:03.972Z" },
+    { url = "https://files.pythonhosted.org/packages/67/7f/db2fc7618925e8c7a601094d5cbe539f732df4fb570740be88ed9e40e99a/aiohttp-3.13.2-cp314-cp314-musllinux_1_2_x86_64.whl", hash = "sha256:88d6c017966a78c5265d996c19cdb79235be5e6412268d7e2ce7dee339471b7a", size = 1697585, upload-time = "2025-10-28T20:58:06.189Z" },
+    { url = "https://files.pythonhosted.org/packages/0c/07/9127916cb09bb38284db5036036042b7b2c514c8ebaeee79da550c43a6d6/aiohttp-3.13.2-cp314-cp314-win32.whl", hash = "sha256:f7c183e786e299b5d6c49fb43a769f8eb8e04a2726a2bd5887b98b5cc2d67940", size = 431621, upload-time = "2025-10-28T20:58:08.636Z" },
+    { url = "https://files.pythonhosted.org/packages/fb/41/554a8a380df6d3a2bba8a7726429a23f4ac62aaf38de43bb6d6cde7b4d4d/aiohttp-3.13.2-cp314-cp314-win_amd64.whl", hash = "sha256:fe242cd381e0fb65758faf5ad96c2e460df6ee5b2de1072fe97e4127927e00b4", size = 457627, upload-time = "2025-10-28T20:58:11Z" },
+    { url = "https://files.pythonhosted.org/packages/c7/8e/3824ef98c039d3951cb65b9205a96dd2b20f22241ee17d89c5701557c826/aiohttp-3.13.2-cp314-cp314t-macosx_10_13_universal2.whl", hash = "sha256:f10d9c0b0188fe85398c61147bbd2a657d616c876863bfeff43376e0e3134673", size = 767360, upload-time = "2025-10-28T20:58:13.358Z" },
+    { url = "https://files.pythonhosted.org/packages/a4/0f/6a03e3fc7595421274fa34122c973bde2d89344f8a881b728fa8c774e4f1/aiohttp-3.13.2-cp314-cp314t-macosx_10_13_x86_64.whl", hash = "sha256:e7c952aefdf2460f4ae55c5e9c3e80aa72f706a6317e06020f80e96253b1accd", size = 504616, upload-time = "2025-10-28T20:58:15.339Z" },
+    { url = "https://files.pythonhosted.org/packages/c6/aa/ed341b670f1bc8a6f2c6a718353d13b9546e2cef3544f573c6a1ff0da711/aiohttp-3.13.2-cp314-cp314t-macosx_11_0_arm64.whl", hash = "sha256:c20423ce14771d98353d2e25e83591fa75dfa90a3c1848f3d7c68243b4fbded3", size = 509131, upload-time = "2025-10-28T20:58:17.693Z" },
+    { url = "https://files.pythonhosted.org/packages/7f/f0/c68dac234189dae5c4bbccc0f96ce0cc16b76632cfc3a08fff180045cfa4/aiohttp-3.13.2-cp314-cp314t-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:e96eb1a34396e9430c19d8338d2ec33015e4a87ef2b4449db94c22412e25ccdf", size = 1864168, upload-time = "2025-10-28T20:58:20.113Z" },
+    { url = "https://files.pythonhosted.org/packages/8f/65/75a9a76db8364b5d0e52a0c20eabc5d52297385d9af9c35335b924fafdee/aiohttp-3.13.2-cp314-cp314t-manylinux2014_armv7l.manylinux_2_17_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:23fb0783bc1a33640036465019d3bba069942616a6a2353c6907d7fe1ccdaf4e", size = 1719200, upload-time = "2025-10-28T20:58:22.583Z" },
+    { url = "https://files.pythonhosted.org/packages/f5/55/8df2ed78d7f41d232f6bd3ff866b6f617026551aa1d07e2f03458f964575/aiohttp-3.13.2-cp314-cp314t-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:2e1a9bea6244a1d05a4e57c295d69e159a5c50d8ef16aa390948ee873478d9a5", size = 1843497, upload-time = "2025-10-28T20:58:24.672Z" },
+    { url = "https://files.pythonhosted.org/packages/e9/e0/94d7215e405c5a02ccb6a35c7a3a6cfff242f457a00196496935f700cde5/aiohttp-3.13.2-cp314-cp314t-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:0a3d54e822688b56e9f6b5816fb3de3a3a64660efac64e4c2dc435230ad23bad", size = 1935703, upload-time = "2025-10-28T20:58:26.758Z" },
+    { url = "https://files.pythonhosted.org/packages/0b/78/1eeb63c3f9b2d1015a4c02788fb543141aad0a03ae3f7a7b669b2483f8d4/aiohttp-3.13.2-cp314-cp314t-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:7a653d872afe9f33497215745da7a943d1dc15b728a9c8da1c3ac423af35178e", size = 1792738, upload-time = "2025-10-28T20:58:29.787Z" },
+    { url = "https://files.pythonhosted.org/packages/41/75/aaf1eea4c188e51538c04cc568040e3082db263a57086ea74a7d38c39e42/aiohttp-3.13.2-cp314-cp314t-manylinux_2_31_riscv64.manylinux_2_39_riscv64.whl", hash = "sha256:56d36e80d2003fa3fc0207fac644216d8532e9504a785ef9a8fd013f84a42c61", size = 1624061, upload-time = "2025-10-28T20:58:32.529Z" },
+    { url = "https://files.pythonhosted.org/packages/9b/c2/3b6034de81fbcc43de8aeb209073a2286dfb50b86e927b4efd81cf848197/aiohttp-3.13.2-cp314-cp314t-musllinux_1_2_aarch64.whl", hash = "sha256:78cd586d8331fb8e241c2dd6b2f4061778cc69e150514b39a9e28dd050475661", size = 1789201, upload-time = "2025-10-28T20:58:34.618Z" },
+    { url = "https://files.pythonhosted.org/packages/c9/38/c15dcf6d4d890217dae79d7213988f4e5fe6183d43893a9cf2fe9e84ca8d/aiohttp-3.13.2-cp314-cp314t-musllinux_1_2_armv7l.whl", hash = "sha256:20b10bbfbff766294fe99987f7bb3b74fdd2f1a2905f2562132641ad434dcf98", size = 1776868, upload-time = "2025-10-28T20:58:38.835Z" },
+    { url = "https://files.pythonhosted.org/packages/04/75/f74fd178ac81adf4f283a74847807ade5150e48feda6aef024403716c30c/aiohttp-3.13.2-cp314-cp314t-musllinux_1_2_ppc64le.whl", hash = "sha256:9ec49dff7e2b3c85cdeaa412e9d438f0ecd71676fde61ec57027dd392f00c693", size = 1790660, upload-time = "2025-10-28T20:58:41.507Z" },
+    { url = "https://files.pythonhosted.org/packages/e7/80/7368bd0d06b16b3aba358c16b919e9c46cf11587dc572091031b0e9e3ef0/aiohttp-3.13.2-cp314-cp314t-musllinux_1_2_riscv64.whl", hash = "sha256:94f05348c4406450f9d73d38efb41d669ad6cd90c7ee194810d0eefbfa875a7a", size = 1617548, upload-time = "2025-10-28T20:58:43.674Z" },
+    { url = "https://files.pythonhosted.org/packages/7d/4b/a6212790c50483cb3212e507378fbe26b5086d73941e1ec4b56a30439688/aiohttp-3.13.2-cp314-cp314t-musllinux_1_2_s390x.whl", hash = "sha256:fa4dcb605c6f82a80c7f95713c2b11c3b8e9893b3ebd2bc9bde93165ed6107be", size = 1817240, upload-time = "2025-10-28T20:58:45.787Z" },
+    { url = "https://files.pythonhosted.org/packages/ff/f7/ba5f0ba4ea8d8f3c32850912944532b933acbf0f3a75546b89269b9b7dde/aiohttp-3.13.2-cp314-cp314t-musllinux_1_2_x86_64.whl", hash = "sha256:cf00e5db968c3f67eccd2778574cf64d8b27d95b237770aa32400bd7a1ca4f6c", size = 1762334, upload-time = "2025-10-28T20:58:47.936Z" },
+    { url = "https://files.pythonhosted.org/packages/7e/83/1a5a1856574588b1cad63609ea9ad75b32a8353ac995d830bf5da9357364/aiohttp-3.13.2-cp314-cp314t-win32.whl", hash = "sha256:d23b5fe492b0805a50d3371e8a728a9134d8de5447dce4c885f5587294750734", size = 464685, upload-time = "2025-10-28T20:58:50.642Z" },
+    { url = "https://files.pythonhosted.org/packages/9f/4d/d22668674122c08f4d56972297c51a624e64b3ed1efaa40187607a7cb66e/aiohttp-3.13.2-cp314-cp314t-win_amd64.whl", hash = "sha256:ff0a7b0a82a7ab905cbda74006318d1b12e37c797eb1b0d4eb3e316cf47f658f", size = 498093, upload-time = "2025-10-28T20:58:52.782Z" },
+]
+
+[[package]]
+name = "aiosignal"
+version = "1.4.0"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "frozenlist" },
+    { name = "typing-extensions", marker = "python_full_version < '3.13'" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/61/62/06741b579156360248d1ec624842ad0edf697050bbaf7c3e46394e106ad1/aiosignal-1.4.0.tar.gz", hash = "sha256:f47eecd9468083c2029cc99945502cb7708b082c232f9aca65da147157b251c7", size = 25007, upload-time = "2025-07-03T22:54:43.528Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/fb/76/641ae371508676492379f16e2fa48f4e2c11741bd63c48be4b12a6b09cba/aiosignal-1.4.0-py3-none-any.whl", hash = "sha256:053243f8b92b990551949e63930a839ff0cf0b0ebbe0597b0f3fb19e1a0fe82e", size = 7490, upload-time = "2025-07-03T22:54:42.156Z" },
+]
+
 [[package]]
 name = "altair"
 version = "6.0.0"
@@ -29,6 +153,28 @@ wheels = [
     { url = "https://files.pythonhosted.org/packages/db/33/ef2f2409450ef6daa61459d5de5c08128e7d3edb773fefd0a324d1310238/altair-6.0.0-py3-none-any.whl", hash = "sha256:09ae95b53d5fe5b16987dccc785a7af8588f2dca50de1e7a156efa8a461515f8", size = 795410, upload-time = "2025-11-12T08:59:09.804Z" },
 ]
 
+[[package]]
+name = "annotated-types"
+version = "0.7.0"
+source = { registry = "https://pypi.org/simple" }
+sdist = { url = "https://files.pythonhosted.org/packages/ee/67/531ea369ba64dcff5ec9c3402f9f51bf748cec26dde048a2f973a4eea7f5/annotated_types-0.7.0.tar.gz", hash = "sha256:aff07c09a53a08bc8cfccb9c85b05f1aa9a2a6f23728d790723543408344ce89", size = 16081, upload-time = "2024-05-20T21:33:25.928Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl", hash = "sha256:1f02e8b43a8fbbc3f3e0d4f0f4bfc8131bcb4eebe8849b8e5c773f3a1c582a53", size = 13643, upload-time = "2024-05-20T21:33:24.1Z" },
+]
+
+[[package]]
+name = "anyio"
+version = "4.12.0"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "idna" },
+    { name = "typing-extensions", marker = "python_full_version < '3.13'" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/16/ce/8a777047513153587e5434fd752e89334ac33e379aa3497db860eeb60377/anyio-4.12.0.tar.gz", hash = "sha256:73c693b567b0c55130c104d0b43a9baf3aa6a31fc6110116509f27bf75e21ec0", size = 228266, upload-time = "2025-11-28T23:37:38.911Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/7f/9c/36c5c37947ebfb8c7f22e0eb6e4d188ee2d53aa3880f3f2744fb894f0cb1/anyio-4.12.0-py3-none-any.whl", hash = "sha256:dad2376a628f98eeca4881fc56cd06affd18f659b17a747d3ff0307ced94b1bb", size = 113362, upload-time = "2025-11-28T23:36:57.897Z" },
+]
+
 [[package]]
 name = "atpublic"
 version = "7.0.0"
@@ -224,6 +370,8 @@ source = { editable = "." }
 dependencies = [
     { name = "duckdb" },
     { name = "ibis-framework", extra = ["duckdb"] },
+    { name = "langchain" },
+    { name = "langchain-community" },
     { name = "lifelines" },
     { name = "openpyxl" },
     { name = "pandas" },
@@ -252,12 +400,16 @@ dev = [
     { name = "mkdocs-mermaid2-plugin" },
     { name = "mkdocstrings", extra = ["python"] },
     { name = "pymdown-extensions" },
+    { name = "pytest" },
+    { name = "pytest-cov" },
 ]
 
 [package.metadata]
 requires-dist = [
     { name = "duckdb", specifier = ">=0.9.0" },
     { name = "ibis-framework", extras = ["duckdb"], specifier = ">=10.0.0" },
+    { name = "langchain", specifier = ">=1.2.0" },
+    { name = "langchain-community", specifier = ">=0.4.1" },
     { name = "lifelines", specifier = ">=0.27.0" },
     { name = "mypy", marker = "extra == 'dev'", specifier = ">=1.5.0" },
     { name = "openpyxl", specifier = ">=3.1.0" },
@@ -283,6 +435,8 @@ dev = [
     { name = "mkdocs-mermaid2-plugin", specifier = ">=1.2.3" },
     { name = "mkdocstrings", extras = ["python"], specifier = ">=1.0.0" },
     { name = "pymdown-extensions", specifier = ">=10.19.1" },
+    { name = "pytest", specifier = ">=9.0.2" },
+    { name = "pytest-cov", specifier = ">=7.0.0" },
 ]
 
 [[package]]
@@ -477,6 +631,19 @@ wheels = [
     { url = "https://files.pythonhosted.org/packages/e7/05/c19819d5e3d95294a6f5947fb9b9629efb316b96de511b418c53d245aae6/cycler-0.12.1-py3-none-any.whl", hash = "sha256:85cef7cff222d8644161529808465972e51340599459b8ac3ccbac5a854e0d30", size = 8321, upload-time = "2023-10-07T05:32:16.783Z" },
 ]
 
+[[package]]
+name = "dataclasses-json"
+version = "0.6.7"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "marshmallow" },
+    { name = "typing-inspect" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/64/a4/f71d9cf3a5ac257c993b5ca3f93df5f7fb395c725e7f1e6479d2514173c3/dataclasses_json-0.6.7.tar.gz", hash = "sha256:b6b3e528266ea45b9535223bc53ca645f5208833c29229e847b3f26a1cc55fc0", size = 32227, upload-time = "2024-06-09T16:20:19.103Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/c3/be/d0d44e092656fe7a06b55e6103cbce807cdbdee17884a5367c68c9860853/dataclasses_json-0.6.7-py3-none-any.whl", hash = "sha256:0dbf33f26c8d5305befd61b39d2b3414e8a407bedc2834dea9b8d642666fb40a", size = 28686, upload-time = "2024-06-09T16:20:16.715Z" },
+]
+
 [[package]]
 name = "docs"
 version = "0.1.0"
@@ -608,6 +775,111 @@ wheels = [
     { url = "https://files.pythonhosted.org/packages/1a/9d/c2c8b51b32f829a16fe042db30ad1dcef7947bf1dcf77c2cfd7b6f37b83a/formulaic-1.2.1-py3-none-any.whl", hash = "sha256:661d6d2467aa961b9afb3a1e2a187494239793c63eb729e422d1307afa98b43b", size = 117290, upload-time = "2025-09-21T05:27:30.025Z" },
 ]
 
+[[package]]
+name = "frozenlist"
+version = "1.8.0"
+source = { registry = "https://pypi.org/simple" }
+sdist = { url = "https://files.pythonhosted.org/packages/2d/f5/c831fac6cc817d26fd54c7eaccd04ef7e0288806943f7cc5bbf69f3ac1f0/frozenlist-1.8.0.tar.gz", hash = "sha256:3ede829ed8d842f6cd48fc7081d7a41001a56f1f38603f9d49bf3020d59a31ad", size = 45875, upload-time = "2025-10-06T05:38:17.865Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/bc/03/077f869d540370db12165c0aa51640a873fb661d8b315d1d4d67b284d7ac/frozenlist-1.8.0-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:09474e9831bc2b2199fad6da3c14c7b0fbdd377cce9d3d77131be28906cb7d84", size = 86912, upload-time = "2025-10-06T05:35:45.98Z" },
+    { url = "https://files.pythonhosted.org/packages/df/b5/7610b6bd13e4ae77b96ba85abea1c8cb249683217ef09ac9e0ae93f25a91/frozenlist-1.8.0-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:17c883ab0ab67200b5f964d2b9ed6b00971917d5d8a92df149dc2c9779208ee9", size = 50046, upload-time = "2025-10-06T05:35:47.009Z" },
+    { url = "https://files.pythonhosted.org/packages/6e/ef/0e8f1fe32f8a53dd26bdd1f9347efe0778b0fddf62789ea683f4cc7d787d/frozenlist-1.8.0-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:fa47e444b8ba08fffd1c18e8cdb9a75db1b6a27f17507522834ad13ed5922b93", size = 50119, upload-time = "2025-10-06T05:35:48.38Z" },
+    { url = "https://files.pythonhosted.org/packages/11/b1/71a477adc7c36e5fb628245dfbdea2166feae310757dea848d02bd0689fd/frozenlist-1.8.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl", hash = "sha256:2552f44204b744fba866e573be4c1f9048d6a324dfe14475103fd51613eb1d1f", size = 231067, upload-time = "2025-10-06T05:35:49.97Z" },
+    { url = "https://files.pythonhosted.org/packages/45/7e/afe40eca3a2dc19b9904c0f5d7edfe82b5304cb831391edec0ac04af94c2/frozenlist-1.8.0-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:957e7c38f250991e48a9a73e6423db1bb9dd14e722a10f6b8bb8e16a0f55f695", size = 233160, upload-time = "2025-10-06T05:35:51.729Z" },
+    { url = "https://files.pythonhosted.org/packages/a6/aa/7416eac95603ce428679d273255ffc7c998d4132cfae200103f164b108aa/frozenlist-1.8.0-cp311-cp311-manylinux2014_armv7l.manylinux_2_17_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:8585e3bb2cdea02fc88ffa245069c36555557ad3609e83be0ec71f54fd4abb52", size = 228544, upload-time = "2025-10-06T05:35:53.246Z" },
+    { url = "https://files.pythonhosted.org/packages/8b/3d/2a2d1f683d55ac7e3875e4263d28410063e738384d3adc294f5ff3d7105e/frozenlist-1.8.0-cp311-cp311-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:edee74874ce20a373d62dc28b0b18b93f645633c2943fd90ee9d898550770581", size = 243797, upload-time = "2025-10-06T05:35:54.497Z" },
+    { url = "https://files.pythonhosted.org/packages/78/1e/2d5565b589e580c296d3bb54da08d206e797d941a83a6fdea42af23be79c/frozenlist-1.8.0-cp311-cp311-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:c9a63152fe95756b85f31186bddf42e4c02c6321207fd6601a1c89ebac4fe567", size = 247923, upload-time = "2025-10-06T05:35:55.861Z" },
+    { url = "https://files.pythonhosted.org/packages/aa/c3/65872fcf1d326a7f101ad4d86285c403c87be7d832b7470b77f6d2ed5ddc/frozenlist-1.8.0-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:b6db2185db9be0a04fecf2f241c70b63b1a242e2805be291855078f2b404dd6b", size = 230886, upload-time = "2025-10-06T05:35:57.399Z" },
+    { url = "https://files.pythonhosted.org/packages/a0/76/ac9ced601d62f6956f03cc794f9e04c81719509f85255abf96e2510f4265/frozenlist-1.8.0-cp311-cp311-musllinux_1_2_armv7l.whl", hash = "sha256:f4be2e3d8bc8aabd566f8d5b8ba7ecc09249d74ba3c9ed52e54dc23a293f0b92", size = 245731, upload-time = "2025-10-06T05:35:58.563Z" },
+    { url = "https://files.pythonhosted.org/packages/b9/49/ecccb5f2598daf0b4a1415497eba4c33c1e8ce07495eb07d2860c731b8d5/frozenlist-1.8.0-cp311-cp311-musllinux_1_2_ppc64le.whl", hash = "sha256:c8d1634419f39ea6f5c427ea2f90ca85126b54b50837f31497f3bf38266e853d", size = 241544, upload-time = "2025-10-06T05:35:59.719Z" },
+    { url = "https://files.pythonhosted.org/packages/53/4b/ddf24113323c0bbcc54cb38c8b8916f1da7165e07b8e24a717b4a12cbf10/frozenlist-1.8.0-cp311-cp311-musllinux_1_2_s390x.whl", hash = "sha256:1a7fa382a4a223773ed64242dbe1c9c326ec09457e6b8428efb4118c685c3dfd", size = 241806, upload-time = "2025-10-06T05:36:00.959Z" },
+    { url = "https://files.pythonhosted.org/packages/a7/fb/9b9a084d73c67175484ba2789a59f8eebebd0827d186a8102005ce41e1ba/frozenlist-1.8.0-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:11847b53d722050808926e785df837353bd4d75f1d494377e59b23594d834967", size = 229382, upload-time = "2025-10-06T05:36:02.22Z" },
+    { url = "https://files.pythonhosted.org/packages/95/a3/c8fb25aac55bf5e12dae5c5aa6a98f85d436c1dc658f21c3ac73f9fa95e5/frozenlist-1.8.0-cp311-cp311-win32.whl", hash = "sha256:27c6e8077956cf73eadd514be8fb04d77fc946a7fe9f7fe167648b0b9085cc25", size = 39647, upload-time = "2025-10-06T05:36:03.409Z" },
+    { url = "https://files.pythonhosted.org/packages/0a/f5/603d0d6a02cfd4c8f2a095a54672b3cf967ad688a60fb9faf04fc4887f65/frozenlist-1.8.0-cp311-cp311-win_amd64.whl", hash = "sha256:ac913f8403b36a2c8610bbfd25b8013488533e71e62b4b4adce9c86c8cea905b", size = 44064, upload-time = "2025-10-06T05:36:04.368Z" },
+    { url = "https://files.pythonhosted.org/packages/5d/16/c2c9ab44e181f043a86f9a8f84d5124b62dbcb3a02c0977ec72b9ac1d3e0/frozenlist-1.8.0-cp311-cp311-win_arm64.whl", hash = "sha256:d4d3214a0f8394edfa3e303136d0575eece0745ff2b47bd2cb2e66dd92d4351a", size = 39937, upload-time = "2025-10-06T05:36:05.669Z" },
+    { url = "https://files.pythonhosted.org/packages/69/29/948b9aa87e75820a38650af445d2ef2b6b8a6fab1a23b6bb9e4ef0be2d59/frozenlist-1.8.0-cp312-cp312-macosx_10_13_universal2.whl", hash = "sha256:78f7b9e5d6f2fdb88cdde9440dc147259b62b9d3b019924def9f6478be254ac1", size = 87782, upload-time = "2025-10-06T05:36:06.649Z" },
+    { url = "https://files.pythonhosted.org/packages/64/80/4f6e318ee2a7c0750ed724fa33a4bdf1eacdc5a39a7a24e818a773cd91af/frozenlist-1.8.0-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:229bf37d2e4acdaf808fd3f06e854a4a7a3661e871b10dc1f8f1896a3b05f18b", size = 50594, upload-time = "2025-10-06T05:36:07.69Z" },
+    { url = "https://files.pythonhosted.org/packages/2b/94/5c8a2b50a496b11dd519f4a24cb5496cf125681dd99e94c604ccdea9419a/frozenlist-1.8.0-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:f833670942247a14eafbb675458b4e61c82e002a148f49e68257b79296e865c4", size = 50448, upload-time = "2025-10-06T05:36:08.78Z" },
+    { url = "https://files.pythonhosted.org/packages/6a/bd/d91c5e39f490a49df14320f4e8c80161cfcce09f1e2cde1edd16a551abb3/frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl", hash = "sha256:494a5952b1c597ba44e0e78113a7266e656b9794eec897b19ead706bd7074383", size = 242411, upload-time = "2025-10-06T05:36:09.801Z" },
+    { url = "https://files.pythonhosted.org/packages/8f/83/f61505a05109ef3293dfb1ff594d13d64a2324ac3482be2cedc2be818256/frozenlist-1.8.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:96f423a119f4777a4a056b66ce11527366a8bb92f54e541ade21f2374433f6d4", size = 243014, upload-time = "2025-10-06T05:36:11.394Z" },
+    { url = "https://files.pythonhosted.org/packages/d8/cb/cb6c7b0f7d4023ddda30cf56b8b17494eb3a79e3fda666bf735f63118b35/frozenlist-1.8.0-cp312-cp312-manylinux2014_armv7l.manylinux_2_17_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:3462dd9475af2025c31cc61be6652dfa25cbfb56cbbf52f4ccfe029f38decaf8", size = 234909, upload-time = "2025-10-06T05:36:12.598Z" },
+    { url = "https://files.pythonhosted.org/packages/31/c5/cd7a1f3b8b34af009fb17d4123c5a778b44ae2804e3ad6b86204255f9ec5/frozenlist-1.8.0-cp312-cp312-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:c4c800524c9cd9bac5166cd6f55285957fcfc907db323e193f2afcd4d9abd69b", size = 250049, upload-time = "2025-10-06T05:36:14.065Z" },
+    { url = "https://files.pythonhosted.org/packages/c0/01/2f95d3b416c584a1e7f0e1d6d31998c4a795f7544069ee2e0962a4b60740/frozenlist-1.8.0-cp312-cp312-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:d6a5df73acd3399d893dafc71663ad22534b5aa4f94e8a2fabfe856c3c1b6a52", size = 256485, upload-time = "2025-10-06T05:36:15.39Z" },
+    { url = "https://files.pythonhosted.org/packages/ce/03/024bf7720b3abaebcff6d0793d73c154237b85bdf67b7ed55e5e9596dc9a/frozenlist-1.8.0-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:405e8fe955c2280ce66428b3ca55e12b3c4e9c336fb2103a4937e891c69a4a29", size = 237619, upload-time = "2025-10-06T05:36:16.558Z" },
+    { url = "https://files.pythonhosted.org/packages/69/fa/f8abdfe7d76b731f5d8bd217827cf6764d4f1d9763407e42717b4bed50a0/frozenlist-1.8.0-cp312-cp312-musllinux_1_2_armv7l.whl", hash = "sha256:908bd3f6439f2fef9e85031b59fd4f1297af54415fb60e4254a95f75b3cab3f3", size = 250320, upload-time = "2025-10-06T05:36:17.821Z" },
+    { url = "https://files.pythonhosted.org/packages/f5/3c/b051329f718b463b22613e269ad72138cc256c540f78a6de89452803a47d/frozenlist-1.8.0-cp312-cp312-musllinux_1_2_ppc64le.whl", hash = "sha256:294e487f9ec720bd8ffcebc99d575f7eff3568a08a253d1ee1a0378754b74143", size = 246820, upload-time = "2025-10-06T05:36:19.046Z" },
+    { url = "https://files.pythonhosted.org/packages/0f/ae/58282e8f98e444b3f4dd42448ff36fa38bef29e40d40f330b22e7108f565/frozenlist-1.8.0-cp312-cp312-musllinux_1_2_s390x.whl", hash = "sha256:74c51543498289c0c43656701be6b077f4b265868fa7f8a8859c197006efb608", size = 250518, upload-time = "2025-10-06T05:36:20.763Z" },
+    { url = "https://files.pythonhosted.org/packages/8f/96/007e5944694d66123183845a106547a15944fbbb7154788cbf7272789536/frozenlist-1.8.0-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:776f352e8329135506a1d6bf16ac3f87bc25b28e765949282dcc627af36123aa", size = 239096, upload-time = "2025-10-06T05:36:22.129Z" },
+    { url = "https://files.pythonhosted.org/packages/66/bb/852b9d6db2fa40be96f29c0d1205c306288f0684df8fd26ca1951d461a56/frozenlist-1.8.0-cp312-cp312-win32.whl", hash = "sha256:433403ae80709741ce34038da08511d4a77062aa924baf411ef73d1146e74faf", size = 39985, upload-time = "2025-10-06T05:36:23.661Z" },
+    { url = "https://files.pythonhosted.org/packages/b8/af/38e51a553dd66eb064cdf193841f16f077585d4d28394c2fa6235cb41765/frozenlist-1.8.0-cp312-cp312-win_amd64.whl", hash = "sha256:34187385b08f866104f0c0617404c8eb08165ab1272e884abc89c112e9c00746", size = 44591, upload-time = "2025-10-06T05:36:24.958Z" },
+    { url = "https://files.pythonhosted.org/packages/a7/06/1dc65480ab147339fecc70797e9c2f69d9cea9cf38934ce08df070fdb9cb/frozenlist-1.8.0-cp312-cp312-win_arm64.whl", hash = "sha256:fe3c58d2f5db5fbd18c2987cba06d51b0529f52bc3a6cdc33d3f4eab725104bd", size = 40102, upload-time = "2025-10-06T05:36:26.333Z" },
+    { url = "https://files.pythonhosted.org/packages/2d/40/0832c31a37d60f60ed79e9dfb5a92e1e2af4f40a16a29abcc7992af9edff/frozenlist-1.8.0-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:8d92f1a84bb12d9e56f818b3a746f3efba93c1b63c8387a73dde655e1e42282a", size = 85717, upload-time = "2025-10-06T05:36:27.341Z" },
+    { url = "https://files.pythonhosted.org/packages/30/ba/b0b3de23f40bc55a7057bd38434e25c34fa48e17f20ee273bbde5e0650f3/frozenlist-1.8.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:96153e77a591c8adc2ee805756c61f59fef4cf4073a9275ee86fe8cba41241f7", size = 49651, upload-time = "2025-10-06T05:36:28.855Z" },
+    { url = "https://files.pythonhosted.org/packages/0c/ab/6e5080ee374f875296c4243c381bbdef97a9ac39c6e3ce1d5f7d42cb78d6/frozenlist-1.8.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:f21f00a91358803399890ab167098c131ec2ddd5f8f5fd5fe9c9f2c6fcd91e40", size = 49417, upload-time = "2025-10-06T05:36:29.877Z" },
+    { url = "https://files.pythonhosted.org/packages/d5/4e/e4691508f9477ce67da2015d8c00acd751e6287739123113a9fca6f1604e/frozenlist-1.8.0-cp313-cp313-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl", hash = "sha256:fb30f9626572a76dfe4293c7194a09fb1fe93ba94c7d4f720dfae3b646b45027", size = 234391, upload-time = "2025-10-06T05:36:31.301Z" },
+    { url = "https://files.pythonhosted.org/packages/40/76/c202df58e3acdf12969a7895fd6f3bc016c642e6726aa63bd3025e0fc71c/frozenlist-1.8.0-cp313-cp313-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:eaa352d7047a31d87dafcacbabe89df0aa506abb5b1b85a2fb91bc3faa02d822", size = 233048, upload-time = "2025-10-06T05:36:32.531Z" },
+    { url = "https://files.pythonhosted.org/packages/f9/c0/8746afb90f17b73ca5979c7a3958116e105ff796e718575175319b5bb4ce/frozenlist-1.8.0-cp313-cp313-manylinux2014_armv7l.manylinux_2_17_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:03ae967b4e297f58f8c774c7eabcce57fe3c2434817d4385c50661845a058121", size = 226549, upload-time = "2025-10-06T05:36:33.706Z" },
+    { url = "https://files.pythonhosted.org/packages/7e/eb/4c7eefc718ff72f9b6c4893291abaae5fbc0c82226a32dcd8ef4f7a5dbef/frozenlist-1.8.0-cp313-cp313-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:f6292f1de555ffcc675941d65fffffb0a5bcd992905015f85d0592201793e0e5", size = 239833, upload-time = "2025-10-06T05:36:34.947Z" },
+    { url = "https://files.pythonhosted.org/packages/c2/4e/e5c02187cf704224f8b21bee886f3d713ca379535f16893233b9d672ea71/frozenlist-1.8.0-cp313-cp313-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:29548f9b5b5e3460ce7378144c3010363d8035cea44bc0bf02d57f5a685e084e", size = 245363, upload-time = "2025-10-06T05:36:36.534Z" },
+    { url = "https://files.pythonhosted.org/packages/1f/96/cb85ec608464472e82ad37a17f844889c36100eed57bea094518bf270692/frozenlist-1.8.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:ec3cc8c5d4084591b4237c0a272cc4f50a5b03396a47d9caaf76f5d7b38a4f11", size = 229314, upload-time = "2025-10-06T05:36:38.582Z" },
+    { url = "https://files.pythonhosted.org/packages/5d/6f/4ae69c550e4cee66b57887daeebe006fe985917c01d0fff9caab9883f6d0/frozenlist-1.8.0-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:517279f58009d0b1f2e7c1b130b377a349405da3f7621ed6bfae50b10adf20c1", size = 243365, upload-time = "2025-10-06T05:36:40.152Z" },
+    { url = "https://files.pythonhosted.org/packages/7a/58/afd56de246cf11780a40a2c28dc7cbabbf06337cc8ddb1c780a2d97e88d8/frozenlist-1.8.0-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:db1e72ede2d0d7ccb213f218df6a078a9c09a7de257c2fe8fcef16d5925230b1", size = 237763, upload-time = "2025-10-06T05:36:41.355Z" },
+    { url = "https://files.pythonhosted.org/packages/cb/36/cdfaf6ed42e2644740d4a10452d8e97fa1c062e2a8006e4b09f1b5fd7d63/frozenlist-1.8.0-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:b4dec9482a65c54a5044486847b8a66bf10c9cb4926d42927ec4e8fd5db7fed8", size = 240110, upload-time = "2025-10-06T05:36:42.716Z" },
+    { url = "https://files.pythonhosted.org/packages/03/a8/9ea226fbefad669f11b52e864c55f0bd57d3c8d7eb07e9f2e9a0b39502e1/frozenlist-1.8.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:21900c48ae04d13d416f0e1e0c4d81f7931f73a9dfa0b7a8746fb2fe7dd970ed", size = 233717, upload-time = "2025-10-06T05:36:44.251Z" },
+    { url = "https://files.pythonhosted.org/packages/1e/0b/1b5531611e83ba7d13ccc9988967ea1b51186af64c42b7a7af465dcc9568/frozenlist-1.8.0-cp313-cp313-win32.whl", hash = "sha256:8b7b94a067d1c504ee0b16def57ad5738701e4ba10cec90529f13fa03c833496", size = 39628, upload-time = "2025-10-06T05:36:45.423Z" },
+    { url = "https://files.pythonhosted.org/packages/d8/cf/174c91dbc9cc49bc7b7aab74d8b734e974d1faa8f191c74af9b7e80848e6/frozenlist-1.8.0-cp313-cp313-win_amd64.whl", hash = "sha256:878be833caa6a3821caf85eb39c5ba92d28e85df26d57afb06b35b2efd937231", size = 43882, upload-time = "2025-10-06T05:36:46.796Z" },
+    { url = "https://files.pythonhosted.org/packages/c1/17/502cd212cbfa96eb1388614fe39a3fc9ab87dbbe042b66f97acb57474834/frozenlist-1.8.0-cp313-cp313-win_arm64.whl", hash = "sha256:44389d135b3ff43ba8cc89ff7f51f5a0bb6b63d829c8300f79a2fe4fe61bcc62", size = 39676, upload-time = "2025-10-06T05:36:47.8Z" },
+    { url = "https://files.pythonhosted.org/packages/d2/5c/3bbfaa920dfab09e76946a5d2833a7cbdf7b9b4a91c714666ac4855b88b4/frozenlist-1.8.0-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:e25ac20a2ef37e91c1b39938b591457666a0fa835c7783c3a8f33ea42870db94", size = 89235, upload-time = "2025-10-06T05:36:48.78Z" },
+    { url = "https://files.pythonhosted.org/packages/d2/d6/f03961ef72166cec1687e84e8925838442b615bd0b8854b54923ce5b7b8a/frozenlist-1.8.0-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:07cdca25a91a4386d2e76ad992916a85038a9b97561bf7a3fd12d5d9ce31870c", size = 50742, upload-time = "2025-10-06T05:36:49.837Z" },
+    { url = "https://files.pythonhosted.org/packages/1e/bb/a6d12b7ba4c3337667d0e421f7181c82dda448ce4e7ad7ecd249a16fa806/frozenlist-1.8.0-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:4e0c11f2cc6717e0a741f84a527c52616140741cd812a50422f83dc31749fb52", size = 51725, upload-time = "2025-10-06T05:36:50.851Z" },
+    { url = "https://files.pythonhosted.org/packages/bc/71/d1fed0ffe2c2ccd70b43714c6cab0f4188f09f8a67a7914a6b46ee30f274/frozenlist-1.8.0-cp313-cp313t-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl", hash = "sha256:b3210649ee28062ea6099cfda39e147fa1bc039583c8ee4481cb7811e2448c51", size = 284533, upload-time = "2025-10-06T05:36:51.898Z" },
+    { url = "https://files.pythonhosted.org/packages/c9/1f/fb1685a7b009d89f9bf78a42d94461bc06581f6e718c39344754a5d9bada/frozenlist-1.8.0-cp313-cp313t-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:581ef5194c48035a7de2aefc72ac6539823bb71508189e5de01d60c9dcd5fa65", size = 292506, upload-time = "2025-10-06T05:36:53.101Z" },
+    { url = "https://files.pythonhosted.org/packages/e6/3b/b991fe1612703f7e0d05c0cf734c1b77aaf7c7d321df4572e8d36e7048c8/frozenlist-1.8.0-cp313-cp313t-manylinux2014_armv7l.manylinux_2_17_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:3ef2d026f16a2b1866e1d86fc4e1291e1ed8a387b2c333809419a2f8b3a77b82", size = 274161, upload-time = "2025-10-06T05:36:54.309Z" },
+    { url = "https://files.pythonhosted.org/packages/ca/ec/c5c618767bcdf66e88945ec0157d7f6c4a1322f1473392319b7a2501ded7/frozenlist-1.8.0-cp313-cp313t-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:5500ef82073f599ac84d888e3a8c1f77ac831183244bfd7f11eaa0289fb30714", size = 294676, upload-time = "2025-10-06T05:36:55.566Z" },
+    { url = "https://files.pythonhosted.org/packages/7c/ce/3934758637d8f8a88d11f0585d6495ef54b2044ed6ec84492a91fa3b27aa/frozenlist-1.8.0-cp313-cp313t-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:50066c3997d0091c411a66e710f4e11752251e6d2d73d70d8d5d4c76442a199d", size = 300638, upload-time = "2025-10-06T05:36:56.758Z" },
+    { url = "https://files.pythonhosted.org/packages/fc/4f/a7e4d0d467298f42de4b41cbc7ddaf19d3cfeabaf9ff97c20c6c7ee409f9/frozenlist-1.8.0-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:5c1c8e78426e59b3f8005e9b19f6ff46e5845895adbde20ece9218319eca6506", size = 283067, upload-time = "2025-10-06T05:36:57.965Z" },
+    { url = "https://files.pythonhosted.org/packages/dc/48/c7b163063d55a83772b268e6d1affb960771b0e203b632cfe09522d67ea5/frozenlist-1.8.0-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:eefdba20de0d938cec6a89bd4d70f346a03108a19b9df4248d3cf0d88f1b0f51", size = 292101, upload-time = "2025-10-06T05:36:59.237Z" },
+    { url = "https://files.pythonhosted.org/packages/9f/d0/2366d3c4ecdc2fd391e0afa6e11500bfba0ea772764d631bbf82f0136c9d/frozenlist-1.8.0-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:cf253e0e1c3ceb4aaff6df637ce033ff6535fb8c70a764a8f46aafd3d6ab798e", size = 289901, upload-time = "2025-10-06T05:37:00.811Z" },
+    { url = "https://files.pythonhosted.org/packages/b8/94/daff920e82c1b70e3618a2ac39fbc01ae3e2ff6124e80739ce5d71c9b920/frozenlist-1.8.0-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:032efa2674356903cd0261c4317a561a6850f3ac864a63fc1583147fb05a79b0", size = 289395, upload-time = "2025-10-06T05:37:02.115Z" },
+    { url = "https://files.pythonhosted.org/packages/e3/20/bba307ab4235a09fdcd3cc5508dbabd17c4634a1af4b96e0f69bfe551ebd/frozenlist-1.8.0-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:6da155091429aeba16851ecb10a9104a108bcd32f6c1642867eadaee401c1c41", size = 283659, upload-time = "2025-10-06T05:37:03.711Z" },
+    { url = "https://files.pythonhosted.org/packages/fd/00/04ca1c3a7a124b6de4f8a9a17cc2fcad138b4608e7a3fc5877804b8715d7/frozenlist-1.8.0-cp313-cp313t-win32.whl", hash = "sha256:0f96534f8bfebc1a394209427d0f8a63d343c9779cda6fc25e8e121b5fd8555b", size = 43492, upload-time = "2025-10-06T05:37:04.915Z" },
+    { url = "https://files.pythonhosted.org/packages/59/5e/c69f733a86a94ab10f68e496dc6b7e8bc078ebb415281d5698313e3af3a1/frozenlist-1.8.0-cp313-cp313t-win_amd64.whl", hash = "sha256:5d63a068f978fc69421fb0e6eb91a9603187527c86b7cd3f534a5b77a592b888", size = 48034, upload-time = "2025-10-06T05:37:06.343Z" },
+    { url = "https://files.pythonhosted.org/packages/16/6c/be9d79775d8abe79b05fa6d23da99ad6e7763a1d080fbae7290b286093fd/frozenlist-1.8.0-cp313-cp313t-win_arm64.whl", hash = "sha256:bf0a7e10b077bf5fb9380ad3ae8ce20ef919a6ad93b4552896419ac7e1d8e042", size = 41749, upload-time = "2025-10-06T05:37:07.431Z" },
+    { url = "https://files.pythonhosted.org/packages/f1/c8/85da824b7e7b9b6e7f7705b2ecaf9591ba6f79c1177f324c2735e41d36a2/frozenlist-1.8.0-cp314-cp314-macosx_10_13_universal2.whl", hash = "sha256:cee686f1f4cadeb2136007ddedd0aaf928ab95216e7691c63e50a8ec066336d0", size = 86127, upload-time = "2025-10-06T05:37:08.438Z" },
+    { url = "https://files.pythonhosted.org/packages/8e/e8/a1185e236ec66c20afd72399522f142c3724c785789255202d27ae992818/frozenlist-1.8.0-cp314-cp314-macosx_10_13_x86_64.whl", hash = "sha256:119fb2a1bd47307e899c2fac7f28e85b9a543864df47aa7ec9d3c1b4545f096f", size = 49698, upload-time = "2025-10-06T05:37:09.48Z" },
+    { url = "https://files.pythonhosted.org/packages/a1/93/72b1736d68f03fda5fdf0f2180fb6caaae3894f1b854d006ac61ecc727ee/frozenlist-1.8.0-cp314-cp314-macosx_11_0_arm64.whl", hash = "sha256:4970ece02dbc8c3a92fcc5228e36a3e933a01a999f7094ff7c23fbd2beeaa67c", size = 49749, upload-time = "2025-10-06T05:37:10.569Z" },
+    { url = "https://files.pythonhosted.org/packages/a7/b2/fabede9fafd976b991e9f1b9c8c873ed86f202889b864756f240ce6dd855/frozenlist-1.8.0-cp314-cp314-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl", hash = "sha256:cba69cb73723c3f329622e34bdbf5ce1f80c21c290ff04256cff1cd3c2036ed2", size = 231298, upload-time = "2025-10-06T05:37:11.993Z" },
+    { url = "https://files.pythonhosted.org/packages/3a/3b/d9b1e0b0eed36e70477ffb8360c49c85c8ca8ef9700a4e6711f39a6e8b45/frozenlist-1.8.0-cp314-cp314-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:778a11b15673f6f1df23d9586f83c4846c471a8af693a22e066508b77d201ec8", size = 232015, upload-time = "2025-10-06T05:37:13.194Z" },
+    { url = "https://files.pythonhosted.org/packages/dc/94/be719d2766c1138148564a3960fc2c06eb688da592bdc25adcf856101be7/frozenlist-1.8.0-cp314-cp314-manylinux2014_armv7l.manylinux_2_17_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:0325024fe97f94c41c08872db482cf8ac4800d80e79222c6b0b7b162d5b13686", size = 225038, upload-time = "2025-10-06T05:37:14.577Z" },
+    { url = "https://files.pythonhosted.org/packages/e4/09/6712b6c5465f083f52f50cf74167b92d4ea2f50e46a9eea0523d658454ae/frozenlist-1.8.0-cp314-cp314-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:97260ff46b207a82a7567b581ab4190bd4dfa09f4db8a8b49d1a958f6aa4940e", size = 240130, upload-time = "2025-10-06T05:37:15.781Z" },
+    { url = "https://files.pythonhosted.org/packages/f8/d4/cd065cdcf21550b54f3ce6a22e143ac9e4836ca42a0de1022da8498eac89/frozenlist-1.8.0-cp314-cp314-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:54b2077180eb7f83dd52c40b2750d0a9f175e06a42e3213ce047219de902717a", size = 242845, upload-time = "2025-10-06T05:37:17.037Z" },
+    { url = "https://files.pythonhosted.org/packages/62/c3/f57a5c8c70cd1ead3d5d5f776f89d33110b1addae0ab010ad774d9a44fb9/frozenlist-1.8.0-cp314-cp314-musllinux_1_2_aarch64.whl", hash = "sha256:2f05983daecab868a31e1da44462873306d3cbfd76d1f0b5b69c473d21dbb128", size = 229131, upload-time = "2025-10-06T05:37:18.221Z" },
+    { url = "https://files.pythonhosted.org/packages/6c/52/232476fe9cb64f0742f3fde2b7d26c1dac18b6d62071c74d4ded55e0ef94/frozenlist-1.8.0-cp314-cp314-musllinux_1_2_armv7l.whl", hash = "sha256:33f48f51a446114bc5d251fb2954ab0164d5be02ad3382abcbfe07e2531d650f", size = 240542, upload-time = "2025-10-06T05:37:19.771Z" },
+    { url = "https://files.pythonhosted.org/packages/5f/85/07bf3f5d0fb5414aee5f47d33c6f5c77bfe49aac680bfece33d4fdf6a246/frozenlist-1.8.0-cp314-cp314-musllinux_1_2_ppc64le.whl", hash = "sha256:154e55ec0655291b5dd1b8731c637ecdb50975a2ae70c606d100750a540082f7", size = 237308, upload-time = "2025-10-06T05:37:20.969Z" },
+    { url = "https://files.pythonhosted.org/packages/11/99/ae3a33d5befd41ac0ca2cc7fd3aa707c9c324de2e89db0e0f45db9a64c26/frozenlist-1.8.0-cp314-cp314-musllinux_1_2_s390x.whl", hash = "sha256:4314debad13beb564b708b4a496020e5306c7333fa9a3ab90374169a20ffab30", size = 238210, upload-time = "2025-10-06T05:37:22.252Z" },
+    { url = "https://files.pythonhosted.org/packages/b2/60/b1d2da22f4970e7a155f0adde9b1435712ece01b3cd45ba63702aea33938/frozenlist-1.8.0-cp314-cp314-musllinux_1_2_x86_64.whl", hash = "sha256:073f8bf8becba60aa931eb3bc420b217bb7d5b8f4750e6f8b3be7f3da85d38b7", size = 231972, upload-time = "2025-10-06T05:37:23.5Z" },
+    { url = "https://files.pythonhosted.org/packages/3f/ab/945b2f32de889993b9c9133216c068b7fcf257d8595a0ac420ac8677cab0/frozenlist-1.8.0-cp314-cp314-win32.whl", hash = "sha256:bac9c42ba2ac65ddc115d930c78d24ab8d4f465fd3fc473cdedfccadb9429806", size = 40536, upload-time = "2025-10-06T05:37:25.581Z" },
+    { url = "https://files.pythonhosted.org/packages/59/ad/9caa9b9c836d9ad6f067157a531ac48b7d36499f5036d4141ce78c230b1b/frozenlist-1.8.0-cp314-cp314-win_amd64.whl", hash = "sha256:3e0761f4d1a44f1d1a47996511752cf3dcec5bbdd9cc2b4fe595caf97754b7a0", size = 44330, upload-time = "2025-10-06T05:37:26.928Z" },
+    { url = "https://files.pythonhosted.org/packages/82/13/e6950121764f2676f43534c555249f57030150260aee9dcf7d64efda11dd/frozenlist-1.8.0-cp314-cp314-win_arm64.whl", hash = "sha256:d1eaff1d00c7751b7c6662e9c5ba6eb2c17a2306ba5e2a37f24ddf3cc953402b", size = 40627, upload-time = "2025-10-06T05:37:28.075Z" },
+    { url = "https://files.pythonhosted.org/packages/c0/c7/43200656ecc4e02d3f8bc248df68256cd9572b3f0017f0a0c4e93440ae23/frozenlist-1.8.0-cp314-cp314t-macosx_10_13_universal2.whl", hash = "sha256:d3bb933317c52d7ea5004a1c442eef86f426886fba134ef8cf4226ea6ee1821d", size = 89238, upload-time = "2025-10-06T05:37:29.373Z" },
+    { url = "https://files.pythonhosted.org/packages/d1/29/55c5f0689b9c0fb765055629f472c0de484dcaf0acee2f7707266ae3583c/frozenlist-1.8.0-cp314-cp314t-macosx_10_13_x86_64.whl", hash = "sha256:8009897cdef112072f93a0efdce29cd819e717fd2f649ee3016efd3cd885a7ed", size = 50738, upload-time = "2025-10-06T05:37:30.792Z" },
+    { url = "https://files.pythonhosted.org/packages/ba/7d/b7282a445956506fa11da8c2db7d276adcbf2b17d8bb8407a47685263f90/frozenlist-1.8.0-cp314-cp314t-macosx_11_0_arm64.whl", hash = "sha256:2c5dcbbc55383e5883246d11fd179782a9d07a986c40f49abe89ddf865913930", size = 51739, upload-time = "2025-10-06T05:37:32.127Z" },
+    { url = "https://files.pythonhosted.org/packages/62/1c/3d8622e60d0b767a5510d1d3cf21065b9db874696a51ea6d7a43180a259c/frozenlist-1.8.0-cp314-cp314t-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl", hash = "sha256:39ecbc32f1390387d2aa4f5a995e465e9e2f79ba3adcac92d68e3e0afae6657c", size = 284186, upload-time = "2025-10-06T05:37:33.21Z" },
+    { url = "https://files.pythonhosted.org/packages/2d/14/aa36d5f85a89679a85a1d44cd7a6657e0b1c75f61e7cad987b203d2daca8/frozenlist-1.8.0-cp314-cp314t-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:92db2bf818d5cc8d9c1f1fc56b897662e24ea5adb36ad1f1d82875bd64e03c24", size = 292196, upload-time = "2025-10-06T05:37:36.107Z" },
+    { url = "https://files.pythonhosted.org/packages/05/23/6bde59eb55abd407d34f77d39a5126fb7b4f109a3f611d3929f14b700c66/frozenlist-1.8.0-cp314-cp314t-manylinux2014_armv7l.manylinux_2_17_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:2dc43a022e555de94c3b68a4ef0b11c4f747d12c024a520c7101709a2144fb37", size = 273830, upload-time = "2025-10-06T05:37:37.663Z" },
+    { url = "https://files.pythonhosted.org/packages/d2/3f/22cff331bfad7a8afa616289000ba793347fcd7bc275f3b28ecea2a27909/frozenlist-1.8.0-cp314-cp314t-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:cb89a7f2de3602cfed448095bab3f178399646ab7c61454315089787df07733a", size = 294289, upload-time = "2025-10-06T05:37:39.261Z" },
+    { url = "https://files.pythonhosted.org/packages/a4/89/5b057c799de4838b6c69aa82b79705f2027615e01be996d2486a69ca99c4/frozenlist-1.8.0-cp314-cp314t-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:33139dc858c580ea50e7e60a1b0ea003efa1fd42e6ec7fdbad78fff65fad2fd2", size = 300318, upload-time = "2025-10-06T05:37:43.213Z" },
+    { url = "https://files.pythonhosted.org/packages/30/de/2c22ab3eb2a8af6d69dc799e48455813bab3690c760de58e1bf43b36da3e/frozenlist-1.8.0-cp314-cp314t-musllinux_1_2_aarch64.whl", hash = "sha256:168c0969a329b416119507ba30b9ea13688fafffac1b7822802537569a1cb0ef", size = 282814, upload-time = "2025-10-06T05:37:45.337Z" },
+    { url = "https://files.pythonhosted.org/packages/59/f7/970141a6a8dbd7f556d94977858cfb36fa9b66e0892c6dd780d2219d8cd8/frozenlist-1.8.0-cp314-cp314t-musllinux_1_2_armv7l.whl", hash = "sha256:28bd570e8e189d7f7b001966435f9dac6718324b5be2990ac496cf1ea9ddb7fe", size = 291762, upload-time = "2025-10-06T05:37:46.657Z" },
+    { url = "https://files.pythonhosted.org/packages/c1/15/ca1adae83a719f82df9116d66f5bb28bb95557b3951903d39135620ef157/frozenlist-1.8.0-cp314-cp314t-musllinux_1_2_ppc64le.whl", hash = "sha256:b2a095d45c5d46e5e79ba1e5b9cb787f541a8dee0433836cea4b96a2c439dcd8", size = 289470, upload-time = "2025-10-06T05:37:47.946Z" },
+    { url = "https://files.pythonhosted.org/packages/ac/83/dca6dc53bf657d371fbc88ddeb21b79891e747189c5de990b9dfff2ccba1/frozenlist-1.8.0-cp314-cp314t-musllinux_1_2_s390x.whl", hash = "sha256:eab8145831a0d56ec9c4139b6c3e594c7a83c2c8be25d5bcf2d86136a532287a", size = 289042, upload-time = "2025-10-06T05:37:49.499Z" },
+    { url = "https://files.pythonhosted.org/packages/96/52/abddd34ca99be142f354398700536c5bd315880ed0a213812bc491cff5e4/frozenlist-1.8.0-cp314-cp314t-musllinux_1_2_x86_64.whl", hash = "sha256:974b28cf63cc99dfb2188d8d222bc6843656188164848c4f679e63dae4b0708e", size = 283148, upload-time = "2025-10-06T05:37:50.745Z" },
+    { url = "https://files.pythonhosted.org/packages/af/d3/76bd4ed4317e7119c2b7f57c3f6934aba26d277acc6309f873341640e21f/frozenlist-1.8.0-cp314-cp314t-win32.whl", hash = "sha256:342c97bf697ac5480c0a7ec73cd700ecfa5a8a40ac923bd035484616efecc2df", size = 44676, upload-time = "2025-10-06T05:37:52.222Z" },
+    { url = "https://files.pythonhosted.org/packages/89/76/c615883b7b521ead2944bb3480398cbb07e12b7b4e4d073d3752eb721558/frozenlist-1.8.0-cp314-cp314t-win_amd64.whl", hash = "sha256:06be8f67f39c8b1dc671f5d83aaefd3358ae5cdcf8314552c57e7ed3e6475bdd", size = 49451, upload-time = "2025-10-06T05:37:53.425Z" },
+    { url = "https://files.pythonhosted.org/packages/e0/a3/5982da14e113d07b325230f95060e2169f5311b1017ea8af2a29b374c289/frozenlist-1.8.0-cp314-cp314t-win_arm64.whl", hash = "sha256:102e6314ca4da683dca92e3b1355490fed5f313b768500084fbe6371fddfdb79", size = 42507, upload-time = "2025-10-06T05:37:54.513Z" },
+    { url = "https://files.pythonhosted.org/packages/9a/9a/e35b4a917281c0b8419d4207f4334c8e8c5dbf4f3f5f9ada73958d937dcc/frozenlist-1.8.0-py3-none-any.whl", hash = "sha256:0c18a16eab41e82c295618a77502e17b195883241c563b00f0aa5106fc4eaa0d", size = 13409, upload-time = "2025-10-06T05:38:16.721Z" },
+]
+
 [[package]]
 name = "fsspec"
 version = "2025.12.0"
@@ -653,6 +925,53 @@ wheels = [
     { url = "https://files.pythonhosted.org/packages/01/61/d4b89fec821f72385526e1b9d9a3a0385dda4a72b206d28049e2c7cd39b8/gitpython-3.1.45-py3-none-any.whl", hash = "sha256:8908cb2e02fb3b93b7eb0f2827125cb699869470432cc885f019b8fd0fccff77", size = 208168, upload-time = "2025-07-24T03:45:52.517Z" },
 ]
 
+[[package]]
+name = "greenlet"
+version = "3.3.0"
+source = { registry = "https://pypi.org/simple" }
+sdist = { url = "https://files.pythonhosted.org/packages/c7/e5/40dbda2736893e3e53d25838e0f19a2b417dfc122b9989c91918db30b5d3/greenlet-3.3.0.tar.gz", hash = "sha256:a82bb225a4e9e4d653dd2fb7b8b2d36e4fb25bc0165422a11e48b88e9e6f78fb", size = 190651, upload-time = "2025-12-04T14:49:44.05Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/1f/cb/48e964c452ca2b92175a9b2dca037a553036cb053ba69e284650ce755f13/greenlet-3.3.0-cp311-cp311-macosx_11_0_universal2.whl", hash = "sha256:e29f3018580e8412d6aaf5641bb7745d38c85228dacf51a73bd4e26ddf2a6a8e", size = 274908, upload-time = "2025-12-04T14:23:26.435Z" },
+    { url = "https://files.pythonhosted.org/packages/28/da/38d7bff4d0277b594ec557f479d65272a893f1f2a716cad91efeb8680953/greenlet-3.3.0-cp311-cp311-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:a687205fb22794e838f947e2194c0566d3812966b41c78709554aa883183fb62", size = 577113, upload-time = "2025-12-04T14:50:05.493Z" },
+    { url = "https://files.pythonhosted.org/packages/3c/f2/89c5eb0faddc3ff014f1c04467d67dee0d1d334ab81fadbf3744847f8a8a/greenlet-3.3.0-cp311-cp311-manylinux_2_24_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:4243050a88ba61842186cb9e63c7dfa677ec146160b0efd73b855a3d9c7fcf32", size = 590338, upload-time = "2025-12-04T14:57:41.136Z" },
+    { url = "https://files.pythonhosted.org/packages/80/d7/db0a5085035d05134f8c089643da2b44cc9b80647c39e93129c5ef170d8f/greenlet-3.3.0-cp311-cp311-manylinux_2_24_s390x.manylinux_2_28_s390x.whl", hash = "sha256:670d0f94cd302d81796e37299bcd04b95d62403883b24225c6b5271466612f45", size = 601098, upload-time = "2025-12-04T15:07:11.898Z" },
+    { url = "https://files.pythonhosted.org/packages/dc/a6/e959a127b630a58e23529972dbc868c107f9d583b5a9f878fb858c46bc1a/greenlet-3.3.0-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:6cb3a8ec3db4a3b0eb8a3c25436c2d49e3505821802074969db017b87bc6a948", size = 590206, upload-time = "2025-12-04T14:26:01.254Z" },
+    { url = "https://files.pythonhosted.org/packages/48/60/29035719feb91798693023608447283b266b12efc576ed013dd9442364bb/greenlet-3.3.0-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:2de5a0b09eab81fc6a382791b995b1ccf2b172a9fec934747a7a23d2ff291794", size = 1550668, upload-time = "2025-12-04T15:04:22.439Z" },
+    { url = "https://files.pythonhosted.org/packages/0a/5f/783a23754b691bfa86bd72c3033aa107490deac9b2ef190837b860996c9f/greenlet-3.3.0-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:4449a736606bd30f27f8e1ff4678ee193bc47f6ca810d705981cfffd6ce0d8c5", size = 1615483, upload-time = "2025-12-04T14:27:28.083Z" },
+    { url = "https://files.pythonhosted.org/packages/1d/d5/c339b3b4bc8198b7caa4f2bd9fd685ac9f29795816d8db112da3d04175bb/greenlet-3.3.0-cp311-cp311-win_amd64.whl", hash = "sha256:7652ee180d16d447a683c04e4c5f6441bae7ba7b17ffd9f6b3aff4605e9e6f71", size = 301164, upload-time = "2025-12-04T14:42:51.577Z" },
+    { url = "https://files.pythonhosted.org/packages/f8/0a/a3871375c7b9727edaeeea994bfff7c63ff7804c9829c19309ba2e058807/greenlet-3.3.0-cp312-cp312-macosx_11_0_universal2.whl", hash = "sha256:b01548f6e0b9e9784a2c99c5651e5dc89ffcbe870bc5fb2e5ef864e9cc6b5dcb", size = 276379, upload-time = "2025-12-04T14:23:30.498Z" },
+    { url = "https://files.pythonhosted.org/packages/43/ab/7ebfe34dce8b87be0d11dae91acbf76f7b8246bf9d6b319c741f99fa59c6/greenlet-3.3.0-cp312-cp312-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:349345b770dc88f81506c6861d22a6ccd422207829d2c854ae2af8025af303e3", size = 597294, upload-time = "2025-12-04T14:50:06.847Z" },
+    { url = "https://files.pythonhosted.org/packages/a4/39/f1c8da50024feecd0793dbd5e08f526809b8ab5609224a2da40aad3a7641/greenlet-3.3.0-cp312-cp312-manylinux_2_24_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:e8e18ed6995e9e2c0b4ed264d2cf89260ab3ac7e13555b8032b25a74c6d18655", size = 607742, upload-time = "2025-12-04T14:57:42.349Z" },
+    { url = "https://files.pythonhosted.org/packages/77/cb/43692bcd5f7a0da6ec0ec6d58ee7cddb606d055ce94a62ac9b1aa481e969/greenlet-3.3.0-cp312-cp312-manylinux_2_24_s390x.manylinux_2_28_s390x.whl", hash = "sha256:c024b1e5696626890038e34f76140ed1daf858e37496d33f2af57f06189e70d7", size = 622297, upload-time = "2025-12-04T15:07:13.552Z" },
+    { url = "https://files.pythonhosted.org/packages/75/b0/6bde0b1011a60782108c01de5913c588cf51a839174538d266de15e4bf4d/greenlet-3.3.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:047ab3df20ede6a57c35c14bf5200fcf04039d50f908270d3f9a7a82064f543b", size = 609885, upload-time = "2025-12-04T14:26:02.368Z" },
+    { url = "https://files.pythonhosted.org/packages/49/0e/49b46ac39f931f59f987b7cd9f34bfec8ef81d2a1e6e00682f55be5de9f4/greenlet-3.3.0-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:2d9ad37fc657b1102ec880e637cccf20191581f75c64087a549e66c57e1ceb53", size = 1567424, upload-time = "2025-12-04T15:04:23.757Z" },
+    { url = "https://files.pythonhosted.org/packages/05/f5/49a9ac2dff7f10091935def9165c90236d8f175afb27cbed38fb1d61ab6b/greenlet-3.3.0-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:83cd0e36932e0e7f36a64b732a6f60c2fc2df28c351bae79fbaf4f8092fe7614", size = 1636017, upload-time = "2025-12-04T14:27:29.688Z" },
+    { url = "https://files.pythonhosted.org/packages/6c/79/3912a94cf27ec503e51ba493692d6db1e3cd8ac7ac52b0b47c8e33d7f4f9/greenlet-3.3.0-cp312-cp312-win_amd64.whl", hash = "sha256:a7a34b13d43a6b78abf828a6d0e87d3385680eaf830cd60d20d52f249faabf39", size = 301964, upload-time = "2025-12-04T14:36:58.316Z" },
+    { url = "https://files.pythonhosted.org/packages/02/2f/28592176381b9ab2cafa12829ba7b472d177f3acc35d8fbcf3673d966fff/greenlet-3.3.0-cp313-cp313-macosx_11_0_universal2.whl", hash = "sha256:a1e41a81c7e2825822f4e068c48cb2196002362619e2d70b148f20a831c00739", size = 275140, upload-time = "2025-12-04T14:23:01.282Z" },
+    { url = "https://files.pythonhosted.org/packages/2c/80/fbe937bf81e9fca98c981fe499e59a3f45df2a04da0baa5c2be0dca0d329/greenlet-3.3.0-cp313-cp313-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:9f515a47d02da4d30caaa85b69474cec77b7929b2e936ff7fb853d42f4bf8808", size = 599219, upload-time = "2025-12-04T14:50:08.309Z" },
+    { url = "https://files.pythonhosted.org/packages/c2/ff/7c985128f0514271b8268476af89aee6866df5eec04ac17dcfbc676213df/greenlet-3.3.0-cp313-cp313-manylinux_2_24_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:7d2d9fd66bfadf230b385fdc90426fcd6eb64db54b40c495b72ac0feb5766c54", size = 610211, upload-time = "2025-12-04T14:57:43.968Z" },
+    { url = "https://files.pythonhosted.org/packages/79/07/c47a82d881319ec18a4510bb30463ed6891f2ad2c1901ed5ec23d3de351f/greenlet-3.3.0-cp313-cp313-manylinux_2_24_s390x.manylinux_2_28_s390x.whl", hash = "sha256:30a6e28487a790417d036088b3bcb3f3ac7d8babaa7d0139edbaddebf3af9492", size = 624311, upload-time = "2025-12-04T15:07:14.697Z" },
+    { url = "https://files.pythonhosted.org/packages/fd/8e/424b8c6e78bd9837d14ff7df01a9829fc883ba2ab4ea787d4f848435f23f/greenlet-3.3.0-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:087ea5e004437321508a8d6f20efc4cfec5e3c30118e1417ea96ed1d93950527", size = 612833, upload-time = "2025-12-04T14:26:03.669Z" },
+    { url = "https://files.pythonhosted.org/packages/b5/ba/56699ff9b7c76ca12f1cdc27a886d0f81f2189c3455ff9f65246780f713d/greenlet-3.3.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:ab97cf74045343f6c60a39913fa59710e4bd26a536ce7ab2397adf8b27e67c39", size = 1567256, upload-time = "2025-12-04T15:04:25.276Z" },
+    { url = "https://files.pythonhosted.org/packages/1e/37/f31136132967982d698c71a281a8901daf1a8fbab935dce7c0cf15f942cc/greenlet-3.3.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:5375d2e23184629112ca1ea89a53389dddbffcf417dad40125713d88eb5f96e8", size = 1636483, upload-time = "2025-12-04T14:27:30.804Z" },
+    { url = "https://files.pythonhosted.org/packages/7e/71/ba21c3fb8c5dce83b8c01f458a42e99ffdb1963aeec08fff5a18588d8fd7/greenlet-3.3.0-cp313-cp313-win_amd64.whl", hash = "sha256:9ee1942ea19550094033c35d25d20726e4f1c40d59545815e1128ac58d416d38", size = 301833, upload-time = "2025-12-04T14:32:23.929Z" },
+    { url = "https://files.pythonhosted.org/packages/d7/7c/f0a6d0ede2c7bf092d00bc83ad5bafb7e6ec9b4aab2fbdfa6f134dc73327/greenlet-3.3.0-cp314-cp314-macosx_11_0_universal2.whl", hash = "sha256:60c2ef0f578afb3c8d92ea07ad327f9a062547137afe91f38408f08aacab667f", size = 275671, upload-time = "2025-12-04T14:23:05.267Z" },
+    { url = "https://files.pythonhosted.org/packages/44/06/dac639ae1a50f5969d82d2e3dd9767d30d6dbdbab0e1a54010c8fe90263c/greenlet-3.3.0-cp314-cp314-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:0a5d554d0712ba1de0a6c94c640f7aeba3f85b3a6e1f2899c11c2c0428da9365", size = 646360, upload-time = "2025-12-04T14:50:10.026Z" },
+    { url = "https://files.pythonhosted.org/packages/e0/94/0fb76fe6c5369fba9bf98529ada6f4c3a1adf19e406a47332245ef0eb357/greenlet-3.3.0-cp314-cp314-manylinux_2_24_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:3a898b1e9c5f7307ebbde4102908e6cbfcb9ea16284a3abe15cab996bee8b9b3", size = 658160, upload-time = "2025-12-04T14:57:45.41Z" },
+    { url = "https://files.pythonhosted.org/packages/93/79/d2c70cae6e823fac36c3bbc9077962105052b7ef81db2f01ec3b9bf17e2b/greenlet-3.3.0-cp314-cp314-manylinux_2_24_s390x.manylinux_2_28_s390x.whl", hash = "sha256:dcd2bdbd444ff340e8d6bdf54d2f206ccddbb3ccfdcd3c25bf4afaa7b8f0cf45", size = 671388, upload-time = "2025-12-04T15:07:15.789Z" },
+    { url = "https://files.pythonhosted.org/packages/b8/14/bab308fc2c1b5228c3224ec2bf928ce2e4d21d8046c161e44a2012b5203e/greenlet-3.3.0-cp314-cp314-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:5773edda4dc00e173820722711d043799d3adb4f01731f40619e07ea2750b955", size = 660166, upload-time = "2025-12-04T14:26:05.099Z" },
+    { url = "https://files.pythonhosted.org/packages/4b/d2/91465d39164eaa0085177f61983d80ffe746c5a1860f009811d498e7259c/greenlet-3.3.0-cp314-cp314-musllinux_1_2_aarch64.whl", hash = "sha256:ac0549373982b36d5fd5d30beb8a7a33ee541ff98d2b502714a09f1169f31b55", size = 1615193, upload-time = "2025-12-04T15:04:27.041Z" },
+    { url = "https://files.pythonhosted.org/packages/42/1b/83d110a37044b92423084d52d5d5a3b3a73cafb51b547e6d7366ff62eff1/greenlet-3.3.0-cp314-cp314-musllinux_1_2_x86_64.whl", hash = "sha256:d198d2d977460358c3b3a4dc844f875d1adb33817f0613f663a656f463764ccc", size = 1683653, upload-time = "2025-12-04T14:27:32.366Z" },
+    { url = "https://files.pythonhosted.org/packages/7c/9a/9030e6f9aa8fd7808e9c31ba4c38f87c4f8ec324ee67431d181fe396d705/greenlet-3.3.0-cp314-cp314-win_amd64.whl", hash = "sha256:73f51dd0e0bdb596fb0417e475fa3c5e32d4c83638296e560086b8d7da7c4170", size = 305387, upload-time = "2025-12-04T14:26:51.063Z" },
+    { url = "https://files.pythonhosted.org/packages/a0/66/bd6317bc5932accf351fc19f177ffba53712a202f9df10587da8df257c7e/greenlet-3.3.0-cp314-cp314t-macosx_11_0_universal2.whl", hash = "sha256:d6ed6f85fae6cdfdb9ce04c9bf7a08d666cfcfb914e7d006f44f840b46741931", size = 282638, upload-time = "2025-12-04T14:25:20.941Z" },
+    { url = "https://files.pythonhosted.org/packages/30/cf/cc81cb030b40e738d6e69502ccbd0dd1bced0588e958f9e757945de24404/greenlet-3.3.0-cp314-cp314t-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:d9125050fcf24554e69c4cacb086b87b3b55dc395a8b3ebe6487b045b2614388", size = 651145, upload-time = "2025-12-04T14:50:11.039Z" },
+    { url = "https://files.pythonhosted.org/packages/9c/ea/1020037b5ecfe95ca7df8d8549959baceb8186031da83d5ecceff8b08cd2/greenlet-3.3.0-cp314-cp314t-manylinux_2_24_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:87e63ccfa13c0a0f6234ed0add552af24cc67dd886731f2261e46e241608bee3", size = 654236, upload-time = "2025-12-04T14:57:47.007Z" },
+    { url = "https://files.pythonhosted.org/packages/69/cc/1e4bae2e45ca2fa55299f4e85854606a78ecc37fead20d69322f96000504/greenlet-3.3.0-cp314-cp314t-manylinux_2_24_s390x.manylinux_2_28_s390x.whl", hash = "sha256:2662433acbca297c9153a4023fe2161c8dcfdcc91f10433171cf7e7d94ba2221", size = 662506, upload-time = "2025-12-04T15:07:16.906Z" },
+    { url = "https://files.pythonhosted.org/packages/57/b9/f8025d71a6085c441a7eaff0fd928bbb275a6633773667023d19179fe815/greenlet-3.3.0-cp314-cp314t-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:3c6e9b9c1527a78520357de498b0e709fb9e2f49c3a513afd5a249007261911b", size = 653783, upload-time = "2025-12-04T14:26:06.225Z" },
+    { url = "https://files.pythonhosted.org/packages/f6/c7/876a8c7a7485d5d6b5c6821201d542ef28be645aa024cfe1145b35c120c1/greenlet-3.3.0-cp314-cp314t-musllinux_1_2_aarch64.whl", hash = "sha256:286d093f95ec98fdd92fcb955003b8a3d054b4e2cab3e2707a5039e7b50520fd", size = 1614857, upload-time = "2025-12-04T15:04:28.484Z" },
+    { url = "https://files.pythonhosted.org/packages/4f/dc/041be1dff9f23dac5f48a43323cd0789cb798342011c19a248d9c9335536/greenlet-3.3.0-cp314-cp314t-musllinux_1_2_x86_64.whl", hash = "sha256:6c10513330af5b8ae16f023e8ddbfb486ab355d04467c4679c5cfe4659975dd9", size = 1676034, upload-time = "2025-12-04T14:27:33.531Z" },
+]
+
 [[package]]
 name = "griffe"
 version = "1.15.0"
@@ -665,6 +984,15 @@ wheels = [
     { url = "https://files.pythonhosted.org/packages/9c/83/3b1d03d36f224edded98e9affd0467630fc09d766c0e56fb1498cbb04a9b/griffe-1.15.0-py3-none-any.whl", hash = "sha256:6f6762661949411031f5fcda9593f586e6ce8340f0ba88921a0f2ef7a81eb9a3", size = 150705, upload-time = "2025-11-10T15:03:13.549Z" },
 ]
 
+[[package]]
+name = "h11"
+version = "0.16.0"
+source = { registry = "https://pypi.org/simple" }
+sdist = { url = "https://files.pythonhosted.org/packages/01/ee/02a2c011bdab74c6fb3c75474d40b3052059d95df7e73351460c8588d963/h11-0.16.0.tar.gz", hash = "sha256:4e35b956cf45792e4caa5885e69fba00bdbc6ffafbfa020300e549b208ee5ff1", size = 101250, upload-time = "2025-04-24T03:35:25.427Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/04/4b/29cac41a4d98d144bf5f6d33995617b185d14b22401f75ca86f384e87ff1/h11-0.16.0-py3-none-any.whl", hash = "sha256:63cf8bbe7522de3bf65932fda1d9c2772064ffb3dae62d55932da54b31cb6c86", size = 37515, upload-time = "2025-04-24T03:35:24.344Z" },
+]
+
 [[package]]
 name = "hf-xet"
 version = "1.2.0"
@@ -694,6 +1022,43 @@ wheels = [
     { url = "https://files.pythonhosted.org/packages/cb/44/870d44b30e1dcfb6a65932e3e1506c103a8a5aea9103c337e7a53180322c/hf_xet-1.2.0-cp37-abi3-win_amd64.whl", hash = "sha256:e6584a52253f72c9f52f9e549d5895ca7a471608495c4ecaa6cc73dba2b24d69", size = 2905735, upload-time = "2025-10-24T19:04:35.928Z" },
 ]
 
+[[package]]
+name = "httpcore"
+version = "1.0.9"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "certifi" },
+    { name = "h11" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/06/94/82699a10bca87a5556c9c59b5963f2d039dbd239f25bc2a63907a05a14cb/httpcore-1.0.9.tar.gz", hash = "sha256:6e34463af53fd2ab5d807f399a9b45ea31c3dfa2276f15a2c3f00afff6e176e8", size = 85484, upload-time = "2025-04-24T22:06:22.219Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/7e/f5/f66802a942d491edb555dd61e3a9961140fd64c90bce1eafd741609d334d/httpcore-1.0.9-py3-none-any.whl", hash = "sha256:2d400746a40668fc9dec9810239072b40b4484b640a8c38fd654a024c7a1bf55", size = 78784, upload-time = "2025-04-24T22:06:20.566Z" },
+]
+
+[[package]]
+name = "httpx"
+version = "0.28.1"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "anyio" },
+    { name = "certifi" },
+    { name = "httpcore" },
+    { name = "idna" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/b1/df/48c586a5fe32a0f01324ee087459e112ebb7224f646c0b5023f5e79e9956/httpx-0.28.1.tar.gz", hash = "sha256:75e98c5f16b0f35b567856f597f06ff2270a374470a5c2392242528e3e3e42fc", size = 141406, upload-time = "2024-12-06T15:37:23.222Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/2a/39/e50c7c3a983047577ee07d2a9e53faf5a69493943ec3f6a384bdc792deb2/httpx-0.28.1-py3-none-any.whl", hash = "sha256:d909fcccc110f8c7faf814ca82a9a4d816bc5a6dbfea25d6591d6985b8ba59ad", size = 73517, upload-time = "2024-12-06T15:37:21.509Z" },
+]
+
+[[package]]
+name = "httpx-sse"
+version = "0.4.3"
+source = { registry = "https://pypi.org/simple" }
+sdist = { url = "https://files.pythonhosted.org/packages/0f/4c/751061ffa58615a32c31b2d82e8482be8dd4a89154f003147acee90f2be9/httpx_sse-0.4.3.tar.gz", hash = "sha256:9b1ed0127459a66014aec3c56bebd93da3c1bc8bb6618c8082039a44889a755d", size = 15943, upload-time = "2025-10-10T21:48:22.271Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/d2/fd/6668e5aec43ab844de6fc74927e155a3b37bf40d7c3790e49fc0406b6578/httpx_sse-0.4.3-py3-none-any.whl", hash = "sha256:0ac1c9fe3c0afad2e0ebb25a934a59f4c7823b60792691f779fad2c5568830fc", size = 8960, upload-time = "2025-10-10T21:48:21.158Z" },
+]
+
 [[package]]
 name = "huggingface-hub"
 version = "0.36.0"
@@ -803,6 +1168,27 @@ wheels = [
     { url = "https://files.pythonhosted.org/packages/2d/14/1c65fccf8413d5f5c6e8425f84675169654395098000d8bddc4e9d3390e1/jsbeautifier-1.15.4-py3-none-any.whl", hash = "sha256:72f65de312a3f10900d7685557f84cb61a9733c50dcc27271a39f5b0051bf528", size = 94707, upload-time = "2025-02-27T17:53:46.152Z" },
 ]
 
+[[package]]
+name = "jsonpatch"
+version = "1.33"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "jsonpointer" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/42/78/18813351fe5d63acad16aec57f94ec2b70a09e53ca98145589e185423873/jsonpatch-1.33.tar.gz", hash = "sha256:9fcd4009c41e6d12348b4a0ff2563ba56a2923a7dfee731d004e212e1ee5030c", size = 21699, upload-time = "2023-06-26T12:07:29.144Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/73/07/02e16ed01e04a374e644b575638ec7987ae846d25ad97bcc9945a3ee4b0e/jsonpatch-1.33-py2.py3-none-any.whl", hash = "sha256:0ae28c0cd062bbd8b8ecc26d7d164fbbea9652a1a3693f3b956c1eae5145dade", size = 12898, upload-time = "2023-06-16T21:01:28.466Z" },
+]
+
+[[package]]
+name = "jsonpointer"
+version = "3.0.0"
+source = { registry = "https://pypi.org/simple" }
+sdist = { url = "https://files.pythonhosted.org/packages/6a/0a/eebeb1fa92507ea94016a2a790b93c2ae41a7e18778f85471dc54475ed25/jsonpointer-3.0.0.tar.gz", hash = "sha256:2b2d729f2091522d61c3b31f82e11870f60b68f43fbc705cb76bf4b832af59ef", size = 9114, upload-time = "2024-06-10T19:24:42.462Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/71/92/5e77f98553e9e75130c78900d000368476aed74276eb8ae8796f65f00918/jsonpointer-3.0.0-py2.py3-none-any.whl", hash = "sha256:13e088adc14fca8b6aa8177c044e12701e6ad4b28ff10e65f2267a90109c9942", size = 7595, upload-time = "2024-06-10T19:24:40.698Z" },
+]
+
 [[package]]
 name = "jsonschema"
 version = "4.25.1"
@@ -920,6 +1306,167 @@ wheels = [
     { url = "https://files.pythonhosted.org/packages/da/e9/0d4add7873a73e462aeb45c036a2dead2562b825aa46ba326727b3f31016/kiwisolver-1.4.9-pp311-pypy311_pp73-win_amd64.whl", hash = "sha256:fb940820c63a9590d31d88b815e7a3aa5915cad3ce735ab45f0c730b39547de1", size = 73929, upload-time = "2025-08-10T21:27:48.236Z" },
 ]
 
+[[package]]
+name = "langchain"
+version = "1.2.0"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "langchain-core" },
+    { name = "langgraph" },
+    { name = "pydantic" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/b1/12/3a74c22abdfddd877dfc2ee666d516f9132877fcd25eb4dd694835c59c79/langchain-1.2.0.tar.gz", hash = "sha256:a087d1e2b2969819e29a91a6d5f98302aafe31bd49ba377ecee3bf5a5dcfe14a", size = 536126, upload-time = "2025-12-15T14:51:42.24Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/23/00/4e3fa0d90f5a5c376ccb8ca983d0f0f7287783dfac48702e18f01d24673b/langchain-1.2.0-py3-none-any.whl", hash = "sha256:82f0d17aa4fbb11560b30e1e7d4aeb75e3ad71ce09b85c90ab208b181a24ffac", size = 102828, upload-time = "2025-12-15T14:51:40.802Z" },
+]
+
+[[package]]
+name = "langchain-classic"
+version = "1.0.1"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "langchain-core" },
+    { name = "langchain-text-splitters" },
+    { name = "langsmith" },
+    { name = "pydantic" },
+    { name = "pyyaml" },
+    { name = "requests" },
+    { name = "sqlalchemy" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/7c/4b/bd03518418ece4c13192a504449b58c28afee915dc4a6f4b02622458cb1b/langchain_classic-1.0.1.tar.gz", hash = "sha256:40a499684df36b005a1213735dc7f8dca8f5eb67978d6ec763e7a49780864fdc", size = 10516020, upload-time = "2025-12-23T22:55:22.615Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/83/0f/eab87f017d7fe28e8c11fff614f4cdbfae32baadb77d0f79e9f922af1df2/langchain_classic-1.0.1-py3-none-any.whl", hash = "sha256:131d83a02bb80044c68fedc1ab4ae885d5b8f8c2c742d8ab9e7534ad9cda8e80", size = 1040666, upload-time = "2025-12-23T22:55:21.025Z" },
+]
+
+[[package]]
+name = "langchain-community"
+version = "0.4.1"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "aiohttp" },
+    { name = "dataclasses-json" },
+    { name = "httpx-sse" },
+    { name = "langchain-classic" },
+    { name = "langchain-core" },
+    { name = "langsmith" },
+    { name = "numpy" },
+    { name = "pydantic-settings" },
+    { name = "pyyaml" },
+    { name = "requests" },
+    { name = "sqlalchemy" },
+    { name = "tenacity" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/53/97/a03585d42b9bdb6fbd935282d6e3348b10322a24e6ce12d0c99eb461d9af/langchain_community-0.4.1.tar.gz", hash = "sha256:f3b211832728ee89f169ddce8579b80a085222ddb4f4ed445a46e977d17b1e85", size = 33241144, upload-time = "2025-10-27T15:20:32.504Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/f0/a4/c4fde67f193401512337456cabc2148f2c43316e445f5decd9f8806e2992/langchain_community-0.4.1-py3-none-any.whl", hash = "sha256:2135abb2c7748a35c84613108f7ebf30f8505b18c3c18305ffaecfc7651f6c6a", size = 2533285, upload-time = "2025-10-27T15:20:30.767Z" },
+]
+
+[[package]]
+name = "langchain-core"
+version = "1.2.5"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "jsonpatch" },
+    { name = "langsmith" },
+    { name = "packaging" },
+    { name = "pydantic" },
+    { name = "pyyaml" },
+    { name = "tenacity" },
+    { name = "typing-extensions" },
+    { name = "uuid-utils" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/c8/86/bd678d69341ae4178bc8dfa04024d63636e5d580ff03d4502c8bc2262917/langchain_core-1.2.5.tar.gz", hash = "sha256:d674f6df42f07e846859b9d3afe547cad333d6bf9763e92c88eb4f8aaedcd3cc", size = 820445, upload-time = "2025-12-22T23:45:32.041Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/83/bd/9df897cbc98290bf71140104ee5b9777cf5291afb80333aa7da5a497339b/langchain_core-1.2.5-py3-none-any.whl", hash = "sha256:3255944ef4e21b2551facb319bfc426057a40247c0a05de5bd6f2fc021fbfa34", size = 484851, upload-time = "2025-12-22T23:45:30.525Z" },
+]
+
+[[package]]
+name = "langchain-text-splitters"
+version = "1.1.0"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "langchain-core" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/41/42/c178dcdc157b473330eb7cc30883ea69b8ec60078c7b85e2d521054c4831/langchain_text_splitters-1.1.0.tar.gz", hash = "sha256:75e58acb7585dc9508f3cd9d9809cb14751283226c2d6e21fb3a9ae57582ca22", size = 272230, upload-time = "2025-12-14T01:15:38.659Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/d8/1a/a84ed1c046deecf271356b0179c1b9fba95bfdaa6f934e1849dee26fad7b/langchain_text_splitters-1.1.0-py3-none-any.whl", hash = "sha256:f00341fe883358786104a5f881375ac830a4dd40253ecd42b4c10536c6e4693f", size = 34182, upload-time = "2025-12-14T01:15:37.382Z" },
+]
+
+[[package]]
+name = "langgraph"
+version = "1.0.5"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "langchain-core" },
+    { name = "langgraph-checkpoint" },
+    { name = "langgraph-prebuilt" },
+    { name = "langgraph-sdk" },
+    { name = "pydantic" },
+    { name = "xxhash" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/7d/47/28f4d4d33d88f69de26f7a54065961ac0c662cec2479b36a2db081ef5cb6/langgraph-1.0.5.tar.gz", hash = "sha256:7f6ae59622386b60fe9fa0ad4c53f42016b668455ed604329e7dc7904adbf3f8", size = 493969, upload-time = "2025-12-12T23:05:48.224Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/23/1b/e318ee76e42d28f515d87356ac5bd7a7acc8bad3b8f54ee377bef62e1cbf/langgraph-1.0.5-py3-none-any.whl", hash = "sha256:b4cfd173dca3c389735b47228ad8b295e6f7b3df779aba3a1e0c23871f81281e", size = 157056, upload-time = "2025-12-12T23:05:46.499Z" },
+]
+
+[[package]]
+name = "langgraph-checkpoint"
+version = "3.0.1"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "langchain-core" },
+    { name = "ormsgpack" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/0f/07/2b1c042fa87d40cf2db5ca27dc4e8dd86f9a0436a10aa4361a8982718ae7/langgraph_checkpoint-3.0.1.tar.gz", hash = "sha256:59222f875f85186a22c494aedc65c4e985a3df27e696e5016ba0b98a5ed2cee0", size = 137785, upload-time = "2025-11-04T21:55:47.774Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/48/e3/616e3a7ff737d98c1bbb5700dd62278914e2a9ded09a79a1fa93cf24ce12/langgraph_checkpoint-3.0.1-py3-none-any.whl", hash = "sha256:9b04a8d0edc0474ce4eaf30c5d731cee38f11ddff50a6177eead95b5c4e4220b", size = 46249, upload-time = "2025-11-04T21:55:46.472Z" },
+]
+
+[[package]]
+name = "langgraph-prebuilt"
+version = "1.0.5"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "langchain-core" },
+    { name = "langgraph-checkpoint" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/46/f9/54f8891b32159e4542236817aea2ee83de0de18bce28e9bdba08c7f93001/langgraph_prebuilt-1.0.5.tar.gz", hash = "sha256:85802675ad778cc7240fd02d47db1e0b59c0c86d8369447d77ce47623845db2d", size = 144453, upload-time = "2025-11-20T16:47:39.23Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/87/5e/aeba4a5b39fe6e874e0dd003a82da71c7153e671312671a8dacc5cb7c1af/langgraph_prebuilt-1.0.5-py3-none-any.whl", hash = "sha256:22369563e1848862ace53fbc11b027c28dd04a9ac39314633bb95f2a7e258496", size = 35072, upload-time = "2025-11-20T16:47:38.187Z" },
+]
+
+[[package]]
+name = "langgraph-sdk"
+version = "0.3.1"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "httpx" },
+    { name = "orjson" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/a9/d3/b6be0b0aba2a53a8920a2b0b4328a83121ec03eea9952e576d06a4182f6f/langgraph_sdk-0.3.1.tar.gz", hash = "sha256:f6dadfd2444eeff3e01405a9005c95fb3a028d4bd954ebec80ea6150084f92bb", size = 130312, upload-time = "2025-12-18T22:11:47.42Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/ab/fe/0c1c9c01a154eba62b20b02fabe811fd94a2b810061ae9e4d8462b8cf85a/langgraph_sdk-0.3.1-py3-none-any.whl", hash = "sha256:0b856923bfd20bf3441ce9d03bef488aa333fb610e972618799a9d584436acad", size = 66517, upload-time = "2025-12-18T22:11:46.625Z" },
+]
+
+[[package]]
+name = "langsmith"
+version = "0.5.1"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "httpx" },
+    { name = "orjson", marker = "platform_python_implementation != 'PyPy'" },
+    { name = "packaging" },
+    { name = "pydantic" },
+    { name = "requests" },
+    { name = "requests-toolbelt" },
+    { name = "uuid-utils" },
+    { name = "zstandard" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/fb/92/967ba83ec40448f46e23f231731b1564207af5ffba32aecef4e1f2f9f83f/langsmith-0.5.1.tar.gz", hash = "sha256:6a10b38cb4ce58941b7f1dbdf41a461868605dd0162bf05d17690f2e4b6e50e7", size = 871631, upload-time = "2025-12-24T19:50:24.823Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/19/67/1720b01e58d3487a44c780a86aabad95d9eaaf6b2fa8d0718c98f0eca18d/langsmith-0.5.1-py3-none-any.whl", hash = "sha256:70aa2a4c75add3f723c3bbac80dbb8adc575077834d3a733ee1ec133206ff351", size = 275527, upload-time = "2025-12-24T19:50:22.808Z" },
+]
+
 [[package]]
 name = "librt"
 version = "0.7.3"
@@ -1096,6 +1643,18 @@ wheels = [
     { url = "https://files.pythonhosted.org/packages/70/bc/6f1c2f612465f5fa89b95bead1f44dcb607670fd42891d8fdcd5d039f4f4/markupsafe-3.0.3-cp314-cp314t-win_arm64.whl", hash = "sha256:32001d6a8fc98c8cb5c947787c5d08b0a50663d139f1305bac5885d98d9b40fa", size = 14146, upload-time = "2025-09-27T18:37:28.327Z" },
 ]
 
+[[package]]
+name = "marshmallow"
+version = "3.26.2"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "packaging" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/55/79/de6c16cc902f4fc372236926b0ce2ab7845268dcc30fb2fbb7f71b418631/marshmallow-3.26.2.tar.gz", hash = "sha256:bbe2adb5a03e6e3571b573f42527c6fe926e17467833660bebd11593ab8dfd57", size = 222095, upload-time = "2025-12-22T06:53:53.309Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/be/2f/5108cb3ee4ba6501748c4908b908e55f42a5b66245b4cfe0c99326e1ef6e/marshmallow-3.26.2-py3-none-any.whl", hash = "sha256:013fa8a3c4c276c24d26d84ce934dc964e2aa794345a0f8c7e5a7191482c8a73", size = 50964, upload-time = "2025-12-22T06:53:51.801Z" },
+]
+
 [[package]]
 name = "matplotlib"
 version = "3.10.7"
@@ -1323,6 +1882,123 @@ wheels = [
     { url = "https://files.pythonhosted.org/packages/43/e3/7d92a15f894aa0c9c4b49b8ee9ac9850d6e63b03c9c32c0367a13ae62209/mpmath-1.3.0-py3-none-any.whl", hash = "sha256:a0b2b9fe80bbcd81a6647ff13108738cfb482d481d826cc0e02f5b35e5c88d2c", size = 536198, upload-time = "2023-03-07T16:47:09.197Z" },
 ]
 
+[[package]]
+name = "multidict"
+version = "6.7.0"
+source = { registry = "https://pypi.org/simple" }
+sdist = { url = "https://files.pythonhosted.org/packages/80/1e/5492c365f222f907de1039b91f922b93fa4f764c713ee858d235495d8f50/multidict-6.7.0.tar.gz", hash = "sha256:c6e99d9a65ca282e578dfea819cfa9c0a62b2499d8677392e09feaf305e9e6f5", size = 101834, upload-time = "2025-10-06T14:52:30.657Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/34/9e/5c727587644d67b2ed479041e4b1c58e30afc011e3d45d25bbe35781217c/multidict-6.7.0-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:4d409aa42a94c0b3fa617708ef5276dfe81012ba6753a0370fcc9d0195d0a1fc", size = 76604, upload-time = "2025-10-06T14:48:54.277Z" },
+    { url = "https://files.pythonhosted.org/packages/17/e4/67b5c27bd17c085a5ea8f1ec05b8a3e5cba0ca734bfcad5560fb129e70ca/multidict-6.7.0-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:14c9e076eede3b54c636f8ce1c9c252b5f057c62131211f0ceeec273810c9721", size = 44715, upload-time = "2025-10-06T14:48:55.445Z" },
+    { url = "https://files.pythonhosted.org/packages/4d/e1/866a5d77be6ea435711bef2a4291eed11032679b6b28b56b4776ab06ba3e/multidict-6.7.0-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:4c09703000a9d0fa3c3404b27041e574cc7f4df4c6563873246d0e11812a94b6", size = 44332, upload-time = "2025-10-06T14:48:56.706Z" },
+    { url = "https://files.pythonhosted.org/packages/31/61/0c2d50241ada71ff61a79518db85ada85fdabfcf395d5968dae1cbda04e5/multidict-6.7.0-cp311-cp311-manylinux1_i686.manylinux_2_28_i686.manylinux_2_5_i686.whl", hash = "sha256:a265acbb7bb33a3a2d626afbe756371dce0279e7b17f4f4eda406459c2b5ff1c", size = 245212, upload-time = "2025-10-06T14:48:58.042Z" },
+    { url = "https://files.pythonhosted.org/packages/ac/e0/919666a4e4b57fff1b57f279be1c9316e6cdc5de8a8b525d76f6598fefc7/multidict-6.7.0-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:51cb455de290ae462593e5b1cb1118c5c22ea7f0d3620d9940bf695cea5a4bd7", size = 246671, upload-time = "2025-10-06T14:49:00.004Z" },
+    { url = "https://files.pythonhosted.org/packages/a1/cc/d027d9c5a520f3321b65adea289b965e7bcbd2c34402663f482648c716ce/multidict-6.7.0-cp311-cp311-manylinux2014_armv7l.manylinux_2_17_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:db99677b4457c7a5c5a949353e125ba72d62b35f74e26da141530fbb012218a7", size = 225491, upload-time = "2025-10-06T14:49:01.393Z" },
+    { url = "https://files.pythonhosted.org/packages/75/c4/bbd633980ce6155a28ff04e6a6492dd3335858394d7bb752d8b108708558/multidict-6.7.0-cp311-cp311-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:f470f68adc395e0183b92a2f4689264d1ea4b40504a24d9882c27375e6662bb9", size = 257322, upload-time = "2025-10-06T14:49:02.745Z" },
+    { url = "https://files.pythonhosted.org/packages/4c/6d/d622322d344f1f053eae47e033b0b3f965af01212de21b10bcf91be991fb/multidict-6.7.0-cp311-cp311-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:0db4956f82723cc1c270de9c6e799b4c341d327762ec78ef82bb962f79cc07d8", size = 254694, upload-time = "2025-10-06T14:49:04.15Z" },
+    { url = "https://files.pythonhosted.org/packages/a8/9f/78f8761c2705d4c6d7516faed63c0ebdac569f6db1bef95e0d5218fdc146/multidict-6.7.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:3e56d780c238f9e1ae66a22d2adf8d16f485381878250db8d496623cd38b22bd", size = 246715, upload-time = "2025-10-06T14:49:05.967Z" },
+    { url = "https://files.pythonhosted.org/packages/78/59/950818e04f91b9c2b95aab3d923d9eabd01689d0dcd889563988e9ea0fd8/multidict-6.7.0-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:9d14baca2ee12c1a64740d4531356ba50b82543017f3ad6de0deb943c5979abb", size = 243189, upload-time = "2025-10-06T14:49:07.37Z" },
+    { url = "https://files.pythonhosted.org/packages/7a/3d/77c79e1934cad2ee74991840f8a0110966d9599b3af95964c0cd79bb905b/multidict-6.7.0-cp311-cp311-musllinux_1_2_armv7l.whl", hash = "sha256:295a92a76188917c7f99cda95858c822f9e4aae5824246bba9b6b44004ddd0a6", size = 237845, upload-time = "2025-10-06T14:49:08.759Z" },
+    { url = "https://files.pythonhosted.org/packages/63/1b/834ce32a0a97a3b70f86437f685f880136677ac00d8bce0027e9fd9c2db7/multidict-6.7.0-cp311-cp311-musllinux_1_2_i686.whl", hash = "sha256:39f1719f57adbb767ef592a50ae5ebb794220d1188f9ca93de471336401c34d2", size = 246374, upload-time = "2025-10-06T14:49:10.574Z" },
+    { url = "https://files.pythonhosted.org/packages/23/ef/43d1c3ba205b5dec93dc97f3fba179dfa47910fc73aaaea4f7ceb41cec2a/multidict-6.7.0-cp311-cp311-musllinux_1_2_ppc64le.whl", hash = "sha256:0a13fb8e748dfc94749f622de065dd5c1def7e0d2216dba72b1d8069a389c6ff", size = 253345, upload-time = "2025-10-06T14:49:12.331Z" },
+    { url = "https://files.pythonhosted.org/packages/6b/03/eaf95bcc2d19ead522001f6a650ef32811aa9e3624ff0ad37c445c7a588c/multidict-6.7.0-cp311-cp311-musllinux_1_2_s390x.whl", hash = "sha256:e3aa16de190d29a0ea1b48253c57d99a68492c8dd8948638073ab9e74dc9410b", size = 246940, upload-time = "2025-10-06T14:49:13.821Z" },
+    { url = "https://files.pythonhosted.org/packages/e8/df/ec8a5fd66ea6cd6f525b1fcbb23511b033c3e9bc42b81384834ffa484a62/multidict-6.7.0-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:a048ce45dcdaaf1defb76b2e684f997fb5abf74437b6cb7b22ddad934a964e34", size = 242229, upload-time = "2025-10-06T14:49:15.603Z" },
+    { url = "https://files.pythonhosted.org/packages/8a/a2/59b405d59fd39ec86d1142630e9049243015a5f5291ba49cadf3c090c541/multidict-6.7.0-cp311-cp311-win32.whl", hash = "sha256:a90af66facec4cebe4181b9e62a68be65e45ac9b52b67de9eec118701856e7ff", size = 41308, upload-time = "2025-10-06T14:49:16.871Z" },
+    { url = "https://files.pythonhosted.org/packages/32/0f/13228f26f8b882c34da36efa776c3b7348455ec383bab4a66390e42963ae/multidict-6.7.0-cp311-cp311-win_amd64.whl", hash = "sha256:95b5ffa4349df2887518bb839409bcf22caa72d82beec453216802f475b23c81", size = 46037, upload-time = "2025-10-06T14:49:18.457Z" },
+    { url = "https://files.pythonhosted.org/packages/84/1f/68588e31b000535a3207fd3c909ebeec4fb36b52c442107499c18a896a2a/multidict-6.7.0-cp311-cp311-win_arm64.whl", hash = "sha256:329aa225b085b6f004a4955271a7ba9f1087e39dcb7e65f6284a988264a63912", size = 43023, upload-time = "2025-10-06T14:49:19.648Z" },
+    { url = "https://files.pythonhosted.org/packages/c2/9e/9f61ac18d9c8b475889f32ccfa91c9f59363480613fc807b6e3023d6f60b/multidict-6.7.0-cp312-cp312-macosx_10_13_universal2.whl", hash = "sha256:8a3862568a36d26e650a19bb5cbbba14b71789032aebc0423f8cc5f150730184", size = 76877, upload-time = "2025-10-06T14:49:20.884Z" },
+    { url = "https://files.pythonhosted.org/packages/38/6f/614f09a04e6184f8824268fce4bc925e9849edfa654ddd59f0b64508c595/multidict-6.7.0-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:960c60b5849b9b4f9dcc9bea6e3626143c252c74113df2c1540aebce70209b45", size = 45467, upload-time = "2025-10-06T14:49:22.054Z" },
+    { url = "https://files.pythonhosted.org/packages/b3/93/c4f67a436dd026f2e780c433277fff72be79152894d9fc36f44569cab1a6/multidict-6.7.0-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:2049be98fb57a31b4ccf870bf377af2504d4ae35646a19037ec271e4c07998aa", size = 43834, upload-time = "2025-10-06T14:49:23.566Z" },
+    { url = "https://files.pythonhosted.org/packages/7f/f5/013798161ca665e4a422afbc5e2d9e4070142a9ff8905e482139cd09e4d0/multidict-6.7.0-cp312-cp312-manylinux1_i686.manylinux_2_28_i686.manylinux_2_5_i686.whl", hash = "sha256:0934f3843a1860dd465d38895c17fce1f1cb37295149ab05cd1b9a03afacb2a7", size = 250545, upload-time = "2025-10-06T14:49:24.882Z" },
+    { url = "https://files.pythonhosted.org/packages/71/2f/91dbac13e0ba94669ea5119ba267c9a832f0cb65419aca75549fcf09a3dc/multidict-6.7.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:b3e34f3a1b8131ba06f1a73adab24f30934d148afcd5f5de9a73565a4404384e", size = 258305, upload-time = "2025-10-06T14:49:26.778Z" },
+    { url = "https://files.pythonhosted.org/packages/ef/b0/754038b26f6e04488b48ac621f779c341338d78503fb45403755af2df477/multidict-6.7.0-cp312-cp312-manylinux2014_armv7l.manylinux_2_17_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:efbb54e98446892590dc2458c19c10344ee9a883a79b5cec4bc34d6656e8d546", size = 242363, upload-time = "2025-10-06T14:49:28.562Z" },
+    { url = "https://files.pythonhosted.org/packages/87/15/9da40b9336a7c9fa606c4cf2ed80a649dffeb42b905d4f63a1d7eb17d746/multidict-6.7.0-cp312-cp312-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:a35c5fc61d4f51eb045061e7967cfe3123d622cd500e8868e7c0c592a09fedc4", size = 268375, upload-time = "2025-10-06T14:49:29.96Z" },
+    { url = "https://files.pythonhosted.org/packages/82/72/c53fcade0cc94dfaad583105fd92b3a783af2091eddcb41a6d5a52474000/multidict-6.7.0-cp312-cp312-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:29fe6740ebccba4175af1b9b87bf553e9c15cd5868ee967e010efcf94e4fd0f1", size = 269346, upload-time = "2025-10-06T14:49:31.404Z" },
+    { url = "https://files.pythonhosted.org/packages/0d/e2/9baffdae21a76f77ef8447f1a05a96ec4bc0a24dae08767abc0a2fe680b8/multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:123e2a72e20537add2f33a79e605f6191fba2afda4cbb876e35c1a7074298a7d", size = 256107, upload-time = "2025-10-06T14:49:32.974Z" },
+    { url = "https://files.pythonhosted.org/packages/3c/06/3f06f611087dc60d65ef775f1fb5aca7c6d61c6db4990e7cda0cef9b1651/multidict-6.7.0-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:b284e319754366c1aee2267a2036248b24eeb17ecd5dc16022095e747f2f4304", size = 253592, upload-time = "2025-10-06T14:49:34.52Z" },
+    { url = "https://files.pythonhosted.org/packages/20/24/54e804ec7945b6023b340c412ce9c3f81e91b3bf5fa5ce65558740141bee/multidict-6.7.0-cp312-cp312-musllinux_1_2_armv7l.whl", hash = "sha256:803d685de7be4303b5a657b76e2f6d1240e7e0a8aa2968ad5811fa2285553a12", size = 251024, upload-time = "2025-10-06T14:49:35.956Z" },
+    { url = "https://files.pythonhosted.org/packages/14/48/011cba467ea0b17ceb938315d219391d3e421dfd35928e5dbdc3f4ae76ef/multidict-6.7.0-cp312-cp312-musllinux_1_2_i686.whl", hash = "sha256:c04a328260dfd5db8c39538f999f02779012268f54614902d0afc775d44e0a62", size = 251484, upload-time = "2025-10-06T14:49:37.631Z" },
+    { url = "https://files.pythonhosted.org/packages/0d/2f/919258b43bb35b99fa127435cfb2d91798eb3a943396631ef43e3720dcf4/multidict-6.7.0-cp312-cp312-musllinux_1_2_ppc64le.whl", hash = "sha256:8a19cdb57cd3df4cd865849d93ee14920fb97224300c88501f16ecfa2604b4e0", size = 263579, upload-time = "2025-10-06T14:49:39.502Z" },
+    { url = "https://files.pythonhosted.org/packages/31/22/a0e884d86b5242b5a74cf08e876bdf299e413016b66e55511f7a804a366e/multidict-6.7.0-cp312-cp312-musllinux_1_2_s390x.whl", hash = "sha256:9b2fd74c52accced7e75de26023b7dccee62511a600e62311b918ec5c168fc2a", size = 259654, upload-time = "2025-10-06T14:49:41.32Z" },
+    { url = "https://files.pythonhosted.org/packages/b2/e5/17e10e1b5c5f5a40f2fcbb45953c9b215f8a4098003915e46a93f5fcaa8f/multidict-6.7.0-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:3e8bfdd0e487acf992407a140d2589fe598238eaeffa3da8448d63a63cd363f8", size = 251511, upload-time = "2025-10-06T14:49:46.021Z" },
+    { url = "https://files.pythonhosted.org/packages/e3/9a/201bb1e17e7af53139597069c375e7b0dcbd47594604f65c2d5359508566/multidict-6.7.0-cp312-cp312-win32.whl", hash = "sha256:dd32a49400a2c3d52088e120ee00c1e3576cbff7e10b98467962c74fdb762ed4", size = 41895, upload-time = "2025-10-06T14:49:48.718Z" },
+    { url = "https://files.pythonhosted.org/packages/46/e2/348cd32faad84eaf1d20cce80e2bb0ef8d312c55bca1f7fa9865e7770aaf/multidict-6.7.0-cp312-cp312-win_amd64.whl", hash = "sha256:92abb658ef2d7ef22ac9f8bb88e8b6c3e571671534e029359b6d9e845923eb1b", size = 46073, upload-time = "2025-10-06T14:49:50.28Z" },
+    { url = "https://files.pythonhosted.org/packages/25/ec/aad2613c1910dce907480e0c3aa306905830f25df2e54ccc9dea450cb5aa/multidict-6.7.0-cp312-cp312-win_arm64.whl", hash = "sha256:490dab541a6a642ce1a9d61a4781656b346a55c13038f0b1244653828e3a83ec", size = 43226, upload-time = "2025-10-06T14:49:52.304Z" },
+    { url = "https://files.pythonhosted.org/packages/d2/86/33272a544eeb36d66e4d9a920602d1a2f57d4ebea4ef3cdfe5a912574c95/multidict-6.7.0-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:bee7c0588aa0076ce77c0ea5d19a68d76ad81fcd9fe8501003b9a24f9d4000f6", size = 76135, upload-time = "2025-10-06T14:49:54.26Z" },
+    { url = "https://files.pythonhosted.org/packages/91/1c/eb97db117a1ebe46d457a3d235a7b9d2e6dcab174f42d1b67663dd9e5371/multidict-6.7.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:7ef6b61cad77091056ce0e7ce69814ef72afacb150b7ac6a3e9470def2198159", size = 45117, upload-time = "2025-10-06T14:49:55.82Z" },
+    { url = "https://files.pythonhosted.org/packages/f1/d8/6c3442322e41fb1dd4de8bd67bfd11cd72352ac131f6368315617de752f1/multidict-6.7.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:9c0359b1ec12b1d6849c59f9d319610b7f20ef990a6d454ab151aa0e3b9f78ca", size = 43472, upload-time = "2025-10-06T14:49:57.048Z" },
+    { url = "https://files.pythonhosted.org/packages/75/3f/e2639e80325af0b6c6febdf8e57cc07043ff15f57fa1ef808f4ccb5ac4cd/multidict-6.7.0-cp313-cp313-manylinux1_i686.manylinux_2_28_i686.manylinux_2_5_i686.whl", hash = "sha256:cd240939f71c64bd658f186330603aac1a9a81bf6273f523fca63673cb7378a8", size = 249342, upload-time = "2025-10-06T14:49:58.368Z" },
+    { url = "https://files.pythonhosted.org/packages/5d/cc/84e0585f805cbeaa9cbdaa95f9a3d6aed745b9d25700623ac89a6ecff400/multidict-6.7.0-cp313-cp313-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:a60a4d75718a5efa473ebd5ab685786ba0c67b8381f781d1be14da49f1a2dc60", size = 257082, upload-time = "2025-10-06T14:49:59.89Z" },
+    { url = "https://files.pythonhosted.org/packages/b0/9c/ac851c107c92289acbbf5cfb485694084690c1b17e555f44952c26ddc5bd/multidict-6.7.0-cp313-cp313-manylinux2014_armv7l.manylinux_2_17_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:53a42d364f323275126aff81fb67c5ca1b7a04fda0546245730a55c8c5f24bc4", size = 240704, upload-time = "2025-10-06T14:50:01.485Z" },
+    { url = "https://files.pythonhosted.org/packages/50/cc/5f93e99427248c09da95b62d64b25748a5f5c98c7c2ab09825a1d6af0e15/multidict-6.7.0-cp313-cp313-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:3b29b980d0ddbecb736735ee5bef69bb2ddca56eff603c86f3f29a1128299b4f", size = 266355, upload-time = "2025-10-06T14:50:02.955Z" },
+    { url = "https://files.pythonhosted.org/packages/ec/0c/2ec1d883ceb79c6f7f6d7ad90c919c898f5d1c6ea96d322751420211e072/multidict-6.7.0-cp313-cp313-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:f8a93b1c0ed2d04b97a5e9336fd2d33371b9a6e29ab7dd6503d63407c20ffbaf", size = 267259, upload-time = "2025-10-06T14:50:04.446Z" },
+    { url = "https://files.pythonhosted.org/packages/c6/2d/f0b184fa88d6630aa267680bdb8623fb69cb0d024b8c6f0d23f9a0f406d3/multidict-6.7.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:9ff96e8815eecacc6645da76c413eb3b3d34cfca256c70b16b286a687d013c32", size = 254903, upload-time = "2025-10-06T14:50:05.98Z" },
+    { url = "https://files.pythonhosted.org/packages/06/c9/11ea263ad0df7dfabcad404feb3c0dd40b131bc7f232d5537f2fb1356951/multidict-6.7.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:7516c579652f6a6be0e266aec0acd0db80829ca305c3d771ed898538804c2036", size = 252365, upload-time = "2025-10-06T14:50:07.511Z" },
+    { url = "https://files.pythonhosted.org/packages/41/88/d714b86ee2c17d6e09850c70c9d310abac3d808ab49dfa16b43aba9d53fd/multidict-6.7.0-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:040f393368e63fb0f3330e70c26bfd336656bed925e5cbe17c9da839a6ab13ec", size = 250062, upload-time = "2025-10-06T14:50:09.074Z" },
+    { url = "https://files.pythonhosted.org/packages/15/fe/ad407bb9e818c2b31383f6131ca19ea7e35ce93cf1310fce69f12e89de75/multidict-6.7.0-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:b3bc26a951007b1057a1c543af845f1c7e3e71cc240ed1ace7bf4484aa99196e", size = 249683, upload-time = "2025-10-06T14:50:10.714Z" },
+    { url = "https://files.pythonhosted.org/packages/8c/a4/a89abdb0229e533fb925e7c6e5c40201c2873efebc9abaf14046a4536ee6/multidict-6.7.0-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:7b022717c748dd1992a83e219587aabe45980d88969f01b316e78683e6285f64", size = 261254, upload-time = "2025-10-06T14:50:12.28Z" },
+    { url = "https://files.pythonhosted.org/packages/8d/aa/0e2b27bd88b40a4fb8dc53dd74eecac70edaa4c1dd0707eb2164da3675b3/multidict-6.7.0-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:9600082733859f00d79dee64effc7aef1beb26adb297416a4ad2116fd61374bd", size = 257967, upload-time = "2025-10-06T14:50:14.16Z" },
+    { url = "https://files.pythonhosted.org/packages/d0/8e/0c67b7120d5d5f6d874ed85a085f9dc770a7f9d8813e80f44a9fec820bb7/multidict-6.7.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:94218fcec4d72bc61df51c198d098ce2b378e0ccbac41ddbed5ef44092913288", size = 250085, upload-time = "2025-10-06T14:50:15.639Z" },
+    { url = "https://files.pythonhosted.org/packages/ba/55/b73e1d624ea4b8fd4dd07a3bb70f6e4c7c6c5d9d640a41c6ffe5cdbd2a55/multidict-6.7.0-cp313-cp313-win32.whl", hash = "sha256:a37bd74c3fa9d00be2d7b8eca074dc56bd8077ddd2917a839bd989612671ed17", size = 41713, upload-time = "2025-10-06T14:50:17.066Z" },
+    { url = "https://files.pythonhosted.org/packages/32/31/75c59e7d3b4205075b4c183fa4ca398a2daf2303ddf616b04ae6ef55cffe/multidict-6.7.0-cp313-cp313-win_amd64.whl", hash = "sha256:30d193c6cc6d559db42b6bcec8a5d395d34d60c9877a0b71ecd7c204fcf15390", size = 45915, upload-time = "2025-10-06T14:50:18.264Z" },
+    { url = "https://files.pythonhosted.org/packages/31/2a/8987831e811f1184c22bc2e45844934385363ee61c0a2dcfa8f71b87e608/multidict-6.7.0-cp313-cp313-win_arm64.whl", hash = "sha256:ea3334cabe4d41b7ccd01e4d349828678794edbc2d3ae97fc162a3312095092e", size = 43077, upload-time = "2025-10-06T14:50:19.853Z" },
+    { url = "https://files.pythonhosted.org/packages/e8/68/7b3a5170a382a340147337b300b9eb25a9ddb573bcdfff19c0fa3f31ffba/multidict-6.7.0-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:ad9ce259f50abd98a1ca0aa6e490b58c316a0fce0617f609723e40804add2c00", size = 83114, upload-time = "2025-10-06T14:50:21.223Z" },
+    { url = "https://files.pythonhosted.org/packages/55/5c/3fa2d07c84df4e302060f555bbf539310980362236ad49f50eeb0a1c1eb9/multidict-6.7.0-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:07f5594ac6d084cbb5de2df218d78baf55ef150b91f0ff8a21cc7a2e3a5a58eb", size = 48442, upload-time = "2025-10-06T14:50:22.871Z" },
+    { url = "https://files.pythonhosted.org/packages/fc/56/67212d33239797f9bd91962bb899d72bb0f4c35a8652dcdb8ed049bef878/multidict-6.7.0-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:0591b48acf279821a579282444814a2d8d0af624ae0bc600aa4d1b920b6e924b", size = 46885, upload-time = "2025-10-06T14:50:24.258Z" },
+    { url = "https://files.pythonhosted.org/packages/46/d1/908f896224290350721597a61a69cd19b89ad8ee0ae1f38b3f5cd12ea2ac/multidict-6.7.0-cp313-cp313t-manylinux1_i686.manylinux_2_28_i686.manylinux_2_5_i686.whl", hash = "sha256:749a72584761531d2b9467cfbdfd29487ee21124c304c4b6cb760d8777b27f9c", size = 242588, upload-time = "2025-10-06T14:50:25.716Z" },
+    { url = "https://files.pythonhosted.org/packages/ab/67/8604288bbd68680eee0ab568fdcb56171d8b23a01bcd5cb0c8fedf6e5d99/multidict-6.7.0-cp313-cp313t-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:6b4c3d199f953acd5b446bf7c0de1fe25d94e09e79086f8dc2f48a11a129cdf1", size = 249966, upload-time = "2025-10-06T14:50:28.192Z" },
+    { url = "https://files.pythonhosted.org/packages/20/33/9228d76339f1ba51e3efef7da3ebd91964d3006217aae13211653193c3ff/multidict-6.7.0-cp313-cp313t-manylinux2014_armv7l.manylinux_2_17_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:9fb0211dfc3b51efea2f349ec92c114d7754dd62c01f81c3e32b765b70c45c9b", size = 228618, upload-time = "2025-10-06T14:50:29.82Z" },
+    { url = "https://files.pythonhosted.org/packages/f8/2d/25d9b566d10cab1c42b3b9e5b11ef79c9111eaf4463b8c257a3bd89e0ead/multidict-6.7.0-cp313-cp313t-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:a027ec240fe73a8d6281872690b988eed307cd7d91b23998ff35ff577ca688b5", size = 257539, upload-time = "2025-10-06T14:50:31.731Z" },
+    { url = "https://files.pythonhosted.org/packages/b6/b1/8d1a965e6637fc33de3c0d8f414485c2b7e4af00f42cab3d84e7b955c222/multidict-6.7.0-cp313-cp313t-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:d1d964afecdf3a8288789df2f5751dc0a8261138c3768d9af117ed384e538fad", size = 256345, upload-time = "2025-10-06T14:50:33.26Z" },
+    { url = "https://files.pythonhosted.org/packages/ba/0c/06b5a8adbdeedada6f4fb8d8f193d44a347223b11939b42953eeb6530b6b/multidict-6.7.0-cp313-cp313t-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:caf53b15b1b7df9fbd0709aa01409000a2b4dd03a5f6f5cc548183c7c8f8b63c", size = 247934, upload-time = "2025-10-06T14:50:34.808Z" },
+    { url = "https://files.pythonhosted.org/packages/8f/31/b2491b5fe167ca044c6eb4b8f2c9f3b8a00b24c432c365358eadac5d7625/multidict-6.7.0-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:654030da3197d927f05a536a66186070e98765aa5142794c9904555d3a9d8fb5", size = 245243, upload-time = "2025-10-06T14:50:36.436Z" },
+    { url = "https://files.pythonhosted.org/packages/61/1a/982913957cb90406c8c94f53001abd9eafc271cb3e70ff6371590bec478e/multidict-6.7.0-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:2090d3718829d1e484706a2f525e50c892237b2bf9b17a79b059cb98cddc2f10", size = 235878, upload-time = "2025-10-06T14:50:37.953Z" },
+    { url = "https://files.pythonhosted.org/packages/be/c0/21435d804c1a1cf7a2608593f4d19bca5bcbd7a81a70b253fdd1c12af9c0/multidict-6.7.0-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:2d2cfeec3f6f45651b3d408c4acec0ebf3daa9bc8a112a084206f5db5d05b754", size = 243452, upload-time = "2025-10-06T14:50:39.574Z" },
+    { url = "https://files.pythonhosted.org/packages/54/0a/4349d540d4a883863191be6eb9a928846d4ec0ea007d3dcd36323bb058ac/multidict-6.7.0-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:4ef089f985b8c194d341eb2c24ae6e7408c9a0e2e5658699c92f497437d88c3c", size = 252312, upload-time = "2025-10-06T14:50:41.612Z" },
+    { url = "https://files.pythonhosted.org/packages/26/64/d5416038dbda1488daf16b676e4dbfd9674dde10a0cc8f4fc2b502d8125d/multidict-6.7.0-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:e93a0617cd16998784bf4414c7e40f17a35d2350e5c6f0bd900d3a8e02bd3762", size = 246935, upload-time = "2025-10-06T14:50:43.972Z" },
+    { url = "https://files.pythonhosted.org/packages/9f/8c/8290c50d14e49f35e0bd4abc25e1bc7711149ca9588ab7d04f886cdf03d9/multidict-6.7.0-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:f0feece2ef8ebc42ed9e2e8c78fc4aa3cf455733b507c09ef7406364c94376c6", size = 243385, upload-time = "2025-10-06T14:50:45.648Z" },
+    { url = "https://files.pythonhosted.org/packages/ef/a0/f83ae75e42d694b3fbad3e047670e511c138be747bc713cf1b10d5096416/multidict-6.7.0-cp313-cp313t-win32.whl", hash = "sha256:19a1d55338ec1be74ef62440ca9e04a2f001a04d0cc49a4983dc320ff0f3212d", size = 47777, upload-time = "2025-10-06T14:50:47.154Z" },
+    { url = "https://files.pythonhosted.org/packages/dc/80/9b174a92814a3830b7357307a792300f42c9e94664b01dee8e457551fa66/multidict-6.7.0-cp313-cp313t-win_amd64.whl", hash = "sha256:3da4fb467498df97e986af166b12d01f05d2e04f978a9c1c680ea1988e0bc4b6", size = 53104, upload-time = "2025-10-06T14:50:48.851Z" },
+    { url = "https://files.pythonhosted.org/packages/cc/28/04baeaf0428d95bb7a7bea0e691ba2f31394338ba424fb0679a9ed0f4c09/multidict-6.7.0-cp313-cp313t-win_arm64.whl", hash = "sha256:b4121773c49a0776461f4a904cdf6264c88e42218aaa8407e803ca8025872792", size = 45503, upload-time = "2025-10-06T14:50:50.16Z" },
+    { url = "https://files.pythonhosted.org/packages/e2/b1/3da6934455dd4b261d4c72f897e3a5728eba81db59959f3a639245891baa/multidict-6.7.0-cp314-cp314-macosx_10_13_universal2.whl", hash = "sha256:3bab1e4aff7adaa34410f93b1f8e57c4b36b9af0426a76003f441ee1d3c7e842", size = 75128, upload-time = "2025-10-06T14:50:51.92Z" },
+    { url = "https://files.pythonhosted.org/packages/14/2c/f069cab5b51d175a1a2cb4ccdf7a2c2dabd58aa5bd933fa036a8d15e2404/multidict-6.7.0-cp314-cp314-macosx_10_13_x86_64.whl", hash = "sha256:b8512bac933afc3e45fb2b18da8e59b78d4f408399a960339598374d4ae3b56b", size = 44410, upload-time = "2025-10-06T14:50:53.275Z" },
+    { url = "https://files.pythonhosted.org/packages/42/e2/64bb41266427af6642b6b128e8774ed84c11b80a90702c13ac0a86bb10cc/multidict-6.7.0-cp314-cp314-macosx_11_0_arm64.whl", hash = "sha256:79dcf9e477bc65414ebfea98ffd013cb39552b5ecd62908752e0e413d6d06e38", size = 43205, upload-time = "2025-10-06T14:50:54.911Z" },
+    { url = "https://files.pythonhosted.org/packages/02/68/6b086fef8a3f1a8541b9236c594f0c9245617c29841f2e0395d979485cde/multidict-6.7.0-cp314-cp314-manylinux1_i686.manylinux_2_28_i686.manylinux_2_5_i686.whl", hash = "sha256:31bae522710064b5cbeddaf2e9f32b1abab70ac6ac91d42572502299e9953128", size = 245084, upload-time = "2025-10-06T14:50:56.369Z" },
+    { url = "https://files.pythonhosted.org/packages/15/ee/f524093232007cd7a75c1d132df70f235cfd590a7c9eaccd7ff422ef4ae8/multidict-6.7.0-cp314-cp314-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:4a0df7ff02397bb63e2fd22af2c87dfa39e8c7f12947bc524dbdc528282c7e34", size = 252667, upload-time = "2025-10-06T14:50:57.991Z" },
+    { url = "https://files.pythonhosted.org/packages/02/a5/eeb3f43ab45878f1895118c3ef157a480db58ede3f248e29b5354139c2c9/multidict-6.7.0-cp314-cp314-manylinux2014_armv7l.manylinux_2_17_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:7a0222514e8e4c514660e182d5156a415c13ef0aabbd71682fc714e327b95e99", size = 233590, upload-time = "2025-10-06T14:50:59.589Z" },
+    { url = "https://files.pythonhosted.org/packages/6a/1e/76d02f8270b97269d7e3dbd45644b1785bda457b474315f8cf999525a193/multidict-6.7.0-cp314-cp314-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:2397ab4daaf2698eb51a76721e98db21ce4f52339e535725de03ea962b5a3202", size = 264112, upload-time = "2025-10-06T14:51:01.183Z" },
+    { url = "https://files.pythonhosted.org/packages/76/0b/c28a70ecb58963847c2a8efe334904cd254812b10e535aefb3bcce513918/multidict-6.7.0-cp314-cp314-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:8891681594162635948a636c9fe0ff21746aeb3dd5463f6e25d9bea3a8a39ca1", size = 261194, upload-time = "2025-10-06T14:51:02.794Z" },
+    { url = "https://files.pythonhosted.org/packages/b4/63/2ab26e4209773223159b83aa32721b4021ffb08102f8ac7d689c943fded1/multidict-6.7.0-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:18706cc31dbf402a7945916dd5cddf160251b6dab8a2c5f3d6d5a55949f676b3", size = 248510, upload-time = "2025-10-06T14:51:04.724Z" },
+    { url = "https://files.pythonhosted.org/packages/93/cd/06c1fa8282af1d1c46fd55c10a7930af652afdce43999501d4d68664170c/multidict-6.7.0-cp314-cp314-musllinux_1_2_aarch64.whl", hash = "sha256:f844a1bbf1d207dd311a56f383f7eda2d0e134921d45751842d8235e7778965d", size = 248395, upload-time = "2025-10-06T14:51:06.306Z" },
+    { url = "https://files.pythonhosted.org/packages/99/ac/82cb419dd6b04ccf9e7e61befc00c77614fc8134362488b553402ecd55ce/multidict-6.7.0-cp314-cp314-musllinux_1_2_armv7l.whl", hash = "sha256:d4393e3581e84e5645506923816b9cc81f5609a778c7e7534054091acc64d1c6", size = 239520, upload-time = "2025-10-06T14:51:08.091Z" },
+    { url = "https://files.pythonhosted.org/packages/fa/f3/a0f9bf09493421bd8716a362e0cd1d244f5a6550f5beffdd6b47e885b331/multidict-6.7.0-cp314-cp314-musllinux_1_2_i686.whl", hash = "sha256:fbd18dc82d7bf274b37aa48d664534330af744e03bccf696d6f4c6042e7d19e7", size = 245479, upload-time = "2025-10-06T14:51:10.365Z" },
+    { url = "https://files.pythonhosted.org/packages/8d/01/476d38fc73a212843f43c852b0eee266b6971f0e28329c2184a8df90c376/multidict-6.7.0-cp314-cp314-musllinux_1_2_ppc64le.whl", hash = "sha256:b6234e14f9314731ec45c42fc4554b88133ad53a09092cc48a88e771c125dadb", size = 258903, upload-time = "2025-10-06T14:51:12.466Z" },
+    { url = "https://files.pythonhosted.org/packages/49/6d/23faeb0868adba613b817d0e69c5f15531b24d462af8012c4f6de4fa8dc3/multidict-6.7.0-cp314-cp314-musllinux_1_2_s390x.whl", hash = "sha256:08d4379f9744d8f78d98c8673c06e202ffa88296f009c71bbafe8a6bf847d01f", size = 252333, upload-time = "2025-10-06T14:51:14.48Z" },
+    { url = "https://files.pythonhosted.org/packages/1e/cc/48d02ac22b30fa247f7dad82866e4b1015431092f4ba6ebc7e77596e0b18/multidict-6.7.0-cp314-cp314-musllinux_1_2_x86_64.whl", hash = "sha256:9fe04da3f79387f450fd0061d4dd2e45a72749d31bf634aecc9e27f24fdc4b3f", size = 243411, upload-time = "2025-10-06T14:51:16.072Z" },
+    { url = "https://files.pythonhosted.org/packages/4a/03/29a8bf5a18abf1fe34535c88adbdfa88c9fb869b5a3b120692c64abe8284/multidict-6.7.0-cp314-cp314-win32.whl", hash = "sha256:fbafe31d191dfa7c4c51f7a6149c9fb7e914dcf9ffead27dcfd9f1ae382b3885", size = 40940, upload-time = "2025-10-06T14:51:17.544Z" },
+    { url = "https://files.pythonhosted.org/packages/82/16/7ed27b680791b939de138f906d5cf2b4657b0d45ca6f5dd6236fdddafb1a/multidict-6.7.0-cp314-cp314-win_amd64.whl", hash = "sha256:2f67396ec0310764b9222a1728ced1ab638f61aadc6226f17a71dd9324f9a99c", size = 45087, upload-time = "2025-10-06T14:51:18.875Z" },
+    { url = "https://files.pythonhosted.org/packages/cd/3c/e3e62eb35a1950292fe39315d3c89941e30a9d07d5d2df42965ab041da43/multidict-6.7.0-cp314-cp314-win_arm64.whl", hash = "sha256:ba672b26069957ee369cfa7fc180dde1fc6f176eaf1e6beaf61fbebbd3d9c000", size = 42368, upload-time = "2025-10-06T14:51:20.225Z" },
+    { url = "https://files.pythonhosted.org/packages/8b/40/cd499bd0dbc5f1136726db3153042a735fffd0d77268e2ee20d5f33c010f/multidict-6.7.0-cp314-cp314t-macosx_10_13_universal2.whl", hash = "sha256:c1dcc7524066fa918c6a27d61444d4ee7900ec635779058571f70d042d86ed63", size = 82326, upload-time = "2025-10-06T14:51:21.588Z" },
+    { url = "https://files.pythonhosted.org/packages/13/8a/18e031eca251c8df76daf0288e6790561806e439f5ce99a170b4af30676b/multidict-6.7.0-cp314-cp314t-macosx_10_13_x86_64.whl", hash = "sha256:27e0b36c2d388dc7b6ced3406671b401e84ad7eb0656b8f3a2f46ed0ce483718", size = 48065, upload-time = "2025-10-06T14:51:22.93Z" },
+    { url = "https://files.pythonhosted.org/packages/40/71/5e6701277470a87d234e433fb0a3a7deaf3bcd92566e421e7ae9776319de/multidict-6.7.0-cp314-cp314t-macosx_11_0_arm64.whl", hash = "sha256:2a7baa46a22e77f0988e3b23d4ede5513ebec1929e34ee9495be535662c0dfe2", size = 46475, upload-time = "2025-10-06T14:51:24.352Z" },
+    { url = "https://files.pythonhosted.org/packages/fe/6a/bab00cbab6d9cfb57afe1663318f72ec28289ea03fd4e8236bb78429893a/multidict-6.7.0-cp314-cp314t-manylinux1_i686.manylinux_2_28_i686.manylinux_2_5_i686.whl", hash = "sha256:7bf77f54997a9166a2f5675d1201520586439424c2511723a7312bdb4bcc034e", size = 239324, upload-time = "2025-10-06T14:51:25.822Z" },
+    { url = "https://files.pythonhosted.org/packages/2a/5f/8de95f629fc22a7769ade8b41028e3e5a822c1f8904f618d175945a81ad3/multidict-6.7.0-cp314-cp314t-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:e011555abada53f1578d63389610ac8a5400fc70ce71156b0aa30d326f1a5064", size = 246877, upload-time = "2025-10-06T14:51:27.604Z" },
+    { url = "https://files.pythonhosted.org/packages/23/b4/38881a960458f25b89e9f4a4fdcb02ac101cfa710190db6e5528841e67de/multidict-6.7.0-cp314-cp314t-manylinux2014_armv7l.manylinux_2_17_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:28b37063541b897fd6a318007373930a75ca6d6ac7c940dbe14731ffdd8d498e", size = 225824, upload-time = "2025-10-06T14:51:29.664Z" },
+    { url = "https://files.pythonhosted.org/packages/1e/39/6566210c83f8a261575f18e7144736059f0c460b362e96e9cf797a24b8e7/multidict-6.7.0-cp314-cp314t-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:05047ada7a2fde2631a0ed706f1fd68b169a681dfe5e4cf0f8e4cb6618bbc2cd", size = 253558, upload-time = "2025-10-06T14:51:31.684Z" },
+    { url = "https://files.pythonhosted.org/packages/00/a3/67f18315100f64c269f46e6c0319fa87ba68f0f64f2b8e7fd7c72b913a0b/multidict-6.7.0-cp314-cp314t-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:716133f7d1d946a4e1b91b1756b23c088881e70ff180c24e864c26192ad7534a", size = 252339, upload-time = "2025-10-06T14:51:33.699Z" },
+    { url = "https://files.pythonhosted.org/packages/c8/2a/1cb77266afee2458d82f50da41beba02159b1d6b1f7973afc9a1cad1499b/multidict-6.7.0-cp314-cp314t-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:d1bed1b467ef657f2a0ae62844a607909ef1c6889562de5e1d505f74457d0b96", size = 244895, upload-time = "2025-10-06T14:51:36.189Z" },
+    { url = "https://files.pythonhosted.org/packages/dd/72/09fa7dd487f119b2eb9524946ddd36e2067c08510576d43ff68469563b3b/multidict-6.7.0-cp314-cp314t-musllinux_1_2_aarch64.whl", hash = "sha256:ca43bdfa5d37bd6aee89d85e1d0831fb86e25541be7e9d376ead1b28974f8e5e", size = 241862, upload-time = "2025-10-06T14:51:41.291Z" },
+    { url = "https://files.pythonhosted.org/packages/65/92/bc1f8bd0853d8669300f732c801974dfc3702c3eeadae2f60cef54dc69d7/multidict-6.7.0-cp314-cp314t-musllinux_1_2_armv7l.whl", hash = "sha256:44b546bd3eb645fd26fb949e43c02a25a2e632e2ca21a35e2e132c8105dc8599", size = 232376, upload-time = "2025-10-06T14:51:43.55Z" },
+    { url = "https://files.pythonhosted.org/packages/09/86/ac39399e5cb9d0c2ac8ef6e10a768e4d3bc933ac808d49c41f9dc23337eb/multidict-6.7.0-cp314-cp314t-musllinux_1_2_i686.whl", hash = "sha256:a6ef16328011d3f468e7ebc326f24c1445f001ca1dec335b2f8e66bed3006394", size = 240272, upload-time = "2025-10-06T14:51:45.265Z" },
+    { url = "https://files.pythonhosted.org/packages/3d/b6/fed5ac6b8563ec72df6cb1ea8dac6d17f0a4a1f65045f66b6d3bf1497c02/multidict-6.7.0-cp314-cp314t-musllinux_1_2_ppc64le.whl", hash = "sha256:5aa873cbc8e593d361ae65c68f85faadd755c3295ea2c12040ee146802f23b38", size = 248774, upload-time = "2025-10-06T14:51:46.836Z" },
+    { url = "https://files.pythonhosted.org/packages/6b/8d/b954d8c0dc132b68f760aefd45870978deec6818897389dace00fcde32ff/multidict-6.7.0-cp314-cp314t-musllinux_1_2_s390x.whl", hash = "sha256:3d7b6ccce016e29df4b7ca819659f516f0bc7a4b3efa3bb2012ba06431b044f9", size = 242731, upload-time = "2025-10-06T14:51:48.541Z" },
+    { url = "https://files.pythonhosted.org/packages/16/9d/a2dac7009125d3540c2f54e194829ea18ac53716c61b655d8ed300120b0f/multidict-6.7.0-cp314-cp314t-musllinux_1_2_x86_64.whl", hash = "sha256:171b73bd4ee683d307599b66793ac80981b06f069b62eea1c9e29c9241aa66b0", size = 240193, upload-time = "2025-10-06T14:51:50.355Z" },
+    { url = "https://files.pythonhosted.org/packages/39/ca/c05f144128ea232ae2178b008d5011d4e2cea86e4ee8c85c2631b1b94802/multidict-6.7.0-cp314-cp314t-win32.whl", hash = "sha256:b2d7f80c4e1fd010b07cb26820aae86b7e73b681ee4889684fb8d2d4537aab13", size = 48023, upload-time = "2025-10-06T14:51:51.883Z" },
+    { url = "https://files.pythonhosted.org/packages/ba/8f/0a60e501584145588be1af5cc829265701ba3c35a64aec8e07cbb71d39bb/multidict-6.7.0-cp314-cp314t-win_amd64.whl", hash = "sha256:09929cab6fcb68122776d575e03c6cc64ee0b8fca48d17e135474b042ce515cd", size = 53507, upload-time = "2025-10-06T14:51:53.672Z" },
+    { url = "https://files.pythonhosted.org/packages/7f/ae/3148b988a9c6239903e786eac19c889fab607c31d6efa7fb2147e5680f23/multidict-6.7.0-cp314-cp314t-win_arm64.whl", hash = "sha256:cc41db090ed742f32bd2d2c721861725e6109681eddf835d0a82bd3a5c382827", size = 44804, upload-time = "2025-10-06T14:51:55.415Z" },
+    { url = "https://files.pythonhosted.org/packages/b7/da/7d22601b625e241d4f23ef1ebff8acfc60da633c9e7e7922e24d10f592b3/multidict-6.7.0-py3-none-any.whl", hash = "sha256:394fc5c42a333c9ffc3e421a4c85e08580d990e08b99f6bf35b4132114c5dcb3", size = 12317, upload-time = "2025-10-06T14:52:29.272Z" },
+]
+
 [[package]]
 name = "mypy"
 version = "1.19.0"
@@ -1616,6 +2292,121 @@ wheels = [
     { url = "https://files.pythonhosted.org/packages/c0/da/977ded879c29cbd04de313843e76868e6e13408a94ed6b987245dc7c8506/openpyxl-3.1.5-py2.py3-none-any.whl", hash = "sha256:5282c12b107bffeef825f4617dc029afaf41d0ea60823bbb665ef3079dc79de2", size = 250910, upload-time = "2024-06-28T14:03:41.161Z" },
 ]
 
+[[package]]
+name = "orjson"
+version = "3.11.5"
+source = { registry = "https://pypi.org/simple" }
+sdist = { url = "https://files.pythonhosted.org/packages/04/b8/333fdb27840f3bf04022d21b654a35f58e15407183aeb16f3b41aa053446/orjson-3.11.5.tar.gz", hash = "sha256:82393ab47b4fe44ffd0a7659fa9cfaacc717eb617c93cde83795f14af5c2e9d5", size = 5972347, upload-time = "2025-12-06T15:55:39.458Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/fd/68/6b3659daec3a81aed5ab47700adb1a577c76a5452d35b91c88efee89987f/orjson-3.11.5-cp311-cp311-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl", hash = "sha256:9c8494625ad60a923af6b2b0bd74107146efe9b55099e20d7740d995f338fcd8", size = 245318, upload-time = "2025-12-06T15:54:02.355Z" },
+    { url = "https://files.pythonhosted.org/packages/e9/00/92db122261425f61803ccf0830699ea5567439d966cbc35856fe711bfe6b/orjson-3.11.5-cp311-cp311-macosx_15_0_arm64.whl", hash = "sha256:7bb2ce0b82bc9fd1168a513ddae7a857994b780b2945a8c51db4ab1c4b751ebc", size = 129491, upload-time = "2025-12-06T15:54:03.877Z" },
+    { url = "https://files.pythonhosted.org/packages/94/4f/ffdcb18356518809d944e1e1f77589845c278a1ebbb5a8297dfefcc4b4cb/orjson-3.11.5-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:67394d3becd50b954c4ecd24ac90b5051ee7c903d167459f93e77fc6f5b4c968", size = 132167, upload-time = "2025-12-06T15:54:04.944Z" },
+    { url = "https://files.pythonhosted.org/packages/97/c6/0a8caff96f4503f4f7dd44e40e90f4d14acf80d3b7a97cb88747bb712d3e/orjson-3.11.5-cp311-cp311-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:298d2451f375e5f17b897794bcc3e7b821c0f32b4788b9bcae47ada24d7f3cf7", size = 130516, upload-time = "2025-12-06T15:54:06.274Z" },
+    { url = "https://files.pythonhosted.org/packages/4d/63/43d4dc9bd9954bff7052f700fdb501067f6fb134a003ddcea2a0bb3854ed/orjson-3.11.5-cp311-cp311-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:aa5e4244063db8e1d87e0f54c3f7522f14b2dc937e65d5241ef0076a096409fd", size = 135695, upload-time = "2025-12-06T15:54:07.702Z" },
+    { url = "https://files.pythonhosted.org/packages/87/6f/27e2e76d110919cb7fcb72b26166ee676480a701bcf8fc53ac5d0edce32f/orjson-3.11.5-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:1db2088b490761976c1b2e956d5d4e6409f3732e9d79cfa69f876c5248d1baf9", size = 139664, upload-time = "2025-12-06T15:54:08.828Z" },
+    { url = "https://files.pythonhosted.org/packages/d4/f8/5966153a5f1be49b5fbb8ca619a529fde7bc71aa0a376f2bb83fed248bcd/orjson-3.11.5-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:c2ed66358f32c24e10ceea518e16eb3549e34f33a9d51f99ce23b0251776a1ef", size = 137289, upload-time = "2025-12-06T15:54:09.898Z" },
+    { url = "https://files.pythonhosted.org/packages/a7/34/8acb12ff0299385c8bbcbb19fbe40030f23f15a6de57a9c587ebf71483fb/orjson-3.11.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:c2021afda46c1ed64d74b555065dbd4c2558d510d8cec5ea6a53001b3e5e82a9", size = 138784, upload-time = "2025-12-06T15:54:11.022Z" },
+    { url = "https://files.pythonhosted.org/packages/ee/27/910421ea6e34a527f73d8f4ee7bdffa48357ff79c7b8d6eb6f7b82dd1176/orjson-3.11.5-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:b42ffbed9128e547a1647a3e50bc88ab28ae9daa61713962e0d3dd35e820c125", size = 141322, upload-time = "2025-12-06T15:54:12.427Z" },
+    { url = "https://files.pythonhosted.org/packages/87/a3/4b703edd1a05555d4bb1753d6ce44e1a05b7a6d7c164d5b332c795c63d70/orjson-3.11.5-cp311-cp311-musllinux_1_2_armv7l.whl", hash = "sha256:8d5f16195bb671a5dd3d1dbea758918bada8f6cc27de72bd64adfbd748770814", size = 413612, upload-time = "2025-12-06T15:54:13.858Z" },
+    { url = "https://files.pythonhosted.org/packages/1b/36/034177f11d7eeea16d3d2c42a1883b0373978e08bc9dad387f5074c786d8/orjson-3.11.5-cp311-cp311-musllinux_1_2_i686.whl", hash = "sha256:c0e5d9f7a0227df2927d343a6e3859bebf9208b427c79bd31949abcc2fa32fa5", size = 150993, upload-time = "2025-12-06T15:54:15.189Z" },
+    { url = "https://files.pythonhosted.org/packages/44/2f/ea8b24ee046a50a7d141c0227c4496b1180b215e728e3b640684f0ea448d/orjson-3.11.5-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:23d04c4543e78f724c4dfe656b3791b5f98e4c9253e13b2636f1af5d90e4a880", size = 141774, upload-time = "2025-12-06T15:54:16.451Z" },
+    { url = "https://files.pythonhosted.org/packages/8a/12/cc440554bf8200eb23348a5744a575a342497b65261cd65ef3b28332510a/orjson-3.11.5-cp311-cp311-win32.whl", hash = "sha256:c404603df4865f8e0afe981aa3c4b62b406e6d06049564d58934860b62b7f91d", size = 135109, upload-time = "2025-12-06T15:54:17.73Z" },
+    { url = "https://files.pythonhosted.org/packages/a3/83/e0c5aa06ba73a6760134b169f11fb970caa1525fa4461f94d76e692299d9/orjson-3.11.5-cp311-cp311-win_amd64.whl", hash = "sha256:9645ef655735a74da4990c24ffbd6894828fbfa117bc97c1edd98c282ecb52e1", size = 133193, upload-time = "2025-12-06T15:54:19.426Z" },
+    { url = "https://files.pythonhosted.org/packages/cb/35/5b77eaebc60d735e832c5b1a20b155667645d123f09d471db0a78280fb49/orjson-3.11.5-cp311-cp311-win_arm64.whl", hash = "sha256:1cbf2735722623fcdee8e712cbaaab9e372bbcb0c7924ad711b261c2eccf4a5c", size = 126830, upload-time = "2025-12-06T15:54:20.836Z" },
+    { url = "https://files.pythonhosted.org/packages/ef/a4/8052a029029b096a78955eadd68ab594ce2197e24ec50e6b6d2ab3f4e33b/orjson-3.11.5-cp312-cp312-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl", hash = "sha256:334e5b4bff9ad101237c2d799d9fd45737752929753bf4faf4b207335a416b7d", size = 245347, upload-time = "2025-12-06T15:54:22.061Z" },
+    { url = "https://files.pythonhosted.org/packages/64/67/574a7732bd9d9d79ac620c8790b4cfe0717a3d5a6eb2b539e6e8995e24a0/orjson-3.11.5-cp312-cp312-macosx_15_0_arm64.whl", hash = "sha256:ff770589960a86eae279f5d8aa536196ebda8273a2a07db2a54e82b93bc86626", size = 129435, upload-time = "2025-12-06T15:54:23.615Z" },
+    { url = "https://files.pythonhosted.org/packages/52/8d/544e77d7a29d90cf4d9eecd0ae801c688e7f3d1adfa2ebae5e1e94d38ab9/orjson-3.11.5-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:ed24250e55efbcb0b35bed7caaec8cedf858ab2f9f2201f17b8938c618c8ca6f", size = 132074, upload-time = "2025-12-06T15:54:24.694Z" },
+    { url = "https://files.pythonhosted.org/packages/6e/57/b9f5b5b6fbff9c26f77e785baf56ae8460ef74acdb3eae4931c25b8f5ba9/orjson-3.11.5-cp312-cp312-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:a66d7769e98a08a12a139049aac2f0ca3adae989817f8c43337455fbc7669b85", size = 130520, upload-time = "2025-12-06T15:54:26.185Z" },
+    { url = "https://files.pythonhosted.org/packages/f6/6d/d34970bf9eb33f9ec7c979a262cad86076814859e54eb9a059a52f6dc13d/orjson-3.11.5-cp312-cp312-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:86cfc555bfd5794d24c6a1903e558b50644e5e68e6471d66502ce5cb5fdef3f9", size = 136209, upload-time = "2025-12-06T15:54:27.264Z" },
+    { url = "https://files.pythonhosted.org/packages/e7/39/bc373b63cc0e117a105ea12e57280f83ae52fdee426890d57412432d63b3/orjson-3.11.5-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:a230065027bc2a025e944f9d4714976a81e7ecfa940923283bca7bbc1f10f626", size = 139837, upload-time = "2025-12-06T15:54:28.75Z" },
+    { url = "https://files.pythonhosted.org/packages/cb/aa/7c4818c8d7d324da220f4f1af55c343956003aa4d1ce1857bdc1d396ba69/orjson-3.11.5-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:b29d36b60e606df01959c4b982729c8845c69d1963f88686608be9ced96dbfaa", size = 137307, upload-time = "2025-12-06T15:54:29.856Z" },
+    { url = "https://files.pythonhosted.org/packages/46/bf/0993b5a056759ba65145effe3a79dd5a939d4a070eaa5da2ee3180fbb13f/orjson-3.11.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:c74099c6b230d4261fdc3169d50efc09abf38ace1a42ea2f9994b1d79153d477", size = 139020, upload-time = "2025-12-06T15:54:31.024Z" },
+    { url = "https://files.pythonhosted.org/packages/65/e8/83a6c95db3039e504eda60fc388f9faedbb4f6472f5aba7084e06552d9aa/orjson-3.11.5-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:e697d06ad57dd0c7a737771d470eedc18e68dfdefcdd3b7de7f33dfda5b6212e", size = 141099, upload-time = "2025-12-06T15:54:32.196Z" },
+    { url = "https://files.pythonhosted.org/packages/b9/b4/24fdc024abfce31c2f6812973b0a693688037ece5dc64b7a60c1ce69e2f2/orjson-3.11.5-cp312-cp312-musllinux_1_2_armv7l.whl", hash = "sha256:e08ca8a6c851e95aaecc32bc44a5aa75d0ad26af8cdac7c77e4ed93acf3d5b69", size = 413540, upload-time = "2025-12-06T15:54:33.361Z" },
+    { url = "https://files.pythonhosted.org/packages/d9/37/01c0ec95d55ed0c11e4cae3e10427e479bba40c77312b63e1f9665e0737d/orjson-3.11.5-cp312-cp312-musllinux_1_2_i686.whl", hash = "sha256:e8b5f96c05fce7d0218df3fdfeb962d6b8cfff7e3e20264306b46dd8b217c0f3", size = 151530, upload-time = "2025-12-06T15:54:34.6Z" },
+    { url = "https://files.pythonhosted.org/packages/f9/d4/f9ebc57182705bb4bbe63f5bbe14af43722a2533135e1d2fb7affa0c355d/orjson-3.11.5-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:ddbfdb5099b3e6ba6d6ea818f61997bb66de14b411357d24c4612cf1ebad08ca", size = 141863, upload-time = "2025-12-06T15:54:35.801Z" },
+    { url = "https://files.pythonhosted.org/packages/0d/04/02102b8d19fdcb009d72d622bb5781e8f3fae1646bf3e18c53d1bc8115b5/orjson-3.11.5-cp312-cp312-win32.whl", hash = "sha256:9172578c4eb09dbfcf1657d43198de59b6cef4054de385365060ed50c458ac98", size = 135255, upload-time = "2025-12-06T15:54:37.209Z" },
+    { url = "https://files.pythonhosted.org/packages/d4/fb/f05646c43d5450492cb387de5549f6de90a71001682c17882d9f66476af5/orjson-3.11.5-cp312-cp312-win_amd64.whl", hash = "sha256:2b91126e7b470ff2e75746f6f6ee32b9ab67b7a93c8ba1d15d3a0caaf16ec875", size = 133252, upload-time = "2025-12-06T15:54:38.401Z" },
+    { url = "https://files.pythonhosted.org/packages/dc/a6/7b8c0b26ba18c793533ac1cd145e131e46fcf43952aa94c109b5b913c1f0/orjson-3.11.5-cp312-cp312-win_arm64.whl", hash = "sha256:acbc5fac7e06777555b0722b8ad5f574739e99ffe99467ed63da98f97f9ca0fe", size = 126777, upload-time = "2025-12-06T15:54:39.515Z" },
+    { url = "https://files.pythonhosted.org/packages/10/43/61a77040ce59f1569edf38f0b9faadc90c8cf7e9bec2e0df51d0132c6bb7/orjson-3.11.5-cp313-cp313-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl", hash = "sha256:3b01799262081a4c47c035dd77c1301d40f568f77cc7ec1bb7db5d63b0a01629", size = 245271, upload-time = "2025-12-06T15:54:40.878Z" },
+    { url = "https://files.pythonhosted.org/packages/55/f9/0f79be617388227866d50edd2fd320cb8fb94dc1501184bb1620981a0aba/orjson-3.11.5-cp313-cp313-macosx_15_0_arm64.whl", hash = "sha256:61de247948108484779f57a9f406e4c84d636fa5a59e411e6352484985e8a7c3", size = 129422, upload-time = "2025-12-06T15:54:42.403Z" },
+    { url = "https://files.pythonhosted.org/packages/77/42/f1bf1549b432d4a78bfa95735b79b5dac75b65b5bb815bba86ad406ead0a/orjson-3.11.5-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:894aea2e63d4f24a7f04a1908307c738d0dce992e9249e744b8f4e8dd9197f39", size = 132060, upload-time = "2025-12-06T15:54:43.531Z" },
+    { url = "https://files.pythonhosted.org/packages/25/49/825aa6b929f1a6ed244c78acd7b22c1481fd7e5fda047dc8bf4c1a807eb6/orjson-3.11.5-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:ddc21521598dbe369d83d4d40338e23d4101dad21dae0e79fa20465dbace019f", size = 130391, upload-time = "2025-12-06T15:54:45.059Z" },
+    { url = "https://files.pythonhosted.org/packages/42/ec/de55391858b49e16e1aa8f0bbbb7e5997b7345d8e984a2dec3746d13065b/orjson-3.11.5-cp313-cp313-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:7cce16ae2f5fb2c53c3eafdd1706cb7b6530a67cc1c17abe8ec747f5cd7c0c51", size = 135964, upload-time = "2025-12-06T15:54:46.576Z" },
+    { url = "https://files.pythonhosted.org/packages/1c/40/820bc63121d2d28818556a2d0a09384a9f0262407cf9fa305e091a8048df/orjson-3.11.5-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:e46c762d9f0e1cfb4ccc8515de7f349abbc95b59cb5a2bd68df5973fdef913f8", size = 139817, upload-time = "2025-12-06T15:54:48.084Z" },
+    { url = "https://files.pythonhosted.org/packages/09/c7/3a445ca9a84a0d59d26365fd8898ff52bdfcdcb825bcc6519830371d2364/orjson-3.11.5-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:d7345c759276b798ccd6d77a87136029e71e66a8bbf2d2755cbdde1d82e78706", size = 137336, upload-time = "2025-12-06T15:54:49.426Z" },
+    { url = "https://files.pythonhosted.org/packages/9a/b3/dc0d3771f2e5d1f13368f56b339c6782f955c6a20b50465a91acb79fe961/orjson-3.11.5-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:75bc2e59e6a2ac1dd28901d07115abdebc4563b5b07dd612bf64260a201b1c7f", size = 138993, upload-time = "2025-12-06T15:54:50.939Z" },
+    { url = "https://files.pythonhosted.org/packages/d1/a2/65267e959de6abe23444659b6e19c888f242bf7725ff927e2292776f6b89/orjson-3.11.5-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:54aae9b654554c3b4edd61896b978568c6daa16af96fa4681c9b5babd469f863", size = 141070, upload-time = "2025-12-06T15:54:52.414Z" },
+    { url = "https://files.pythonhosted.org/packages/63/c9/da44a321b288727a322c6ab17e1754195708786a04f4f9d2220a5076a649/orjson-3.11.5-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:4bdd8d164a871c4ec773f9de0f6fe8769c2d6727879c37a9666ba4183b7f8228", size = 413505, upload-time = "2025-12-06T15:54:53.67Z" },
+    { url = "https://files.pythonhosted.org/packages/7f/17/68dc14fa7000eefb3d4d6d7326a190c99bb65e319f02747ef3ebf2452f12/orjson-3.11.5-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:a261fef929bcf98a60713bf5e95ad067cea16ae345d9a35034e73c3990e927d2", size = 151342, upload-time = "2025-12-06T15:54:55.113Z" },
+    { url = "https://files.pythonhosted.org/packages/c4/c5/ccee774b67225bed630a57478529fc026eda33d94fe4c0eac8fe58d4aa52/orjson-3.11.5-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:c028a394c766693c5c9909dec76b24f37e6a1b91999e8d0c0d5feecbe93c3e05", size = 141823, upload-time = "2025-12-06T15:54:56.331Z" },
+    { url = "https://files.pythonhosted.org/packages/67/80/5d00e4155d0cd7390ae2087130637671da713959bb558db9bac5e6f6b042/orjson-3.11.5-cp313-cp313-win32.whl", hash = "sha256:2cc79aaad1dfabe1bd2d50ee09814a1253164b3da4c00a78c458d82d04b3bdef", size = 135236, upload-time = "2025-12-06T15:54:57.507Z" },
+    { url = "https://files.pythonhosted.org/packages/95/fe/792cc06a84808dbdc20ac6eab6811c53091b42f8e51ecebf14b540e9cfe4/orjson-3.11.5-cp313-cp313-win_amd64.whl", hash = "sha256:ff7877d376add4e16b274e35a3f58b7f37b362abf4aa31863dadacdd20e3a583", size = 133167, upload-time = "2025-12-06T15:54:58.71Z" },
+    { url = "https://files.pythonhosted.org/packages/46/2c/d158bd8b50e3b1cfdcf406a7e463f6ffe3f0d167b99634717acdaf5e299f/orjson-3.11.5-cp313-cp313-win_arm64.whl", hash = "sha256:59ac72ea775c88b163ba8d21b0177628bd015c5dd060647bbab6e22da3aad287", size = 126712, upload-time = "2025-12-06T15:54:59.892Z" },
+    { url = "https://files.pythonhosted.org/packages/c2/60/77d7b839e317ead7bb225d55bb50f7ea75f47afc489c81199befc5435b50/orjson-3.11.5-cp314-cp314-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl", hash = "sha256:e446a8ea0a4c366ceafc7d97067bfd55292969143b57e3c846d87fc701e797a0", size = 245252, upload-time = "2025-12-06T15:55:01.127Z" },
+    { url = "https://files.pythonhosted.org/packages/f1/aa/d4639163b400f8044cef0fb9aa51b0337be0da3a27187a20d1166e742370/orjson-3.11.5-cp314-cp314-macosx_15_0_arm64.whl", hash = "sha256:53deb5addae9c22bbe3739298f5f2196afa881ea75944e7720681c7080909a81", size = 129419, upload-time = "2025-12-06T15:55:02.723Z" },
+    { url = "https://files.pythonhosted.org/packages/30/94/9eabf94f2e11c671111139edf5ec410d2f21e6feee717804f7e8872d883f/orjson-3.11.5-cp314-cp314-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:82cd00d49d6063d2b8791da5d4f9d20539c5951f965e45ccf4e96d33505ce68f", size = 132050, upload-time = "2025-12-06T15:55:03.918Z" },
+    { url = "https://files.pythonhosted.org/packages/3d/c8/ca10f5c5322f341ea9a9f1097e140be17a88f88d1cfdd29df522970d9744/orjson-3.11.5-cp314-cp314-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:3fd15f9fc8c203aeceff4fda211157fad114dde66e92e24097b3647a08f4ee9e", size = 130370, upload-time = "2025-12-06T15:55:05.173Z" },
+    { url = "https://files.pythonhosted.org/packages/25/d4/e96824476d361ee2edd5c6290ceb8d7edf88d81148a6ce172fc00278ca7f/orjson-3.11.5-cp314-cp314-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:9df95000fbe6777bf9820ae82ab7578e8662051bb5f83d71a28992f539d2cda7", size = 136012, upload-time = "2025-12-06T15:55:06.402Z" },
+    { url = "https://files.pythonhosted.org/packages/85/8e/9bc3423308c425c588903f2d103cfcfe2539e07a25d6522900645a6f257f/orjson-3.11.5-cp314-cp314-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:92a8d676748fca47ade5bc3da7430ed7767afe51b2f8100e3cd65e151c0eaceb", size = 139809, upload-time = "2025-12-06T15:55:07.656Z" },
+    { url = "https://files.pythonhosted.org/packages/e9/3c/b404e94e0b02a232b957c54643ce68d0268dacb67ac33ffdee24008c8b27/orjson-3.11.5-cp314-cp314-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:aa0f513be38b40234c77975e68805506cad5d57b3dfd8fe3baa7f4f4051e15b4", size = 137332, upload-time = "2025-12-06T15:55:08.961Z" },
+    { url = "https://files.pythonhosted.org/packages/51/30/cc2d69d5ce0ad9b84811cdf4a0cd5362ac27205a921da524ff42f26d65e0/orjson-3.11.5-cp314-cp314-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:fa1863e75b92891f553b7922ce4ee10ed06db061e104f2b7815de80cdcb135ad", size = 138983, upload-time = "2025-12-06T15:55:10.595Z" },
+    { url = "https://files.pythonhosted.org/packages/0e/87/de3223944a3e297d4707d2fe3b1ffb71437550e165eaf0ca8bbe43ccbcb1/orjson-3.11.5-cp314-cp314-musllinux_1_2_aarch64.whl", hash = "sha256:d4be86b58e9ea262617b8ca6251a2f0d63cc132a6da4b5fcc8e0a4128782c829", size = 141069, upload-time = "2025-12-06T15:55:11.832Z" },
+    { url = "https://files.pythonhosted.org/packages/65/30/81d5087ae74be33bcae3ff2d80f5ccaa4a8fedc6d39bf65a427a95b8977f/orjson-3.11.5-cp314-cp314-musllinux_1_2_armv7l.whl", hash = "sha256:b923c1c13fa02084eb38c9c065afd860a5cff58026813319a06949c3af5732ac", size = 413491, upload-time = "2025-12-06T15:55:13.314Z" },
+    { url = "https://files.pythonhosted.org/packages/d0/6f/f6058c21e2fc1efaf918986dbc2da5cd38044f1a2d4b7b91ad17c4acf786/orjson-3.11.5-cp314-cp314-musllinux_1_2_i686.whl", hash = "sha256:1b6bd351202b2cd987f35a13b5e16471cf4d952b42a73c391cc537974c43ef6d", size = 151375, upload-time = "2025-12-06T15:55:14.715Z" },
+    { url = "https://files.pythonhosted.org/packages/54/92/c6921f17d45e110892899a7a563a925b2273d929959ce2ad89e2525b885b/orjson-3.11.5-cp314-cp314-musllinux_1_2_x86_64.whl", hash = "sha256:bb150d529637d541e6af06bbe3d02f5498d628b7f98267ff87647584293ab439", size = 141850, upload-time = "2025-12-06T15:55:15.94Z" },
+    { url = "https://files.pythonhosted.org/packages/88/86/cdecb0140a05e1a477b81f24739da93b25070ee01ce7f7242f44a6437594/orjson-3.11.5-cp314-cp314-win32.whl", hash = "sha256:9cc1e55c884921434a84a0c3dd2699eb9f92e7b441d7f53f3941079ec6ce7499", size = 135278, upload-time = "2025-12-06T15:55:17.202Z" },
+    { url = "https://files.pythonhosted.org/packages/e4/97/b638d69b1e947d24f6109216997e38922d54dcdcdb1b11c18d7efd2d3c59/orjson-3.11.5-cp314-cp314-win_amd64.whl", hash = "sha256:a4f3cb2d874e03bc7767c8f88adaa1a9a05cecea3712649c3b58589ec7317310", size = 133170, upload-time = "2025-12-06T15:55:18.468Z" },
+    { url = "https://files.pythonhosted.org/packages/8f/dd/f4fff4a6fe601b4f8f3ba3aa6da8ac33d17d124491a3b804c662a70e1636/orjson-3.11.5-cp314-cp314-win_arm64.whl", hash = "sha256:38b22f476c351f9a1c43e5b07d8b5a02eb24a6ab8e75f700f7d479d4568346a5", size = 126713, upload-time = "2025-12-06T15:55:19.738Z" },
+]
+
+[[package]]
+name = "ormsgpack"
+version = "1.12.1"
+source = { registry = "https://pypi.org/simple" }
+sdist = { url = "https://files.pythonhosted.org/packages/fe/96/34c40d621996c2f377a18decbd3c59f031dde73c3ba47d1e1e8f29a05aaa/ormsgpack-1.12.1.tar.gz", hash = "sha256:a3877fde1e4f27a39f92681a0aab6385af3a41d0c25375d33590ae20410ea2ac", size = 39476, upload-time = "2025-12-14T07:57:43.248Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/57/e2/f5b89365c8dc8025c27d31316038f1c103758ddbf87dc0fa8e3f78f66907/ormsgpack-1.12.1-cp311-cp311-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl", hash = "sha256:4038f59ae0e19dac5e5d9aae4ec17ff84a79e046342ee73ccdecf3547ecf0d34", size = 376180, upload-time = "2025-12-14T07:56:56.521Z" },
+    { url = "https://files.pythonhosted.org/packages/ca/87/3f694e06f5e32c6d65066f53b4a025282a5072b6b336c17560b00e04606d/ormsgpack-1.12.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:16c63b0c5a3eec467e4bb33a14dabba076b7d934dff62898297b5c0b5f7c3cb3", size = 202338, upload-time = "2025-12-14T07:56:57.585Z" },
+    { url = "https://files.pythonhosted.org/packages/e5/f5/6d95d7b7c11f97a92522082fc7e5d1ab34537929f1e13f4c369f392f19d0/ormsgpack-1.12.1-cp311-cp311-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:74fd6a8e037eb310dda865298e8d122540af00fe5658ec18b97a1d34f4012e4d", size = 210720, upload-time = "2025-12-14T07:56:58.968Z" },
+    { url = "https://files.pythonhosted.org/packages/2b/9d/9a49a2686f8b7165dcb2342b8554951263c30c0f0825f1fcc2d56e736a6b/ormsgpack-1.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:58ad60308e233dd824a1859eabb5fe092e123e885eafa4ad5789322329c80fb5", size = 211264, upload-time = "2025-12-14T07:57:00.099Z" },
+    { url = "https://files.pythonhosted.org/packages/02/31/2fdc36eaeca2182900b96fc7b19755f293283fe681750e3d295733d62f0e/ormsgpack-1.12.1-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:35127464c941c1219acbe1a220e48d55e7933373d12257202f4042f7044b4c90", size = 386081, upload-time = "2025-12-14T07:57:01.177Z" },
+    { url = "https://files.pythonhosted.org/packages/f0/65/0a765432f08ae26b4013c6a9aed97be17a9ef85f1600948a474b518e27dd/ormsgpack-1.12.1-cp311-cp311-musllinux_1_2_armv7l.whl", hash = "sha256:c48d1c50794692d1e6e3f8c3bb65f5c3acfaae9347e506484a65d60b3d91fb50", size = 479572, upload-time = "2025-12-14T07:57:02.738Z" },
+    { url = "https://files.pythonhosted.org/packages/4e/4f/f2f15ebef786ad71cea420bf8692448fbddf04d1bf3feaa68bd5ee3172e6/ormsgpack-1.12.1-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:b512b2ad6feaaefdc26e05431ed2843e42483041e354e167c53401afaa83d919", size = 387862, upload-time = "2025-12-14T07:57:03.842Z" },
+    { url = "https://files.pythonhosted.org/packages/15/eb/86fbef1d605fa91ecef077f93f9d0e34fc39b23475dfe3ffb92f6c8db28d/ormsgpack-1.12.1-cp311-cp311-win_amd64.whl", hash = "sha256:93f30db95e101a9616323bfc50807ad00e7f6197cea2216d2d24af42afc77d88", size = 115900, upload-time = "2025-12-14T07:57:05.137Z" },
+    { url = "https://files.pythonhosted.org/packages/5b/67/7ba1a46e6a6e263fc42a4fafc24afc1ab21a66116553cad670426f0bd9ef/ormsgpack-1.12.1-cp311-cp311-win_arm64.whl", hash = "sha256:d75b5fa14f6abffce2c392ee03b4731199d8a964c81ee8645c4c79af0e80fd50", size = 109868, upload-time = "2025-12-14T07:57:06.834Z" },
+    { url = "https://files.pythonhosted.org/packages/17/fe/ab9167ca037406b5703add24049cf3e18021a3b16133ea20615b1f160ea4/ormsgpack-1.12.1-cp312-cp312-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl", hash = "sha256:4d7fb0e1b6fbc701d75269f7405a4f79230a6ce0063fb1092e4f6577e312f86d", size = 376725, upload-time = "2025-12-14T07:57:07.894Z" },
+    { url = "https://files.pythonhosted.org/packages/c7/ea/2820e65f506894c459b840d1091ae6e327fde3d5a3f3b002a11a1b9bdf7d/ormsgpack-1.12.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:43a9353e2db5b024c91a47d864ef15eaa62d81824cfc7740fed4cef7db738694", size = 202466, upload-time = "2025-12-14T07:57:09.049Z" },
+    { url = "https://files.pythonhosted.org/packages/45/8b/def01c13339c5bbec2ee1469ef53e7fadd66c8d775df974ee4def1572515/ormsgpack-1.12.1-cp312-cp312-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:fc8fe866b7706fc25af0adf1f600bc06ece5b15ca44e34641327198b821e5c3c", size = 210748, upload-time = "2025-12-14T07:57:10.074Z" },
+    { url = "https://files.pythonhosted.org/packages/5d/d2/bf350c92f7f067dd9484499705f2d8366d8d9008a670e3d1d0add1908f85/ormsgpack-1.12.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:813755b5f598a78242042e05dfd1ada4e769e94b98c9ab82554550f97ff4d641", size = 211510, upload-time = "2025-12-14T07:57:11.165Z" },
+    { url = "https://files.pythonhosted.org/packages/74/92/9d689bcb95304a6da26c4d59439c350940c25d1b35f146d402ccc6344c51/ormsgpack-1.12.1-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:8eea2a13536fae45d78f93f2cc846c9765c7160c85f19cfefecc20873c137cdd", size = 386237, upload-time = "2025-12-14T07:57:12.306Z" },
+    { url = "https://files.pythonhosted.org/packages/17/fe/bd3107547f8b6129265dd957f40b9cd547d2445db2292aacb13335a7ea89/ormsgpack-1.12.1-cp312-cp312-musllinux_1_2_armv7l.whl", hash = "sha256:7a02ebda1a863cbc604740e76faca8eee1add322db2dcbe6cf32669fffdff65c", size = 479589, upload-time = "2025-12-14T07:57:13.475Z" },
+    { url = "https://files.pythonhosted.org/packages/c1/7c/e8e5cc9edb967d44f6f85e9ebdad440b59af3fae00b137a4327dc5aed9bb/ormsgpack-1.12.1-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:3c0bd63897c439931cdf29348e5e6e8c330d529830e848d10767615c0f3d1b82", size = 388077, upload-time = "2025-12-14T07:57:14.551Z" },
+    { url = "https://files.pythonhosted.org/packages/35/6b/5031797e43b58506f28a8760b26dc23f2620fb4f2200c4c1b3045603e67e/ormsgpack-1.12.1-cp312-cp312-win_amd64.whl", hash = "sha256:362f2e812f8d7035dc25a009171e09d7cc97cb30d3c9e75a16aeae00ca3c1dcf", size = 116190, upload-time = "2025-12-14T07:57:15.575Z" },
+    { url = "https://files.pythonhosted.org/packages/1e/fd/9f43ea6425e383a6b2dbfafebb06fd60e8d68c700ef715adfbcdb499f75d/ormsgpack-1.12.1-cp312-cp312-win_arm64.whl", hash = "sha256:6190281e381db2ed0045052208f47a995ccf61eed48f1215ae3cce3fbccd59c5", size = 109990, upload-time = "2025-12-14T07:57:16.419Z" },
+    { url = "https://files.pythonhosted.org/packages/11/42/f110dfe7cf23a52a82e23eb23d9a6a76ae495447d474686dfa758f3d71d6/ormsgpack-1.12.1-cp313-cp313-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl", hash = "sha256:9663d6b3ecc917c063d61a99169ce196a80f3852e541ae404206836749459279", size = 376746, upload-time = "2025-12-14T07:57:17.699Z" },
+    { url = "https://files.pythonhosted.org/packages/11/76/b386e508a8ae207daec240201a81adb26467bf99b163560724e86bd9ff33/ormsgpack-1.12.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:32e85cfbaf01a94a92520e7fe7851cfcfe21a5698299c28ab86194895f9b9233", size = 202489, upload-time = "2025-12-14T07:57:18.807Z" },
+    { url = "https://files.pythonhosted.org/packages/ea/0e/5db7a63f387149024572daa3d9512fe8fb14bf4efa0722d6d491bed280e7/ormsgpack-1.12.1-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:dabfd2c24b59c7c69870a5ecee480dfae914a42a0c2e7c9d971cf531e2ba471a", size = 210757, upload-time = "2025-12-14T07:57:19.893Z" },
+    { url = "https://files.pythonhosted.org/packages/64/79/3a9899e57cb57430bd766fc1b4c9ad410cb2ba6070bc8cf6301e7d385768/ormsgpack-1.12.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:51bbf2b64afeded34ccd8e25402e4bca038757913931fa0d693078d75563f6f9", size = 211518, upload-time = "2025-12-14T07:57:20.972Z" },
+    { url = "https://files.pythonhosted.org/packages/d7/cd/4f41710ae9fe50d7fcbe476793b3c487746d0e1cc194cc0fee42ff6d989b/ormsgpack-1.12.1-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:9959a71dde1bd0ced84af17facc06a8afada495a34e9cb1bad8e9b20d4c59cef", size = 386251, upload-time = "2025-12-14T07:57:22.099Z" },
+    { url = "https://files.pythonhosted.org/packages/bf/54/ba0c97d6231b1f01daafaa520c8cce1e1b7fceaae6fdc1c763925874a7de/ormsgpack-1.12.1-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:e9be0e3b62d758f21f5b20e0e06b3a240ec546c4a327bf771f5825462aa74714", size = 479607, upload-time = "2025-12-14T07:57:23.525Z" },
+    { url = "https://files.pythonhosted.org/packages/18/75/19a9a97a462776d525baf41cfb7072734528775f0a3d5fbfab3aa7756b9b/ormsgpack-1.12.1-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:a29d49ab7fdd77ea787818e60cb4ef491708105b9c4c9b0f919201625eb036b5", size = 388062, upload-time = "2025-12-14T07:57:24.616Z" },
+    { url = "https://files.pythonhosted.org/packages/a8/6a/ec26e3f44e9632ecd2f43638b7b37b500eaea5d79cab984ad0b94be14f82/ormsgpack-1.12.1-cp313-cp313-win_amd64.whl", hash = "sha256:c418390b47a1d367e803f6c187f77e4d67c7ae07ba962e3a4a019001f4b0291a", size = 116195, upload-time = "2025-12-14T07:57:25.626Z" },
+    { url = "https://files.pythonhosted.org/packages/7d/64/bfa5f4a34d0f15c6aba1b73e73f7441a66d635bd03249d334a4796b7a924/ormsgpack-1.12.1-cp313-cp313-win_arm64.whl", hash = "sha256:cfa22c91cffc10a7fbd43729baff2de7d9c28cef2509085a704168ae31f02568", size = 109986, upload-time = "2025-12-14T07:57:26.569Z" },
+    { url = "https://files.pythonhosted.org/packages/87/0e/78e5697164e3223b9b216c13e99f1acbc1ee9833490d68842b13da8ba883/ormsgpack-1.12.1-cp314-cp314-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl", hash = "sha256:b93c91efb1a70751a1902a5b43b27bd8fd38e0ca0365cf2cde2716423c15c3a6", size = 376758, upload-time = "2025-12-14T07:57:27.641Z" },
+    { url = "https://files.pythonhosted.org/packages/2c/0e/3a3cbb64703263d7bbaed7effa3ce78cb9add360a60aa7c544d7df28b641/ormsgpack-1.12.1-cp314-cp314-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:3cf0ea0389167b5fa8d2933dd3f33e887ec4ba68f89c25214d7eec4afd746d22", size = 202487, upload-time = "2025-12-14T07:57:29.051Z" },
+    { url = "https://files.pythonhosted.org/packages/d7/2c/807ebe2b77995599bbb1dec8c3f450d5d7dddee14ce3e1e71dc60e2e2a74/ormsgpack-1.12.1-cp314-cp314-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:f4c29af837f35af3375070689e781161e7cf019eb2f7cd641734ae45cd001c0d", size = 210853, upload-time = "2025-12-14T07:57:30.508Z" },
+    { url = "https://files.pythonhosted.org/packages/25/57/2cdfc354e3ad8e847628f511f4d238799d90e9e090941e50b9d5ba955ae2/ormsgpack-1.12.1-cp314-cp314-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:336fc65aa0fe65896a3dabaae31e332a0a98b4a00ad7b0afde21a7505fd23ff3", size = 211545, upload-time = "2025-12-14T07:57:31.585Z" },
+    { url = "https://files.pythonhosted.org/packages/76/1d/c6fda560e4a8ff865b3aec8a86f7c95ab53f4532193a6ae4ab9db35f85aa/ormsgpack-1.12.1-cp314-cp314-musllinux_1_2_aarch64.whl", hash = "sha256:940f60aabfefe71dd6b82cb33f4ff10b2e7f5fcfa5f103cdb0a23b6aae4c713c", size = 386333, upload-time = "2025-12-14T07:57:32.957Z" },
+    { url = "https://files.pythonhosted.org/packages/fc/3e/715081b36fceb8b497c68b87d384e1cc6d9c9c130ce3b435634d3d785b86/ormsgpack-1.12.1-cp314-cp314-musllinux_1_2_armv7l.whl", hash = "sha256:596ad9e1b6d4c95595c54aaf49b1392609ca68f562ce06f4f74a5bc4053bcda4", size = 479701, upload-time = "2025-12-14T07:57:34.686Z" },
+    { url = "https://files.pythonhosted.org/packages/6d/cf/01ad04def42b3970fc1a302c07f4b46339edf62ef9650247097260471f40/ormsgpack-1.12.1-cp314-cp314-musllinux_1_2_x86_64.whl", hash = "sha256:575210e8fcbc7b0375026ba040a5eef223e9f66a4453d9623fc23282ae09c3c8", size = 388148, upload-time = "2025-12-14T07:57:35.771Z" },
+    { url = "https://files.pythonhosted.org/packages/15/91/1fff2fc2b5943c740028f339154e7103c8f2edf1a881d9fbba2ce11c3b1d/ormsgpack-1.12.1-cp314-cp314-win_amd64.whl", hash = "sha256:647daa3718572280893456be44c60aea6690b7f2edc54c55648ee66e8f06550f", size = 116201, upload-time = "2025-12-14T07:57:36.763Z" },
+    { url = "https://files.pythonhosted.org/packages/ed/66/142b542aed3f96002c7d1c33507ca6e1e0d0a42b9253ab27ef7ed5793bd9/ormsgpack-1.12.1-cp314-cp314-win_arm64.whl", hash = "sha256:a8b3ab762a6deaf1b6490ab46dda0c51528cf8037e0246c40875c6fe9e37b699", size = 110029, upload-time = "2025-12-14T07:57:37.703Z" },
+    { url = "https://files.pythonhosted.org/packages/38/b3/ef4494438c90359e1547eaed3c5ec46e2c431d59a3de2af4e70ebd594c49/ormsgpack-1.12.1-cp314-cp314t-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl", hash = "sha256:12087214e436c1f6c28491949571abea759a63111908c4f7266586d78144d7a8", size = 376777, upload-time = "2025-12-14T07:57:38.795Z" },
+    { url = "https://files.pythonhosted.org/packages/05/a0/1149a7163f8b0dfbc64bf9099b6f16d102ad3b03bcc11afee198d751da2d/ormsgpack-1.12.1-cp314-cp314t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:4e6d54c14cf86ef13f10ccade94d1e7de146aa9b17d371e18b16e95f329393b7", size = 202490, upload-time = "2025-12-14T07:57:40.168Z" },
+    { url = "https://files.pythonhosted.org/packages/68/82/f2ec5e758d6a7106645cca9bb7137d98bce5d363789fa94075be6572057c/ormsgpack-1.12.1-cp314-cp314t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:5f3584d07882b7ea2a1a589f795a3af97fe4c2932b739408e6d1d9d286cad862", size = 211733, upload-time = "2025-12-14T07:57:42.253Z" },
+]
+
 [[package]]
 name = "packaging"
 version = "25.0"
@@ -1849,6 +2640,105 @@ wheels = [
     { url = "https://files.pythonhosted.org/packages/f4/d1/8d1b28d007da43c750367c8bf5cb0f22758c16b1104b2b73b9acadb2d17a/polars_runtime_32-1.35.2-cp39-abi3-win_arm64.whl", hash = "sha256:6861145aa321a44eda7cc6694fb7751cb7aa0f21026df51b5faa52e64f9dc39b", size = 36955684, upload-time = "2025-11-09T13:19:15.666Z" },
 ]
 
+[[package]]
+name = "propcache"
+version = "0.4.1"
+source = { registry = "https://pypi.org/simple" }
+sdist = { url = "https://files.pythonhosted.org/packages/9e/da/e9fc233cf63743258bff22b3dfa7ea5baef7b5bc324af47a0ad89b8ffc6f/propcache-0.4.1.tar.gz", hash = "sha256:f48107a8c637e80362555f37ecf49abe20370e557cc4ab374f04ec4423c97c3d", size = 46442, upload-time = "2025-10-08T19:49:02.291Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/8c/d4/4e2c9aaf7ac2242b9358f98dccd8f90f2605402f5afeff6c578682c2c491/propcache-0.4.1-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:60a8fda9644b7dfd5dece8c61d8a85e271cb958075bfc4e01083c148b61a7caf", size = 80208, upload-time = "2025-10-08T19:46:24.597Z" },
+    { url = "https://files.pythonhosted.org/packages/c2/21/d7b68e911f9c8e18e4ae43bdbc1e1e9bbd971f8866eb81608947b6f585ff/propcache-0.4.1-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:c30b53e7e6bda1d547cabb47c825f3843a0a1a42b0496087bb58d8fedf9f41b5", size = 45777, upload-time = "2025-10-08T19:46:25.733Z" },
+    { url = "https://files.pythonhosted.org/packages/d3/1d/11605e99ac8ea9435651ee71ab4cb4bf03f0949586246476a25aadfec54a/propcache-0.4.1-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:6918ecbd897443087a3b7cd978d56546a812517dcaaca51b49526720571fa93e", size = 47647, upload-time = "2025-10-08T19:46:27.304Z" },
+    { url = "https://files.pythonhosted.org/packages/58/1a/3c62c127a8466c9c843bccb503d40a273e5cc69838805f322e2826509e0d/propcache-0.4.1-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:3d902a36df4e5989763425a8ab9e98cd8ad5c52c823b34ee7ef307fd50582566", size = 214929, upload-time = "2025-10-08T19:46:28.62Z" },
+    { url = "https://files.pythonhosted.org/packages/56/b9/8fa98f850960b367c4b8fe0592e7fc341daa7a9462e925228f10a60cf74f/propcache-0.4.1-cp311-cp311-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:a9695397f85973bb40427dedddf70d8dc4a44b22f1650dd4af9eedf443d45165", size = 221778, upload-time = "2025-10-08T19:46:30.358Z" },
+    { url = "https://files.pythonhosted.org/packages/46/a6/0ab4f660eb59649d14b3d3d65c439421cf2f87fe5dd68591cbe3c1e78a89/propcache-0.4.1-cp311-cp311-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:2bb07ffd7eaad486576430c89f9b215f9e4be68c4866a96e97db9e97fead85dc", size = 228144, upload-time = "2025-10-08T19:46:32.607Z" },
+    { url = "https://files.pythonhosted.org/packages/52/6a/57f43e054fb3d3a56ac9fc532bc684fc6169a26c75c353e65425b3e56eef/propcache-0.4.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:fd6f30fdcf9ae2a70abd34da54f18da086160e4d7d9251f81f3da0ff84fc5a48", size = 210030, upload-time = "2025-10-08T19:46:33.969Z" },
+    { url = "https://files.pythonhosted.org/packages/40/e2/27e6feebb5f6b8408fa29f5efbb765cd54c153ac77314d27e457a3e993b7/propcache-0.4.1-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:fc38cba02d1acba4e2869eef1a57a43dfbd3d49a59bf90dda7444ec2be6a5570", size = 208252, upload-time = "2025-10-08T19:46:35.309Z" },
+    { url = "https://files.pythonhosted.org/packages/9e/f8/91c27b22ccda1dbc7967f921c42825564fa5336a01ecd72eb78a9f4f53c2/propcache-0.4.1-cp311-cp311-musllinux_1_2_armv7l.whl", hash = "sha256:67fad6162281e80e882fb3ec355398cf72864a54069d060321f6cd0ade95fe85", size = 202064, upload-time = "2025-10-08T19:46:36.993Z" },
+    { url = "https://files.pythonhosted.org/packages/f2/26/7f00bd6bd1adba5aafe5f4a66390f243acab58eab24ff1a08bebb2ef9d40/propcache-0.4.1-cp311-cp311-musllinux_1_2_ppc64le.whl", hash = "sha256:f10207adf04d08bec185bae14d9606a1444715bc99180f9331c9c02093e1959e", size = 212429, upload-time = "2025-10-08T19:46:38.398Z" },
+    { url = "https://files.pythonhosted.org/packages/84/89/fd108ba7815c1117ddca79c228f3f8a15fc82a73bca8b142eb5de13b2785/propcache-0.4.1-cp311-cp311-musllinux_1_2_s390x.whl", hash = "sha256:e9b0d8d0845bbc4cfcdcbcdbf5086886bc8157aa963c31c777ceff7846c77757", size = 216727, upload-time = "2025-10-08T19:46:39.732Z" },
+    { url = "https://files.pythonhosted.org/packages/79/37/3ec3f7e3173e73f1d600495d8b545b53802cbf35506e5732dd8578db3724/propcache-0.4.1-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:981333cb2f4c1896a12f4ab92a9cc8f09ea664e9b7dbdc4eff74627af3a11c0f", size = 205097, upload-time = "2025-10-08T19:46:41.025Z" },
+    { url = "https://files.pythonhosted.org/packages/61/b0/b2631c19793f869d35f47d5a3a56fb19e9160d3c119f15ac7344fc3ccae7/propcache-0.4.1-cp311-cp311-win32.whl", hash = "sha256:f1d2f90aeec838a52f1c1a32fe9a619fefd5e411721a9117fbf82aea638fe8a1", size = 38084, upload-time = "2025-10-08T19:46:42.693Z" },
+    { url = "https://files.pythonhosted.org/packages/f4/78/6cce448e2098e9f3bfc91bb877f06aa24b6ccace872e39c53b2f707c4648/propcache-0.4.1-cp311-cp311-win_amd64.whl", hash = "sha256:364426a62660f3f699949ac8c621aad6977be7126c5807ce48c0aeb8e7333ea6", size = 41637, upload-time = "2025-10-08T19:46:43.778Z" },
+    { url = "https://files.pythonhosted.org/packages/9c/e9/754f180cccd7f51a39913782c74717c581b9cc8177ad0e949f4d51812383/propcache-0.4.1-cp311-cp311-win_arm64.whl", hash = "sha256:e53f3a38d3510c11953f3e6a33f205c6d1b001129f972805ca9b42fc308bc239", size = 38064, upload-time = "2025-10-08T19:46:44.872Z" },
+    { url = "https://files.pythonhosted.org/packages/a2/0f/f17b1b2b221d5ca28b4b876e8bb046ac40466513960646bda8e1853cdfa2/propcache-0.4.1-cp312-cp312-macosx_10_13_universal2.whl", hash = "sha256:e153e9cd40cc8945138822807139367f256f89c6810c2634a4f6902b52d3b4e2", size = 80061, upload-time = "2025-10-08T19:46:46.075Z" },
+    { url = "https://files.pythonhosted.org/packages/76/47/8ccf75935f51448ba9a16a71b783eb7ef6b9ee60f5d14c7f8a8a79fbeed7/propcache-0.4.1-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:cd547953428f7abb73c5ad82cbb32109566204260d98e41e5dfdc682eb7f8403", size = 46037, upload-time = "2025-10-08T19:46:47.23Z" },
+    { url = "https://files.pythonhosted.org/packages/0a/b6/5c9a0e42df4d00bfb4a3cbbe5cf9f54260300c88a0e9af1f47ca5ce17ac0/propcache-0.4.1-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:f048da1b4f243fc44f205dfd320933a951b8d89e0afd4c7cacc762a8b9165207", size = 47324, upload-time = "2025-10-08T19:46:48.384Z" },
+    { url = "https://files.pythonhosted.org/packages/9e/d3/6c7ee328b39a81ee877c962469f1e795f9db87f925251efeb0545e0020d0/propcache-0.4.1-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:ec17c65562a827bba85e3872ead335f95405ea1674860d96483a02f5c698fa72", size = 225505, upload-time = "2025-10-08T19:46:50.055Z" },
+    { url = "https://files.pythonhosted.org/packages/01/5d/1c53f4563490b1d06a684742cc6076ef944bc6457df6051b7d1a877c057b/propcache-0.4.1-cp312-cp312-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:405aac25c6394ef275dee4c709be43745d36674b223ba4eb7144bf4d691b7367", size = 230242, upload-time = "2025-10-08T19:46:51.815Z" },
+    { url = "https://files.pythonhosted.org/packages/20/e1/ce4620633b0e2422207c3cb774a0ee61cac13abc6217763a7b9e2e3f4a12/propcache-0.4.1-cp312-cp312-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:0013cb6f8dde4b2a2f66903b8ba740bdfe378c943c4377a200551ceb27f379e4", size = 238474, upload-time = "2025-10-08T19:46:53.208Z" },
+    { url = "https://files.pythonhosted.org/packages/46/4b/3aae6835b8e5f44ea6a68348ad90f78134047b503765087be2f9912140ea/propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:15932ab57837c3368b024473a525e25d316d8353016e7cc0e5ba9eb343fbb1cf", size = 221575, upload-time = "2025-10-08T19:46:54.511Z" },
+    { url = "https://files.pythonhosted.org/packages/6e/a5/8a5e8678bcc9d3a1a15b9a29165640d64762d424a16af543f00629c87338/propcache-0.4.1-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:031dce78b9dc099f4c29785d9cf5577a3faf9ebf74ecbd3c856a7b92768c3df3", size = 216736, upload-time = "2025-10-08T19:46:56.212Z" },
+    { url = "https://files.pythonhosted.org/packages/f1/63/b7b215eddeac83ca1c6b934f89d09a625aa9ee4ba158338854c87210cc36/propcache-0.4.1-cp312-cp312-musllinux_1_2_armv7l.whl", hash = "sha256:ab08df6c9a035bee56e31af99be621526bd237bea9f32def431c656b29e41778", size = 213019, upload-time = "2025-10-08T19:46:57.595Z" },
+    { url = "https://files.pythonhosted.org/packages/57/74/f580099a58c8af587cac7ba19ee7cb418506342fbbe2d4a4401661cca886/propcache-0.4.1-cp312-cp312-musllinux_1_2_ppc64le.whl", hash = "sha256:4d7af63f9f93fe593afbf104c21b3b15868efb2c21d07d8732c0c4287e66b6a6", size = 220376, upload-time = "2025-10-08T19:46:59.067Z" },
+    { url = "https://files.pythonhosted.org/packages/c4/ee/542f1313aff7eaf19c2bb758c5d0560d2683dac001a1c96d0774af799843/propcache-0.4.1-cp312-cp312-musllinux_1_2_s390x.whl", hash = "sha256:cfc27c945f422e8b5071b6e93169679e4eb5bf73bbcbf1ba3ae3a83d2f78ebd9", size = 226988, upload-time = "2025-10-08T19:47:00.544Z" },
+    { url = "https://files.pythonhosted.org/packages/8f/18/9c6b015dd9c6930f6ce2229e1f02fb35298b847f2087ea2b436a5bfa7287/propcache-0.4.1-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:35c3277624a080cc6ec6f847cbbbb5b49affa3598c4535a0a4682a697aaa5c75", size = 215615, upload-time = "2025-10-08T19:47:01.968Z" },
+    { url = "https://files.pythonhosted.org/packages/80/9e/e7b85720b98c45a45e1fca6a177024934dc9bc5f4d5dd04207f216fc33ed/propcache-0.4.1-cp312-cp312-win32.whl", hash = "sha256:671538c2262dadb5ba6395e26c1731e1d52534bfe9ae56d0b5573ce539266aa8", size = 38066, upload-time = "2025-10-08T19:47:03.503Z" },
+    { url = "https://files.pythonhosted.org/packages/54/09/d19cff2a5aaac632ec8fc03737b223597b1e347416934c1b3a7df079784c/propcache-0.4.1-cp312-cp312-win_amd64.whl", hash = "sha256:cb2d222e72399fcf5890d1d5cc1060857b9b236adff2792ff48ca2dfd46c81db", size = 41655, upload-time = "2025-10-08T19:47:04.973Z" },
+    { url = "https://files.pythonhosted.org/packages/68/ab/6b5c191bb5de08036a8c697b265d4ca76148efb10fa162f14af14fb5f076/propcache-0.4.1-cp312-cp312-win_arm64.whl", hash = "sha256:204483131fb222bdaaeeea9f9e6c6ed0cac32731f75dfc1d4a567fc1926477c1", size = 37789, upload-time = "2025-10-08T19:47:06.077Z" },
+    { url = "https://files.pythonhosted.org/packages/bf/df/6d9c1b6ac12b003837dde8a10231a7344512186e87b36e855bef32241942/propcache-0.4.1-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:43eedf29202c08550aac1d14e0ee619b0430aaef78f85864c1a892294fbc28cf", size = 77750, upload-time = "2025-10-08T19:47:07.648Z" },
+    { url = "https://files.pythonhosted.org/packages/8b/e8/677a0025e8a2acf07d3418a2e7ba529c9c33caf09d3c1f25513023c1db56/propcache-0.4.1-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:d62cdfcfd89ccb8de04e0eda998535c406bf5e060ffd56be6c586cbcc05b3311", size = 44780, upload-time = "2025-10-08T19:47:08.851Z" },
+    { url = "https://files.pythonhosted.org/packages/89/a4/92380f7ca60f99ebae761936bc48a72a639e8a47b29050615eef757cb2a7/propcache-0.4.1-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:cae65ad55793da34db5f54e4029b89d3b9b9490d8abe1b4c7ab5d4b8ec7ebf74", size = 46308, upload-time = "2025-10-08T19:47:09.982Z" },
+    { url = "https://files.pythonhosted.org/packages/2d/48/c5ac64dee5262044348d1d78a5f85dd1a57464a60d30daee946699963eb3/propcache-0.4.1-cp313-cp313-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:333ddb9031d2704a301ee3e506dc46b1fe5f294ec198ed6435ad5b6a085facfe", size = 208182, upload-time = "2025-10-08T19:47:11.319Z" },
+    { url = "https://files.pythonhosted.org/packages/c6/0c/cd762dd011a9287389a6a3eb43aa30207bde253610cca06824aeabfe9653/propcache-0.4.1-cp313-cp313-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:fd0858c20f078a32cf55f7e81473d96dcf3b93fd2ccdb3d40fdf54b8573df3af", size = 211215, upload-time = "2025-10-08T19:47:13.146Z" },
+    { url = "https://files.pythonhosted.org/packages/30/3e/49861e90233ba36890ae0ca4c660e95df565b2cd15d4a68556ab5865974e/propcache-0.4.1-cp313-cp313-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:678ae89ebc632c5c204c794f8dab2837c5f159aeb59e6ed0539500400577298c", size = 218112, upload-time = "2025-10-08T19:47:14.913Z" },
+    { url = "https://files.pythonhosted.org/packages/f1/8b/544bc867e24e1bd48f3118cecd3b05c694e160a168478fa28770f22fd094/propcache-0.4.1-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:d472aeb4fbf9865e0c6d622d7f4d54a4e101a89715d8904282bb5f9a2f476c3f", size = 204442, upload-time = "2025-10-08T19:47:16.277Z" },
+    { url = "https://files.pythonhosted.org/packages/50/a6/4282772fd016a76d3e5c0df58380a5ea64900afd836cec2c2f662d1b9bb3/propcache-0.4.1-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:4d3df5fa7e36b3225954fba85589da77a0fe6a53e3976de39caf04a0db4c36f1", size = 199398, upload-time = "2025-10-08T19:47:17.962Z" },
+    { url = "https://files.pythonhosted.org/packages/3e/ec/d8a7cd406ee1ddb705db2139f8a10a8a427100347bd698e7014351c7af09/propcache-0.4.1-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:ee17f18d2498f2673e432faaa71698032b0127ebf23ae5974eeaf806c279df24", size = 196920, upload-time = "2025-10-08T19:47:19.355Z" },
+    { url = "https://files.pythonhosted.org/packages/f6/6c/f38ab64af3764f431e359f8baf9e0a21013e24329e8b85d2da32e8ed07ca/propcache-0.4.1-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:580e97762b950f993ae618e167e7be9256b8353c2dcd8b99ec100eb50f5286aa", size = 203748, upload-time = "2025-10-08T19:47:21.338Z" },
+    { url = "https://files.pythonhosted.org/packages/d6/e3/fa846bd70f6534d647886621388f0a265254d30e3ce47e5c8e6e27dbf153/propcache-0.4.1-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:501d20b891688eb8e7aa903021f0b72d5a55db40ffaab27edefd1027caaafa61", size = 205877, upload-time = "2025-10-08T19:47:23.059Z" },
+    { url = "https://files.pythonhosted.org/packages/e2/39/8163fc6f3133fea7b5f2827e8eba2029a0277ab2c5beee6c1db7b10fc23d/propcache-0.4.1-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:9a0bd56e5b100aef69bd8562b74b46254e7c8812918d3baa700c8a8009b0af66", size = 199437, upload-time = "2025-10-08T19:47:24.445Z" },
+    { url = "https://files.pythonhosted.org/packages/93/89/caa9089970ca49c7c01662bd0eeedfe85494e863e8043565aeb6472ce8fe/propcache-0.4.1-cp313-cp313-win32.whl", hash = "sha256:bcc9aaa5d80322bc2fb24bb7accb4a30f81e90ab8d6ba187aec0744bc302ad81", size = 37586, upload-time = "2025-10-08T19:47:25.736Z" },
+    { url = "https://files.pythonhosted.org/packages/f5/ab/f76ec3c3627c883215b5c8080debb4394ef5a7a29be811f786415fc1e6fd/propcache-0.4.1-cp313-cp313-win_amd64.whl", hash = "sha256:381914df18634f5494334d201e98245c0596067504b9372d8cf93f4bb23e025e", size = 40790, upload-time = "2025-10-08T19:47:26.847Z" },
+    { url = "https://files.pythonhosted.org/packages/59/1b/e71ae98235f8e2ba5004d8cb19765a74877abf189bc53fc0c80d799e56c3/propcache-0.4.1-cp313-cp313-win_arm64.whl", hash = "sha256:8873eb4460fd55333ea49b7d189749ecf6e55bf85080f11b1c4530ed3034cba1", size = 37158, upload-time = "2025-10-08T19:47:27.961Z" },
+    { url = "https://files.pythonhosted.org/packages/83/ce/a31bbdfc24ee0dcbba458c8175ed26089cf109a55bbe7b7640ed2470cfe9/propcache-0.4.1-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:92d1935ee1f8d7442da9c0c4fa7ac20d07e94064184811b685f5c4fada64553b", size = 81451, upload-time = "2025-10-08T19:47:29.445Z" },
+    { url = "https://files.pythonhosted.org/packages/25/9c/442a45a470a68456e710d96cacd3573ef26a1d0a60067e6a7d5e655621ed/propcache-0.4.1-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:473c61b39e1460d386479b9b2f337da492042447c9b685f28be4f74d3529e566", size = 46374, upload-time = "2025-10-08T19:47:30.579Z" },
+    { url = "https://files.pythonhosted.org/packages/f4/bf/b1d5e21dbc3b2e889ea4327044fb16312a736d97640fb8b6aa3f9c7b3b65/propcache-0.4.1-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:c0ef0aaafc66fbd87842a3fe3902fd889825646bc21149eafe47be6072725835", size = 48396, upload-time = "2025-10-08T19:47:31.79Z" },
+    { url = "https://files.pythonhosted.org/packages/f4/04/5b4c54a103d480e978d3c8a76073502b18db0c4bc17ab91b3cb5092ad949/propcache-0.4.1-cp313-cp313t-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:f95393b4d66bfae908c3ca8d169d5f79cd65636ae15b5e7a4f6e67af675adb0e", size = 275950, upload-time = "2025-10-08T19:47:33.481Z" },
+    { url = "https://files.pythonhosted.org/packages/b4/c1/86f846827fb969c4b78b0af79bba1d1ea2156492e1b83dea8b8a6ae27395/propcache-0.4.1-cp313-cp313t-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:c07fda85708bc48578467e85099645167a955ba093be0a2dcba962195676e859", size = 273856, upload-time = "2025-10-08T19:47:34.906Z" },
+    { url = "https://files.pythonhosted.org/packages/36/1d/fc272a63c8d3bbad6878c336c7a7dea15e8f2d23a544bda43205dfa83ada/propcache-0.4.1-cp313-cp313t-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:af223b406d6d000830c6f65f1e6431783fc3f713ba3e6cc8c024d5ee96170a4b", size = 280420, upload-time = "2025-10-08T19:47:36.338Z" },
+    { url = "https://files.pythonhosted.org/packages/07/0c/01f2219d39f7e53d52e5173bcb09c976609ba30209912a0680adfb8c593a/propcache-0.4.1-cp313-cp313t-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:a78372c932c90ee474559c5ddfffd718238e8673c340dc21fe45c5b8b54559a0", size = 263254, upload-time = "2025-10-08T19:47:37.692Z" },
+    { url = "https://files.pythonhosted.org/packages/2d/18/cd28081658ce597898f0c4d174d4d0f3c5b6d4dc27ffafeef835c95eb359/propcache-0.4.1-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:564d9f0d4d9509e1a870c920a89b2fec951b44bf5ba7d537a9e7c1ccec2c18af", size = 261205, upload-time = "2025-10-08T19:47:39.659Z" },
+    { url = "https://files.pythonhosted.org/packages/7a/71/1f9e22eb8b8316701c2a19fa1f388c8a3185082607da8e406a803c9b954e/propcache-0.4.1-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:17612831fda0138059cc5546f4d12a2aacfb9e47068c06af35c400ba58ba7393", size = 247873, upload-time = "2025-10-08T19:47:41.084Z" },
+    { url = "https://files.pythonhosted.org/packages/4a/65/3d4b61f36af2b4eddba9def857959f1016a51066b4f1ce348e0cf7881f58/propcache-0.4.1-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:41a89040cb10bd345b3c1a873b2bf36413d48da1def52f268a055f7398514874", size = 262739, upload-time = "2025-10-08T19:47:42.51Z" },
+    { url = "https://files.pythonhosted.org/packages/2a/42/26746ab087faa77c1c68079b228810436ccd9a5ce9ac85e2b7307195fd06/propcache-0.4.1-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:e35b88984e7fa64aacecea39236cee32dd9bd8c55f57ba8a75cf2399553f9bd7", size = 263514, upload-time = "2025-10-08T19:47:43.927Z" },
+    { url = "https://files.pythonhosted.org/packages/94/13/630690fe201f5502d2403dd3cfd451ed8858fe3c738ee88d095ad2ff407b/propcache-0.4.1-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:6f8b465489f927b0df505cbe26ffbeed4d6d8a2bbc61ce90eb074ff129ef0ab1", size = 257781, upload-time = "2025-10-08T19:47:45.448Z" },
+    { url = "https://files.pythonhosted.org/packages/92/f7/1d4ec5841505f423469efbfc381d64b7b467438cd5a4bbcbb063f3b73d27/propcache-0.4.1-cp313-cp313t-win32.whl", hash = "sha256:2ad890caa1d928c7c2965b48f3a3815c853180831d0e5503d35cf00c472f4717", size = 41396, upload-time = "2025-10-08T19:47:47.202Z" },
+    { url = "https://files.pythonhosted.org/packages/48/f0/615c30622316496d2cbbc29f5985f7777d3ada70f23370608c1d3e081c1f/propcache-0.4.1-cp313-cp313t-win_amd64.whl", hash = "sha256:f7ee0e597f495cf415bcbd3da3caa3bd7e816b74d0d52b8145954c5e6fd3ff37", size = 44897, upload-time = "2025-10-08T19:47:48.336Z" },
+    { url = "https://files.pythonhosted.org/packages/fd/ca/6002e46eccbe0e33dcd4069ef32f7f1c9e243736e07adca37ae8c4830ec3/propcache-0.4.1-cp313-cp313t-win_arm64.whl", hash = "sha256:929d7cbe1f01bb7baffb33dc14eb5691c95831450a26354cd210a8155170c93a", size = 39789, upload-time = "2025-10-08T19:47:49.876Z" },
+    { url = "https://files.pythonhosted.org/packages/8e/5c/bca52d654a896f831b8256683457ceddd490ec18d9ec50e97dfd8fc726a8/propcache-0.4.1-cp314-cp314-macosx_10_13_universal2.whl", hash = "sha256:3f7124c9d820ba5548d431afb4632301acf965db49e666aa21c305cbe8c6de12", size = 78152, upload-time = "2025-10-08T19:47:51.051Z" },
+    { url = "https://files.pythonhosted.org/packages/65/9b/03b04e7d82a5f54fb16113d839f5ea1ede58a61e90edf515f6577c66fa8f/propcache-0.4.1-cp314-cp314-macosx_10_13_x86_64.whl", hash = "sha256:c0d4b719b7da33599dfe3b22d3db1ef789210a0597bc650b7cee9c77c2be8c5c", size = 44869, upload-time = "2025-10-08T19:47:52.594Z" },
+    { url = "https://files.pythonhosted.org/packages/b2/fa/89a8ef0468d5833a23fff277b143d0573897cf75bd56670a6d28126c7d68/propcache-0.4.1-cp314-cp314-macosx_11_0_arm64.whl", hash = "sha256:9f302f4783709a78240ebc311b793f123328716a60911d667e0c036bc5dcbded", size = 46596, upload-time = "2025-10-08T19:47:54.073Z" },
+    { url = "https://files.pythonhosted.org/packages/86/bd/47816020d337f4a746edc42fe8d53669965138f39ee117414c7d7a340cfe/propcache-0.4.1-cp314-cp314-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:c80ee5802e3fb9ea37938e7eecc307fb984837091d5fd262bb37238b1ae97641", size = 206981, upload-time = "2025-10-08T19:47:55.715Z" },
+    { url = "https://files.pythonhosted.org/packages/df/f6/c5fa1357cc9748510ee55f37173eb31bfde6d94e98ccd9e6f033f2fc06e1/propcache-0.4.1-cp314-cp314-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:ed5a841e8bb29a55fb8159ed526b26adc5bdd7e8bd7bf793ce647cb08656cdf4", size = 211490, upload-time = "2025-10-08T19:47:57.499Z" },
+    { url = "https://files.pythonhosted.org/packages/80/1e/e5889652a7c4a3846683401a48f0f2e5083ce0ec1a8a5221d8058fbd1adf/propcache-0.4.1-cp314-cp314-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:55c72fd6ea2da4c318e74ffdf93c4fe4e926051133657459131a95c846d16d44", size = 215371, upload-time = "2025-10-08T19:47:59.317Z" },
+    { url = "https://files.pythonhosted.org/packages/b2/f2/889ad4b2408f72fe1a4f6a19491177b30ea7bf1a0fd5f17050ca08cfc882/propcache-0.4.1-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:8326e144341460402713f91df60ade3c999d601e7eb5ff8f6f7862d54de0610d", size = 201424, upload-time = "2025-10-08T19:48:00.67Z" },
+    { url = "https://files.pythonhosted.org/packages/27/73/033d63069b57b0812c8bd19f311faebeceb6ba31b8f32b73432d12a0b826/propcache-0.4.1-cp314-cp314-musllinux_1_2_aarch64.whl", hash = "sha256:060b16ae65bc098da7f6d25bf359f1f31f688384858204fe5d652979e0015e5b", size = 197566, upload-time = "2025-10-08T19:48:02.604Z" },
+    { url = "https://files.pythonhosted.org/packages/dc/89/ce24f3dc182630b4e07aa6d15f0ff4b14ed4b9955fae95a0b54c58d66c05/propcache-0.4.1-cp314-cp314-musllinux_1_2_armv7l.whl", hash = "sha256:89eb3fa9524f7bec9de6e83cf3faed9d79bffa560672c118a96a171a6f55831e", size = 193130, upload-time = "2025-10-08T19:48:04.499Z" },
+    { url = "https://files.pythonhosted.org/packages/a9/24/ef0d5fd1a811fb5c609278d0209c9f10c35f20581fcc16f818da959fc5b4/propcache-0.4.1-cp314-cp314-musllinux_1_2_ppc64le.whl", hash = "sha256:dee69d7015dc235f526fe80a9c90d65eb0039103fe565776250881731f06349f", size = 202625, upload-time = "2025-10-08T19:48:06.213Z" },
+    { url = "https://files.pythonhosted.org/packages/f5/02/98ec20ff5546f68d673df2f7a69e8c0d076b5abd05ca882dc7ee3a83653d/propcache-0.4.1-cp314-cp314-musllinux_1_2_s390x.whl", hash = "sha256:5558992a00dfd54ccbc64a32726a3357ec93825a418a401f5cc67df0ac5d9e49", size = 204209, upload-time = "2025-10-08T19:48:08.432Z" },
+    { url = "https://files.pythonhosted.org/packages/a0/87/492694f76759b15f0467a2a93ab68d32859672b646aa8a04ce4864e7932d/propcache-0.4.1-cp314-cp314-musllinux_1_2_x86_64.whl", hash = "sha256:c9b822a577f560fbd9554812526831712c1436d2c046cedee4c3796d3543b144", size = 197797, upload-time = "2025-10-08T19:48:09.968Z" },
+    { url = "https://files.pythonhosted.org/packages/ee/36/66367de3575db1d2d3f3d177432bd14ee577a39d3f5d1b3d5df8afe3b6e2/propcache-0.4.1-cp314-cp314-win32.whl", hash = "sha256:ab4c29b49d560fe48b696cdcb127dd36e0bc2472548f3bf56cc5cb3da2b2984f", size = 38140, upload-time = "2025-10-08T19:48:11.232Z" },
+    { url = "https://files.pythonhosted.org/packages/0c/2a/a758b47de253636e1b8aef181c0b4f4f204bf0dd964914fb2af90a95b49b/propcache-0.4.1-cp314-cp314-win_amd64.whl", hash = "sha256:5a103c3eb905fcea0ab98be99c3a9a5ab2de60228aa5aceedc614c0281cf6153", size = 41257, upload-time = "2025-10-08T19:48:12.707Z" },
+    { url = "https://files.pythonhosted.org/packages/34/5e/63bd5896c3fec12edcbd6f12508d4890d23c265df28c74b175e1ef9f4f3b/propcache-0.4.1-cp314-cp314-win_arm64.whl", hash = "sha256:74c1fb26515153e482e00177a1ad654721bf9207da8a494a0c05e797ad27b992", size = 38097, upload-time = "2025-10-08T19:48:13.923Z" },
+    { url = "https://files.pythonhosted.org/packages/99/85/9ff785d787ccf9bbb3f3106f79884a130951436f58392000231b4c737c80/propcache-0.4.1-cp314-cp314t-macosx_10_13_universal2.whl", hash = "sha256:824e908bce90fb2743bd6b59db36eb4f45cd350a39637c9f73b1c1ea66f5b75f", size = 81455, upload-time = "2025-10-08T19:48:15.16Z" },
+    { url = "https://files.pythonhosted.org/packages/90/85/2431c10c8e7ddb1445c1f7c4b54d886e8ad20e3c6307e7218f05922cad67/propcache-0.4.1-cp314-cp314t-macosx_10_13_x86_64.whl", hash = "sha256:c2b5e7db5328427c57c8e8831abda175421b709672f6cfc3d630c3b7e2146393", size = 46372, upload-time = "2025-10-08T19:48:16.424Z" },
+    { url = "https://files.pythonhosted.org/packages/01/20/b0972d902472da9bcb683fa595099911f4d2e86e5683bcc45de60dd05dc3/propcache-0.4.1-cp314-cp314t-macosx_11_0_arm64.whl", hash = "sha256:6f6ff873ed40292cd4969ef5310179afd5db59fdf055897e282485043fc80ad0", size = 48411, upload-time = "2025-10-08T19:48:17.577Z" },
+    { url = "https://files.pythonhosted.org/packages/e2/e3/7dc89f4f21e8f99bad3d5ddb3a3389afcf9da4ac69e3deb2dcdc96e74169/propcache-0.4.1-cp314-cp314t-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:49a2dc67c154db2c1463013594c458881a069fcf98940e61a0569016a583020a", size = 275712, upload-time = "2025-10-08T19:48:18.901Z" },
+    { url = "https://files.pythonhosted.org/packages/20/67/89800c8352489b21a8047c773067644e3897f02ecbbd610f4d46b7f08612/propcache-0.4.1-cp314-cp314t-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:005f08e6a0529984491e37d8dbc3dd86f84bd78a8ceb5fa9a021f4c48d4984be", size = 273557, upload-time = "2025-10-08T19:48:20.762Z" },
+    { url = "https://files.pythonhosted.org/packages/e2/a1/b52b055c766a54ce6d9c16d9aca0cad8059acd9637cdf8aa0222f4a026ef/propcache-0.4.1-cp314-cp314t-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:5c3310452e0d31390da9035c348633b43d7e7feb2e37be252be6da45abd1abcc", size = 280015, upload-time = "2025-10-08T19:48:22.592Z" },
+    { url = "https://files.pythonhosted.org/packages/48/c8/33cee30bd890672c63743049f3c9e4be087e6780906bfc3ec58528be59c1/propcache-0.4.1-cp314-cp314t-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:4c3c70630930447f9ef1caac7728c8ad1c56bc5015338b20fed0d08ea2480b3a", size = 262880, upload-time = "2025-10-08T19:48:23.947Z" },
+    { url = "https://files.pythonhosted.org/packages/0c/b1/8f08a143b204b418285c88b83d00edbd61afbc2c6415ffafc8905da7038b/propcache-0.4.1-cp314-cp314t-musllinux_1_2_aarch64.whl", hash = "sha256:8e57061305815dfc910a3634dcf584f08168a8836e6999983569f51a8544cd89", size = 260938, upload-time = "2025-10-08T19:48:25.656Z" },
+    { url = "https://files.pythonhosted.org/packages/cf/12/96e4664c82ca2f31e1c8dff86afb867348979eb78d3cb8546a680287a1e9/propcache-0.4.1-cp314-cp314t-musllinux_1_2_armv7l.whl", hash = "sha256:521a463429ef54143092c11a77e04056dd00636f72e8c45b70aaa3140d639726", size = 247641, upload-time = "2025-10-08T19:48:27.207Z" },
+    { url = "https://files.pythonhosted.org/packages/18/ed/e7a9cfca28133386ba52278136d42209d3125db08d0a6395f0cba0c0285c/propcache-0.4.1-cp314-cp314t-musllinux_1_2_ppc64le.whl", hash = "sha256:120c964da3fdc75e3731aa392527136d4ad35868cc556fd09bb6d09172d9a367", size = 262510, upload-time = "2025-10-08T19:48:28.65Z" },
+    { url = "https://files.pythonhosted.org/packages/f5/76/16d8bf65e8845dd62b4e2b57444ab81f07f40caa5652b8969b87ddcf2ef6/propcache-0.4.1-cp314-cp314t-musllinux_1_2_s390x.whl", hash = "sha256:d8f353eb14ee3441ee844ade4277d560cdd68288838673273b978e3d6d2c8f36", size = 263161, upload-time = "2025-10-08T19:48:30.133Z" },
+    { url = "https://files.pythonhosted.org/packages/e7/70/c99e9edb5d91d5ad8a49fa3c1e8285ba64f1476782fed10ab251ff413ba1/propcache-0.4.1-cp314-cp314t-musllinux_1_2_x86_64.whl", hash = "sha256:ab2943be7c652f09638800905ee1bab2c544e537edb57d527997a24c13dc1455", size = 257393, upload-time = "2025-10-08T19:48:31.567Z" },
+    { url = "https://files.pythonhosted.org/packages/08/02/87b25304249a35c0915d236575bc3574a323f60b47939a2262b77632a3ee/propcache-0.4.1-cp314-cp314t-win32.whl", hash = "sha256:05674a162469f31358c30bcaa8883cb7829fa3110bf9c0991fe27d7896c42d85", size = 42546, upload-time = "2025-10-08T19:48:32.872Z" },
+    { url = "https://files.pythonhosted.org/packages/cb/ef/3c6ecf8b317aa982f309835e8f96987466123c6e596646d4e6a1dfcd080f/propcache-0.4.1-cp314-cp314t-win_amd64.whl", hash = "sha256:990f6b3e2a27d683cb7602ed6c86f15ee6b43b1194736f9baaeb93d0016633b1", size = 46259, upload-time = "2025-10-08T19:48:34.226Z" },
+    { url = "https://files.pythonhosted.org/packages/c4/2d/346e946d4951f37eca1e4f55be0f0174c52cd70720f84029b02f296f4a38/propcache-0.4.1-cp314-cp314t-win_arm64.whl", hash = "sha256:ecef2343af4cc68e05131e45024ba34f6095821988a9d0a02aa7c73fcc448aa9", size = 40428, upload-time = "2025-10-08T19:48:35.441Z" },
+    { url = "https://files.pythonhosted.org/packages/5b/5a/bc7b4a4ef808fa59a816c17b20c4bef6884daebbdf627ff2a161da67da19/propcache-0.4.1-py3-none-any.whl", hash = "sha256:af2a6052aeb6cf17d3e46ee169099044fd8224cbaf75c76a2ef596e8163e2237", size = 13305, upload-time = "2025-10-08T19:49:00.792Z" },
+]
+
 [[package]]
 name = "protobuf"
 version = "6.33.2"
@@ -1923,6 +2813,132 @@ wheels = [
     { url = "https://files.pythonhosted.org/packages/2e/c3/94ade4906a2f88bc935772f59c934013b4205e773bcb4239db114a6da136/pyarrow_hotfix-0.7-py3-none-any.whl", hash = "sha256:3236f3b5f1260f0e2ac070a55c1a7b339c4bb7267839bd2015e283234e758100", size = 7923, upload-time = "2025-04-25T10:17:05.224Z" },
 ]
 
+[[package]]
+name = "pydantic"
+version = "2.12.5"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "annotated-types" },
+    { name = "pydantic-core" },
+    { name = "typing-extensions" },
+    { name = "typing-inspection" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/69/44/36f1a6e523abc58ae5f928898e4aca2e0ea509b5aa6f6f392a5d882be928/pydantic-2.12.5.tar.gz", hash = "sha256:4d351024c75c0f085a9febbb665ce8c0c6ec5d30e903bdb6394b7ede26aebb49", size = 821591, upload-time = "2025-11-26T15:11:46.471Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/5a/87/b70ad306ebb6f9b585f114d0ac2137d792b48be34d732d60e597c2f8465a/pydantic-2.12.5-py3-none-any.whl", hash = "sha256:e561593fccf61e8a20fc46dfc2dfe075b8be7d0188df33f221ad1f0139180f9d", size = 463580, upload-time = "2025-11-26T15:11:44.605Z" },
+]
+
+[[package]]
+name = "pydantic-core"
+version = "2.41.5"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "typing-extensions" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/71/70/23b021c950c2addd24ec408e9ab05d59b035b39d97cdc1130e1bce647bb6/pydantic_core-2.41.5.tar.gz", hash = "sha256:08daa51ea16ad373ffd5e7606252cc32f07bc72b28284b6bc9c6df804816476e", size = 460952, upload-time = "2025-11-04T13:43:49.098Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/e8/72/74a989dd9f2084b3d9530b0915fdda64ac48831c30dbf7c72a41a5232db8/pydantic_core-2.41.5-cp311-cp311-macosx_10_12_x86_64.whl", hash = "sha256:a3a52f6156e73e7ccb0f8cced536adccb7042be67cb45f9562e12b319c119da6", size = 2105873, upload-time = "2025-11-04T13:39:31.373Z" },
+    { url = "https://files.pythonhosted.org/packages/12/44/37e403fd9455708b3b942949e1d7febc02167662bf1a7da5b78ee1ea2842/pydantic_core-2.41.5-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:7f3bf998340c6d4b0c9a2f02d6a400e51f123b59565d74dc60d252ce888c260b", size = 1899826, upload-time = "2025-11-04T13:39:32.897Z" },
+    { url = "https://files.pythonhosted.org/packages/33/7f/1d5cab3ccf44c1935a359d51a8a2a9e1a654b744b5e7f80d41b88d501eec/pydantic_core-2.41.5-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:378bec5c66998815d224c9ca994f1e14c0c21cb95d2f52b6021cc0b2a58f2a5a", size = 1917869, upload-time = "2025-11-04T13:39:34.469Z" },
+    { url = "https://files.pythonhosted.org/packages/6e/6a/30d94a9674a7fe4f4744052ed6c5e083424510be1e93da5bc47569d11810/pydantic_core-2.41.5-cp311-cp311-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:e7b576130c69225432866fe2f4a469a85a54ade141d96fd396dffcf607b558f8", size = 2063890, upload-time = "2025-11-04T13:39:36.053Z" },
+    { url = "https://files.pythonhosted.org/packages/50/be/76e5d46203fcb2750e542f32e6c371ffa9b8ad17364cf94bb0818dbfb50c/pydantic_core-2.41.5-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:6cb58b9c66f7e4179a2d5e0f849c48eff5c1fca560994d6eb6543abf955a149e", size = 2229740, upload-time = "2025-11-04T13:39:37.753Z" },
+    { url = "https://files.pythonhosted.org/packages/d3/ee/fed784df0144793489f87db310a6bbf8118d7b630ed07aa180d6067e653a/pydantic_core-2.41.5-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:88942d3a3dff3afc8288c21e565e476fc278902ae4d6d134f1eeda118cc830b1", size = 2350021, upload-time = "2025-11-04T13:39:40.94Z" },
+    { url = "https://files.pythonhosted.org/packages/c8/be/8fed28dd0a180dca19e72c233cbf58efa36df055e5b9d90d64fd1740b828/pydantic_core-2.41.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:f31d95a179f8d64d90f6831d71fa93290893a33148d890ba15de25642c5d075b", size = 2066378, upload-time = "2025-11-04T13:39:42.523Z" },
+    { url = "https://files.pythonhosted.org/packages/b0/3b/698cf8ae1d536a010e05121b4958b1257f0b5522085e335360e53a6b1c8b/pydantic_core-2.41.5-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:c1df3d34aced70add6f867a8cf413e299177e0c22660cc767218373d0779487b", size = 2175761, upload-time = "2025-11-04T13:39:44.553Z" },
+    { url = "https://files.pythonhosted.org/packages/b8/ba/15d537423939553116dea94ce02f9c31be0fa9d0b806d427e0308ec17145/pydantic_core-2.41.5-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:4009935984bd36bd2c774e13f9a09563ce8de4abaa7226f5108262fa3e637284", size = 2146303, upload-time = "2025-11-04T13:39:46.238Z" },
+    { url = "https://files.pythonhosted.org/packages/58/7f/0de669bf37d206723795f9c90c82966726a2ab06c336deba4735b55af431/pydantic_core-2.41.5-cp311-cp311-musllinux_1_1_armv7l.whl", hash = "sha256:34a64bc3441dc1213096a20fe27e8e128bd3ff89921706e83c0b1ac971276594", size = 2340355, upload-time = "2025-11-04T13:39:48.002Z" },
+    { url = "https://files.pythonhosted.org/packages/e5/de/e7482c435b83d7e3c3ee5ee4451f6e8973cff0eb6007d2872ce6383f6398/pydantic_core-2.41.5-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:c9e19dd6e28fdcaa5a1de679aec4141f691023916427ef9bae8584f9c2fb3b0e", size = 2319875, upload-time = "2025-11-04T13:39:49.705Z" },
+    { url = "https://files.pythonhosted.org/packages/fe/e6/8c9e81bb6dd7560e33b9053351c29f30c8194b72f2d6932888581f503482/pydantic_core-2.41.5-cp311-cp311-win32.whl", hash = "sha256:2c010c6ded393148374c0f6f0bf89d206bf3217f201faa0635dcd56bd1520f6b", size = 1987549, upload-time = "2025-11-04T13:39:51.842Z" },
+    { url = "https://files.pythonhosted.org/packages/11/66/f14d1d978ea94d1bc21fc98fcf570f9542fe55bfcc40269d4e1a21c19bf7/pydantic_core-2.41.5-cp311-cp311-win_amd64.whl", hash = "sha256:76ee27c6e9c7f16f47db7a94157112a2f3a00e958bc626e2f4ee8bec5c328fbe", size = 2011305, upload-time = "2025-11-04T13:39:53.485Z" },
+    { url = "https://files.pythonhosted.org/packages/56/d8/0e271434e8efd03186c5386671328154ee349ff0354d83c74f5caaf096ed/pydantic_core-2.41.5-cp311-cp311-win_arm64.whl", hash = "sha256:4bc36bbc0b7584de96561184ad7f012478987882ebf9f9c389b23f432ea3d90f", size = 1972902, upload-time = "2025-11-04T13:39:56.488Z" },
+    { url = "https://files.pythonhosted.org/packages/5f/5d/5f6c63eebb5afee93bcaae4ce9a898f3373ca23df3ccaef086d0233a35a7/pydantic_core-2.41.5-cp312-cp312-macosx_10_12_x86_64.whl", hash = "sha256:f41a7489d32336dbf2199c8c0a215390a751c5b014c2c1c5366e817202e9cdf7", size = 2110990, upload-time = "2025-11-04T13:39:58.079Z" },
+    { url = "https://files.pythonhosted.org/packages/aa/32/9c2e8ccb57c01111e0fd091f236c7b371c1bccea0fa85247ac55b1e2b6b6/pydantic_core-2.41.5-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:070259a8818988b9a84a449a2a7337c7f430a22acc0859c6b110aa7212a6d9c0", size = 1896003, upload-time = "2025-11-04T13:39:59.956Z" },
+    { url = "https://files.pythonhosted.org/packages/68/b8/a01b53cb0e59139fbc9e4fda3e9724ede8de279097179be4ff31f1abb65a/pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:e96cea19e34778f8d59fe40775a7a574d95816eb150850a85a7a4c8f4b94ac69", size = 1919200, upload-time = "2025-11-04T13:40:02.241Z" },
+    { url = "https://files.pythonhosted.org/packages/38/de/8c36b5198a29bdaade07b5985e80a233a5ac27137846f3bc2d3b40a47360/pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:ed2e99c456e3fadd05c991f8f437ef902e00eedf34320ba2b0842bd1c3ca3a75", size = 2052578, upload-time = "2025-11-04T13:40:04.401Z" },
+    { url = "https://files.pythonhosted.org/packages/00/b5/0e8e4b5b081eac6cb3dbb7e60a65907549a1ce035a724368c330112adfdd/pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:65840751b72fbfd82c3c640cff9284545342a4f1eb1586ad0636955b261b0b05", size = 2208504, upload-time = "2025-11-04T13:40:06.072Z" },
+    { url = "https://files.pythonhosted.org/packages/77/56/87a61aad59c7c5b9dc8caad5a41a5545cba3810c3e828708b3d7404f6cef/pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:e536c98a7626a98feb2d3eaf75944ef6f3dbee447e1f841eae16f2f0a72d8ddc", size = 2335816, upload-time = "2025-11-04T13:40:07.835Z" },
+    { url = "https://files.pythonhosted.org/packages/0d/76/941cc9f73529988688a665a5c0ecff1112b3d95ab48f81db5f7606f522d3/pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:eceb81a8d74f9267ef4081e246ffd6d129da5d87e37a77c9bde550cb04870c1c", size = 2075366, upload-time = "2025-11-04T13:40:09.804Z" },
+    { url = "https://files.pythonhosted.org/packages/d3/43/ebef01f69baa07a482844faaa0a591bad1ef129253ffd0cdaa9d8a7f72d3/pydantic_core-2.41.5-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:d38548150c39b74aeeb0ce8ee1d8e82696f4a4e16ddc6de7b1d8823f7de4b9b5", size = 2171698, upload-time = "2025-11-04T13:40:12.004Z" },
+    { url = "https://files.pythonhosted.org/packages/b1/87/41f3202e4193e3bacfc2c065fab7706ebe81af46a83d3e27605029c1f5a6/pydantic_core-2.41.5-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:c23e27686783f60290e36827f9c626e63154b82b116d7fe9adba1fda36da706c", size = 2132603, upload-time = "2025-11-04T13:40:13.868Z" },
+    { url = "https://files.pythonhosted.org/packages/49/7d/4c00df99cb12070b6bccdef4a195255e6020a550d572768d92cc54dba91a/pydantic_core-2.41.5-cp312-cp312-musllinux_1_1_armv7l.whl", hash = "sha256:482c982f814460eabe1d3bb0adfdc583387bd4691ef00b90575ca0d2b6fe2294", size = 2329591, upload-time = "2025-11-04T13:40:15.672Z" },
+    { url = "https://files.pythonhosted.org/packages/cc/6a/ebf4b1d65d458f3cda6a7335d141305dfa19bdc61140a884d165a8a1bbc7/pydantic_core-2.41.5-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:bfea2a5f0b4d8d43adf9d7b8bf019fb46fdd10a2e5cde477fbcb9d1fa08c68e1", size = 2319068, upload-time = "2025-11-04T13:40:17.532Z" },
+    { url = "https://files.pythonhosted.org/packages/49/3b/774f2b5cd4192d5ab75870ce4381fd89cf218af999515baf07e7206753f0/pydantic_core-2.41.5-cp312-cp312-win32.whl", hash = "sha256:b74557b16e390ec12dca509bce9264c3bbd128f8a2c376eaa68003d7f327276d", size = 1985908, upload-time = "2025-11-04T13:40:19.309Z" },
+    { url = "https://files.pythonhosted.org/packages/86/45/00173a033c801cacf67c190fef088789394feaf88a98a7035b0e40d53dc9/pydantic_core-2.41.5-cp312-cp312-win_amd64.whl", hash = "sha256:1962293292865bca8e54702b08a4f26da73adc83dd1fcf26fbc875b35d81c815", size = 2020145, upload-time = "2025-11-04T13:40:21.548Z" },
+    { url = "https://files.pythonhosted.org/packages/f9/22/91fbc821fa6d261b376a3f73809f907cec5ca6025642c463d3488aad22fb/pydantic_core-2.41.5-cp312-cp312-win_arm64.whl", hash = "sha256:1746d4a3d9a794cacae06a5eaaccb4b8643a131d45fbc9af23e353dc0a5ba5c3", size = 1976179, upload-time = "2025-11-04T13:40:23.393Z" },
+    { url = "https://files.pythonhosted.org/packages/87/06/8806241ff1f70d9939f9af039c6c35f2360cf16e93c2ca76f184e76b1564/pydantic_core-2.41.5-cp313-cp313-macosx_10_12_x86_64.whl", hash = "sha256:941103c9be18ac8daf7b7adca8228f8ed6bb7a1849020f643b3a14d15b1924d9", size = 2120403, upload-time = "2025-11-04T13:40:25.248Z" },
+    { url = "https://files.pythonhosted.org/packages/94/02/abfa0e0bda67faa65fef1c84971c7e45928e108fe24333c81f3bfe35d5f5/pydantic_core-2.41.5-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:112e305c3314f40c93998e567879e887a3160bb8689ef3d2c04b6cc62c33ac34", size = 1896206, upload-time = "2025-11-04T13:40:27.099Z" },
+    { url = "https://files.pythonhosted.org/packages/15/df/a4c740c0943e93e6500f9eb23f4ca7ec9bf71b19e608ae5b579678c8d02f/pydantic_core-2.41.5-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0cbaad15cb0c90aa221d43c00e77bb33c93e8d36e0bf74760cd00e732d10a6a0", size = 1919307, upload-time = "2025-11-04T13:40:29.806Z" },
+    { url = "https://files.pythonhosted.org/packages/9a/e3/6324802931ae1d123528988e0e86587c2072ac2e5394b4bc2bc34b61ff6e/pydantic_core-2.41.5-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:03ca43e12fab6023fc79d28ca6b39b05f794ad08ec2feccc59a339b02f2b3d33", size = 2063258, upload-time = "2025-11-04T13:40:33.544Z" },
+    { url = "https://files.pythonhosted.org/packages/c9/d4/2230d7151d4957dd79c3044ea26346c148c98fbf0ee6ebd41056f2d62ab5/pydantic_core-2.41.5-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:dc799088c08fa04e43144b164feb0c13f9a0bc40503f8df3e9fde58a3c0c101e", size = 2214917, upload-time = "2025-11-04T13:40:35.479Z" },
+    { url = "https://files.pythonhosted.org/packages/e6/9f/eaac5df17a3672fef0081b6c1bb0b82b33ee89aa5cec0d7b05f52fd4a1fa/pydantic_core-2.41.5-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:97aeba56665b4c3235a0e52b2c2f5ae9cd071b8a8310ad27bddb3f7fb30e9aa2", size = 2332186, upload-time = "2025-11-04T13:40:37.436Z" },
+    { url = "https://files.pythonhosted.org/packages/cf/4e/35a80cae583a37cf15604b44240e45c05e04e86f9cfd766623149297e971/pydantic_core-2.41.5-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:406bf18d345822d6c21366031003612b9c77b3e29ffdb0f612367352aab7d586", size = 2073164, upload-time = "2025-11-04T13:40:40.289Z" },
+    { url = "https://files.pythonhosted.org/packages/bf/e3/f6e262673c6140dd3305d144d032f7bd5f7497d3871c1428521f19f9efa2/pydantic_core-2.41.5-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:b93590ae81f7010dbe380cdeab6f515902ebcbefe0b9327cc4804d74e93ae69d", size = 2179146, upload-time = "2025-11-04T13:40:42.809Z" },
+    { url = "https://files.pythonhosted.org/packages/75/c7/20bd7fc05f0c6ea2056a4565c6f36f8968c0924f19b7d97bbfea55780e73/pydantic_core-2.41.5-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:01a3d0ab748ee531f4ea6c3e48ad9dac84ddba4b0d82291f87248f2f9de8d740", size = 2137788, upload-time = "2025-11-04T13:40:44.752Z" },
+    { url = "https://files.pythonhosted.org/packages/3a/8d/34318ef985c45196e004bc46c6eab2eda437e744c124ef0dbe1ff2c9d06b/pydantic_core-2.41.5-cp313-cp313-musllinux_1_1_armv7l.whl", hash = "sha256:6561e94ba9dacc9c61bce40e2d6bdc3bfaa0259d3ff36ace3b1e6901936d2e3e", size = 2340133, upload-time = "2025-11-04T13:40:46.66Z" },
+    { url = "https://files.pythonhosted.org/packages/9c/59/013626bf8c78a5a5d9350d12e7697d3d4de951a75565496abd40ccd46bee/pydantic_core-2.41.5-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:915c3d10f81bec3a74fbd4faebe8391013ba61e5a1a8d48c4455b923bdda7858", size = 2324852, upload-time = "2025-11-04T13:40:48.575Z" },
+    { url = "https://files.pythonhosted.org/packages/1a/d9/c248c103856f807ef70c18a4f986693a46a8ffe1602e5d361485da502d20/pydantic_core-2.41.5-cp313-cp313-win32.whl", hash = "sha256:650ae77860b45cfa6e2cdafc42618ceafab3a2d9a3811fcfbd3bbf8ac3c40d36", size = 1994679, upload-time = "2025-11-04T13:40:50.619Z" },
+    { url = "https://files.pythonhosted.org/packages/9e/8b/341991b158ddab181cff136acd2552c9f35bd30380422a639c0671e99a91/pydantic_core-2.41.5-cp313-cp313-win_amd64.whl", hash = "sha256:79ec52ec461e99e13791ec6508c722742ad745571f234ea6255bed38c6480f11", size = 2019766, upload-time = "2025-11-04T13:40:52.631Z" },
+    { url = "https://files.pythonhosted.org/packages/73/7d/f2f9db34af103bea3e09735bb40b021788a5e834c81eedb541991badf8f5/pydantic_core-2.41.5-cp313-cp313-win_arm64.whl", hash = "sha256:3f84d5c1b4ab906093bdc1ff10484838aca54ef08de4afa9de0f5f14d69639cd", size = 1981005, upload-time = "2025-11-04T13:40:54.734Z" },
+    { url = "https://files.pythonhosted.org/packages/ea/28/46b7c5c9635ae96ea0fbb779e271a38129df2550f763937659ee6c5dbc65/pydantic_core-2.41.5-cp314-cp314-macosx_10_12_x86_64.whl", hash = "sha256:3f37a19d7ebcdd20b96485056ba9e8b304e27d9904d233d7b1015db320e51f0a", size = 2119622, upload-time = "2025-11-04T13:40:56.68Z" },
+    { url = "https://files.pythonhosted.org/packages/74/1a/145646e5687e8d9a1e8d09acb278c8535ebe9e972e1f162ed338a622f193/pydantic_core-2.41.5-cp314-cp314-macosx_11_0_arm64.whl", hash = "sha256:1d1d9764366c73f996edd17abb6d9d7649a7eb690006ab6adbda117717099b14", size = 1891725, upload-time = "2025-11-04T13:40:58.807Z" },
+    { url = "https://files.pythonhosted.org/packages/23/04/e89c29e267b8060b40dca97bfc64a19b2a3cf99018167ea1677d96368273/pydantic_core-2.41.5-cp314-cp314-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:25e1c2af0fce638d5f1988b686f3b3ea8cd7de5f244ca147c777769e798a9cd1", size = 1915040, upload-time = "2025-11-04T13:41:00.853Z" },
+    { url = "https://files.pythonhosted.org/packages/84/a3/15a82ac7bd97992a82257f777b3583d3e84bdb06ba6858f745daa2ec8a85/pydantic_core-2.41.5-cp314-cp314-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:506d766a8727beef16b7adaeb8ee6217c64fc813646b424d0804d67c16eddb66", size = 2063691, upload-time = "2025-11-04T13:41:03.504Z" },
+    { url = "https://files.pythonhosted.org/packages/74/9b/0046701313c6ef08c0c1cf0e028c67c770a4e1275ca73131563c5f2a310a/pydantic_core-2.41.5-cp314-cp314-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:4819fa52133c9aa3c387b3328f25c1facc356491e6135b459f1de698ff64d869", size = 2213897, upload-time = "2025-11-04T13:41:05.804Z" },
+    { url = "https://files.pythonhosted.org/packages/8a/cd/6bac76ecd1b27e75a95ca3a9a559c643b3afcd2dd62086d4b7a32a18b169/pydantic_core-2.41.5-cp314-cp314-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:2b761d210c9ea91feda40d25b4efe82a1707da2ef62901466a42492c028553a2", size = 2333302, upload-time = "2025-11-04T13:41:07.809Z" },
+    { url = "https://files.pythonhosted.org/packages/4c/d2/ef2074dc020dd6e109611a8be4449b98cd25e1b9b8a303c2f0fca2f2bcf7/pydantic_core-2.41.5-cp314-cp314-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:22f0fb8c1c583a3b6f24df2470833b40207e907b90c928cc8d3594b76f874375", size = 2064877, upload-time = "2025-11-04T13:41:09.827Z" },
+    { url = "https://files.pythonhosted.org/packages/18/66/e9db17a9a763d72f03de903883c057b2592c09509ccfe468187f2a2eef29/pydantic_core-2.41.5-cp314-cp314-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:2782c870e99878c634505236d81e5443092fba820f0373997ff75f90f68cd553", size = 2180680, upload-time = "2025-11-04T13:41:12.379Z" },
+    { url = "https://files.pythonhosted.org/packages/d3/9e/3ce66cebb929f3ced22be85d4c2399b8e85b622db77dad36b73c5387f8f8/pydantic_core-2.41.5-cp314-cp314-musllinux_1_1_aarch64.whl", hash = "sha256:0177272f88ab8312479336e1d777f6b124537d47f2123f89cb37e0accea97f90", size = 2138960, upload-time = "2025-11-04T13:41:14.627Z" },
+    { url = "https://files.pythonhosted.org/packages/a6/62/205a998f4327d2079326b01abee48e502ea739d174f0a89295c481a2272e/pydantic_core-2.41.5-cp314-cp314-musllinux_1_1_armv7l.whl", hash = "sha256:63510af5e38f8955b8ee5687740d6ebf7c2a0886d15a6d65c32814613681bc07", size = 2339102, upload-time = "2025-11-04T13:41:16.868Z" },
+    { url = "https://files.pythonhosted.org/packages/3c/0d/f05e79471e889d74d3d88f5bd20d0ed189ad94c2423d81ff8d0000aab4ff/pydantic_core-2.41.5-cp314-cp314-musllinux_1_1_x86_64.whl", hash = "sha256:e56ba91f47764cc14f1daacd723e3e82d1a89d783f0f5afe9c364b8bb491ccdb", size = 2326039, upload-time = "2025-11-04T13:41:18.934Z" },
+    { url = "https://files.pythonhosted.org/packages/ec/e1/e08a6208bb100da7e0c4b288eed624a703f4d129bde2da475721a80cab32/pydantic_core-2.41.5-cp314-cp314-win32.whl", hash = "sha256:aec5cf2fd867b4ff45b9959f8b20ea3993fc93e63c7363fe6851424c8a7e7c23", size = 1995126, upload-time = "2025-11-04T13:41:21.418Z" },
+    { url = "https://files.pythonhosted.org/packages/48/5d/56ba7b24e9557f99c9237e29f5c09913c81eeb2f3217e40e922353668092/pydantic_core-2.41.5-cp314-cp314-win_amd64.whl", hash = "sha256:8e7c86f27c585ef37c35e56a96363ab8de4e549a95512445b85c96d3e2f7c1bf", size = 2015489, upload-time = "2025-11-04T13:41:24.076Z" },
+    { url = "https://files.pythonhosted.org/packages/4e/bb/f7a190991ec9e3e0ba22e4993d8755bbc4a32925c0b5b42775c03e8148f9/pydantic_core-2.41.5-cp314-cp314-win_arm64.whl", hash = "sha256:e672ba74fbc2dc8eea59fb6d4aed6845e6905fc2a8afe93175d94a83ba2a01a0", size = 1977288, upload-time = "2025-11-04T13:41:26.33Z" },
+    { url = "https://files.pythonhosted.org/packages/92/ed/77542d0c51538e32e15afe7899d79efce4b81eee631d99850edc2f5e9349/pydantic_core-2.41.5-cp314-cp314t-macosx_10_12_x86_64.whl", hash = "sha256:8566def80554c3faa0e65ac30ab0932b9e3a5cd7f8323764303d468e5c37595a", size = 2120255, upload-time = "2025-11-04T13:41:28.569Z" },
+    { url = "https://files.pythonhosted.org/packages/bb/3d/6913dde84d5be21e284439676168b28d8bbba5600d838b9dca99de0fad71/pydantic_core-2.41.5-cp314-cp314t-macosx_11_0_arm64.whl", hash = "sha256:b80aa5095cd3109962a298ce14110ae16b8c1aece8b72f9dafe81cf597ad80b3", size = 1863760, upload-time = "2025-11-04T13:41:31.055Z" },
+    { url = "https://files.pythonhosted.org/packages/5a/f0/e5e6b99d4191da102f2b0eb9687aaa7f5bea5d9964071a84effc3e40f997/pydantic_core-2.41.5-cp314-cp314t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:3006c3dd9ba34b0c094c544c6006cc79e87d8612999f1a5d43b769b89181f23c", size = 1878092, upload-time = "2025-11-04T13:41:33.21Z" },
+    { url = "https://files.pythonhosted.org/packages/71/48/36fb760642d568925953bcc8116455513d6e34c4beaa37544118c36aba6d/pydantic_core-2.41.5-cp314-cp314t-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:72f6c8b11857a856bcfa48c86f5368439f74453563f951e473514579d44aa612", size = 2053385, upload-time = "2025-11-04T13:41:35.508Z" },
+    { url = "https://files.pythonhosted.org/packages/20/25/92dc684dd8eb75a234bc1c764b4210cf2646479d54b47bf46061657292a8/pydantic_core-2.41.5-cp314-cp314t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:5cb1b2f9742240e4bb26b652a5aeb840aa4b417c7748b6f8387927bc6e45e40d", size = 2218832, upload-time = "2025-11-04T13:41:37.732Z" },
+    { url = "https://files.pythonhosted.org/packages/e2/09/f53e0b05023d3e30357d82eb35835d0f6340ca344720a4599cd663dca599/pydantic_core-2.41.5-cp314-cp314t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:bd3d54f38609ff308209bd43acea66061494157703364ae40c951f83ba99a1a9", size = 2327585, upload-time = "2025-11-04T13:41:40Z" },
+    { url = "https://files.pythonhosted.org/packages/aa/4e/2ae1aa85d6af35a39b236b1b1641de73f5a6ac4d5a7509f77b814885760c/pydantic_core-2.41.5-cp314-cp314t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:2ff4321e56e879ee8d2a879501c8e469414d948f4aba74a2d4593184eb326660", size = 2041078, upload-time = "2025-11-04T13:41:42.323Z" },
+    { url = "https://files.pythonhosted.org/packages/cd/13/2e215f17f0ef326fc72afe94776edb77525142c693767fc347ed6288728d/pydantic_core-2.41.5-cp314-cp314t-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:d0d2568a8c11bf8225044aa94409e21da0cb09dcdafe9ecd10250b2baad531a9", size = 2173914, upload-time = "2025-11-04T13:41:45.221Z" },
+    { url = "https://files.pythonhosted.org/packages/02/7a/f999a6dcbcd0e5660bc348a3991c8915ce6599f4f2c6ac22f01d7a10816c/pydantic_core-2.41.5-cp314-cp314t-musllinux_1_1_aarch64.whl", hash = "sha256:a39455728aabd58ceabb03c90e12f71fd30fa69615760a075b9fec596456ccc3", size = 2129560, upload-time = "2025-11-04T13:41:47.474Z" },
+    { url = "https://files.pythonhosted.org/packages/3a/b1/6c990ac65e3b4c079a4fb9f5b05f5b013afa0f4ed6780a3dd236d2cbdc64/pydantic_core-2.41.5-cp314-cp314t-musllinux_1_1_armv7l.whl", hash = "sha256:239edca560d05757817c13dc17c50766136d21f7cd0fac50295499ae24f90fdf", size = 2329244, upload-time = "2025-11-04T13:41:49.992Z" },
+    { url = "https://files.pythonhosted.org/packages/d9/02/3c562f3a51afd4d88fff8dffb1771b30cfdfd79befd9883ee094f5b6c0d8/pydantic_core-2.41.5-cp314-cp314t-musllinux_1_1_x86_64.whl", hash = "sha256:2a5e06546e19f24c6a96a129142a75cee553cc018ffee48a460059b1185f4470", size = 2331955, upload-time = "2025-11-04T13:41:54.079Z" },
+    { url = "https://files.pythonhosted.org/packages/5c/96/5fb7d8c3c17bc8c62fdb031c47d77a1af698f1d7a406b0f79aaa1338f9ad/pydantic_core-2.41.5-cp314-cp314t-win32.whl", hash = "sha256:b4ececa40ac28afa90871c2cc2b9ffd2ff0bf749380fbdf57d165fd23da353aa", size = 1988906, upload-time = "2025-11-04T13:41:56.606Z" },
+    { url = "https://files.pythonhosted.org/packages/22/ed/182129d83032702912c2e2d8bbe33c036f342cc735737064668585dac28f/pydantic_core-2.41.5-cp314-cp314t-win_amd64.whl", hash = "sha256:80aa89cad80b32a912a65332f64a4450ed00966111b6615ca6816153d3585a8c", size = 1981607, upload-time = "2025-11-04T13:41:58.889Z" },
+    { url = "https://files.pythonhosted.org/packages/9f/ed/068e41660b832bb0b1aa5b58011dea2a3fe0ba7861ff38c4d4904c1c1a99/pydantic_core-2.41.5-cp314-cp314t-win_arm64.whl", hash = "sha256:35b44f37a3199f771c3eaa53051bc8a70cd7b54f333531c59e29fd4db5d15008", size = 1974769, upload-time = "2025-11-04T13:42:01.186Z" },
+    { url = "https://files.pythonhosted.org/packages/11/72/90fda5ee3b97e51c494938a4a44c3a35a9c96c19bba12372fb9c634d6f57/pydantic_core-2.41.5-graalpy311-graalpy242_311_native-macosx_10_12_x86_64.whl", hash = "sha256:b96d5f26b05d03cc60f11a7761a5ded1741da411e7fe0909e27a5e6a0cb7b034", size = 2115441, upload-time = "2025-11-04T13:42:39.557Z" },
+    { url = "https://files.pythonhosted.org/packages/1f/53/8942f884fa33f50794f119012dc6a1a02ac43a56407adaac20463df8e98f/pydantic_core-2.41.5-graalpy311-graalpy242_311_native-macosx_11_0_arm64.whl", hash = "sha256:634e8609e89ceecea15e2d61bc9ac3718caaaa71963717bf3c8f38bfde64242c", size = 1930291, upload-time = "2025-11-04T13:42:42.169Z" },
+    { url = "https://files.pythonhosted.org/packages/79/c8/ecb9ed9cd942bce09fc888ee960b52654fbdbede4ba6c2d6e0d3b1d8b49c/pydantic_core-2.41.5-graalpy311-graalpy242_311_native-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:93e8740d7503eb008aa2df04d3b9735f845d43ae845e6dcd2be0b55a2da43cd2", size = 1948632, upload-time = "2025-11-04T13:42:44.564Z" },
+    { url = "https://files.pythonhosted.org/packages/2e/1b/687711069de7efa6af934e74f601e2a4307365e8fdc404703afc453eab26/pydantic_core-2.41.5-graalpy311-graalpy242_311_native-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:f15489ba13d61f670dcc96772e733aad1a6f9c429cc27574c6cdaed82d0146ad", size = 2138905, upload-time = "2025-11-04T13:42:47.156Z" },
+    { url = "https://files.pythonhosted.org/packages/09/32/59b0c7e63e277fa7911c2fc70ccfb45ce4b98991e7ef37110663437005af/pydantic_core-2.41.5-graalpy312-graalpy250_312_native-macosx_10_12_x86_64.whl", hash = "sha256:7da7087d756b19037bc2c06edc6c170eeef3c3bafcb8f532ff17d64dc427adfd", size = 2110495, upload-time = "2025-11-04T13:42:49.689Z" },
+    { url = "https://files.pythonhosted.org/packages/aa/81/05e400037eaf55ad400bcd318c05bb345b57e708887f07ddb2d20e3f0e98/pydantic_core-2.41.5-graalpy312-graalpy250_312_native-macosx_11_0_arm64.whl", hash = "sha256:aabf5777b5c8ca26f7824cb4a120a740c9588ed58df9b2d196ce92fba42ff8dc", size = 1915388, upload-time = "2025-11-04T13:42:52.215Z" },
+    { url = "https://files.pythonhosted.org/packages/6e/0d/e3549b2399f71d56476b77dbf3cf8937cec5cd70536bdc0e374a421d0599/pydantic_core-2.41.5-graalpy312-graalpy250_312_native-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:c007fe8a43d43b3969e8469004e9845944f1a80e6acd47c150856bb87f230c56", size = 1942879, upload-time = "2025-11-04T13:42:56.483Z" },
+    { url = "https://files.pythonhosted.org/packages/f7/07/34573da085946b6a313d7c42f82f16e8920bfd730665de2d11c0c37a74b5/pydantic_core-2.41.5-graalpy312-graalpy250_312_native-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:76d0819de158cd855d1cbb8fcafdf6f5cf1eb8e470abe056d5d161106e38062b", size = 2139017, upload-time = "2025-11-04T13:42:59.471Z" },
+    { url = "https://files.pythonhosted.org/packages/5f/9b/1b3f0e9f9305839d7e84912f9e8bfbd191ed1b1ef48083609f0dabde978c/pydantic_core-2.41.5-pp311-pypy311_pp73-macosx_10_12_x86_64.whl", hash = "sha256:b2379fa7ed44ddecb5bfe4e48577d752db9fc10be00a6b7446e9663ba143de26", size = 2101980, upload-time = "2025-11-04T13:43:25.97Z" },
+    { url = "https://files.pythonhosted.org/packages/a4/ed/d71fefcb4263df0da6a85b5d8a7508360f2f2e9b3bf5814be9c8bccdccc1/pydantic_core-2.41.5-pp311-pypy311_pp73-macosx_11_0_arm64.whl", hash = "sha256:266fb4cbf5e3cbd0b53669a6d1b039c45e3ce651fd5442eff4d07c2cc8d66808", size = 1923865, upload-time = "2025-11-04T13:43:28.763Z" },
+    { url = "https://files.pythonhosted.org/packages/ce/3a/626b38db460d675f873e4444b4bb030453bbe7b4ba55df821d026a0493c4/pydantic_core-2.41.5-pp311-pypy311_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:58133647260ea01e4d0500089a8c4f07bd7aa6ce109682b1426394988d8aaacc", size = 2134256, upload-time = "2025-11-04T13:43:31.71Z" },
+    { url = "https://files.pythonhosted.org/packages/83/d9/8412d7f06f616bbc053d30cb4e5f76786af3221462ad5eee1f202021eb4e/pydantic_core-2.41.5-pp311-pypy311_pp73-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:287dad91cfb551c363dc62899a80e9e14da1f0e2b6ebde82c806612ca2a13ef1", size = 2174762, upload-time = "2025-11-04T13:43:34.744Z" },
+    { url = "https://files.pythonhosted.org/packages/55/4c/162d906b8e3ba3a99354e20faa1b49a85206c47de97a639510a0e673f5da/pydantic_core-2.41.5-pp311-pypy311_pp73-musllinux_1_1_aarch64.whl", hash = "sha256:03b77d184b9eb40240ae9fd676ca364ce1085f203e1b1256f8ab9984dca80a84", size = 2143141, upload-time = "2025-11-04T13:43:37.701Z" },
+    { url = "https://files.pythonhosted.org/packages/1f/f2/f11dd73284122713f5f89fc940f370d035fa8e1e078d446b3313955157fe/pydantic_core-2.41.5-pp311-pypy311_pp73-musllinux_1_1_armv7l.whl", hash = "sha256:a668ce24de96165bb239160b3d854943128f4334822900534f2fe947930e5770", size = 2330317, upload-time = "2025-11-04T13:43:40.406Z" },
+    { url = "https://files.pythonhosted.org/packages/88/9d/b06ca6acfe4abb296110fb1273a4d848a0bfb2ff65f3ee92127b3244e16b/pydantic_core-2.41.5-pp311-pypy311_pp73-musllinux_1_1_x86_64.whl", hash = "sha256:f14f8f046c14563f8eb3f45f499cc658ab8d10072961e07225e507adb700e93f", size = 2316992, upload-time = "2025-11-04T13:43:43.602Z" },
+    { url = "https://files.pythonhosted.org/packages/36/c7/cfc8e811f061c841d7990b0201912c3556bfeb99cdcb7ed24adc8d6f8704/pydantic_core-2.41.5-pp311-pypy311_pp73-win_amd64.whl", hash = "sha256:56121965f7a4dc965bff783d70b907ddf3d57f6eba29b6d2e5dabfaf07799c51", size = 2145302, upload-time = "2025-11-04T13:43:46.64Z" },
+]
+
+[[package]]
+name = "pydantic-settings"
+version = "2.12.0"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "pydantic" },
+    { name = "python-dotenv" },
+    { name = "typing-inspection" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/43/4b/ac7e0aae12027748076d72a8764ff1c9d82ca75a7a52622e67ed3f765c54/pydantic_settings-2.12.0.tar.gz", hash = "sha256:005538ef951e3c2a68e1c08b292b5f2e71490def8589d4221b95dab00dafcfd0", size = 194184, upload-time = "2025-11-10T14:25:47.013Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/c1/60/5d4751ba3f4a40a6891f24eec885f51afd78d208498268c734e256fb13c4/pydantic_settings-2.12.0-py3-none-any.whl", hash = "sha256:fddb9fd99a5b18da837b29710391e945b1e30c135477f484084ee513adb93809", size = 51880, upload-time = "2025-11-10T14:25:45.546Z" },
+]
+
 [[package]]
 name = "pydeck"
 version = "0.9.1"
@@ -2041,6 +3057,15 @@ wheels = [
     { url = "https://files.pythonhosted.org/packages/ec/57/56b9bcc3c9c6a792fcbaf139543cee77261f3651ca9da0c93f5c1221264b/python_dateutil-2.9.0.post0-py2.py3-none-any.whl", hash = "sha256:a8b2bc7bffae282281c8140a97d3aa9c14da0b136dfe83f850eea9a5f7470427", size = 229892, upload-time = "2024-03-01T18:36:18.57Z" },
 ]
 
+[[package]]
+name = "python-dotenv"
+version = "1.2.1"
+source = { registry = "https://pypi.org/simple" }
+sdist = { url = "https://files.pythonhosted.org/packages/f0/26/19cadc79a718c5edbec86fd4919a6b6d3f681039a2f6d66d14be94e75fb9/python_dotenv-1.2.1.tar.gz", hash = "sha256:42667e897e16ab0d66954af0e60a9caa94f0fd4ecf3aaf6d2d260eec1aa36ad6", size = 44221, upload-time = "2025-10-26T15:12:10.434Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/14/1b/a298b06749107c305e1fe0f814c6c74aea7b2f1e10989cb30f544a1b3253/python_dotenv-1.2.1-py3-none-any.whl", hash = "sha256:b81ee9561e9ca4004139c6cbba3a238c32b03e4894671e181b671e8cb8425d61", size = 21230, upload-time = "2025-10-26T15:12:09.109Z" },
+]
+
 [[package]]
 name = "pytz"
 version = "2025.2"
@@ -2238,6 +3263,18 @@ wheels = [
     { url = "https://files.pythonhosted.org/packages/1e/db/4254e3eabe8020b458f1a747140d32277ec7a271daf1d235b70dc0b4e6e3/requests-2.32.5-py3-none-any.whl", hash = "sha256:2462f94637a34fd532264295e186976db0f5d453d1cdd31473c85a6a161affb6", size = 64738, upload-time = "2025-08-18T20:46:00.542Z" },
 ]
 
+[[package]]
+name = "requests-toolbelt"
+version = "1.0.0"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "requests" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/f3/61/d7545dafb7ac2230c70d38d31cbfe4cc64f7144dc41f6e4e4b78ecd9f5bb/requests-toolbelt-1.0.0.tar.gz", hash = "sha256:7681a0a3d047012b5bdc0ee37d7f8f07ebe76ab08caeccfc3921ce23c88d5bc6", size = 206888, upload-time = "2023-05-01T04:11:33.229Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/3f/51/d4db610ef29373b879047326cbf6fa98b6c1969d6f6dc423279de2b1be2c/requests_toolbelt-1.0.0-py2.py3-none-any.whl", hash = "sha256:cccfdd665f0a24fcf4726e690f65639d272bb0637b9b92dfd91a5568ccf6bd06", size = 54481, upload-time = "2023-05-01T04:11:28.427Z" },
+]
+
 [[package]]
 name = "rich"
 version = "14.2.0"
@@ -2596,6 +3633,48 @@ wheels = [
     { url = "https://files.pythonhosted.org/packages/48/f3/b67d6ea49ca9154453b6d70b34ea22f3996b9fa55da105a79d8732227adc/soupsieve-2.8.1-py3-none-any.whl", hash = "sha256:a11fe2a6f3d76ab3cf2de04eb339c1be5b506a8a47f2ceb6d139803177f85434", size = 36710, upload-time = "2025-12-18T13:50:33.267Z" },
 ]
 
+[[package]]
+name = "sqlalchemy"
+version = "2.0.45"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "greenlet", marker = "platform_machine == 'AMD64' or platform_machine == 'WIN32' or platform_machine == 'aarch64' or platform_machine == 'amd64' or platform_machine == 'ppc64le' or platform_machine == 'win32' or platform_machine == 'x86_64'" },
+    { name = "typing-extensions" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/be/f9/5e4491e5ccf42f5d9cfc663741d261b3e6e1683ae7812114e7636409fcc6/sqlalchemy-2.0.45.tar.gz", hash = "sha256:1632a4bda8d2d25703fdad6363058d882541bdaaee0e5e3ddfa0cd3229efce88", size = 9869912, upload-time = "2025-12-09T21:05:16.737Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/a2/1c/769552a9d840065137272ebe86ffbb0bc92b0f1e0a68ee5266a225f8cd7b/sqlalchemy-2.0.45-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:2e90a344c644a4fa871eb01809c32096487928bd2038bf10f3e4515cb688cc56", size = 2153860, upload-time = "2025-12-10T20:03:23.843Z" },
+    { url = "https://files.pythonhosted.org/packages/f3/f8/9be54ff620e5b796ca7b44670ef58bc678095d51b0e89d6e3102ea468216/sqlalchemy-2.0.45-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:b8c8b41b97fba5f62349aa285654230296829672fc9939cd7f35aab246d1c08b", size = 3309379, upload-time = "2025-12-09T22:06:07.461Z" },
+    { url = "https://files.pythonhosted.org/packages/f6/2b/60ce3ee7a5ae172bfcd419ce23259bb874d2cddd44f67c5df3760a1e22f9/sqlalchemy-2.0.45-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:12c694ed6468333a090d2f60950e4250b928f457e4962389553d6ba5fe9951ac", size = 3309948, upload-time = "2025-12-09T22:09:57.643Z" },
+    { url = "https://files.pythonhosted.org/packages/a3/42/bac8d393f5db550e4e466d03d16daaafd2bad1f74e48c12673fb499a7fc1/sqlalchemy-2.0.45-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:f7d27a1d977a1cfef38a0e2e1ca86f09c4212666ce34e6ae542f3ed0a33bc606", size = 3261239, upload-time = "2025-12-09T22:06:08.879Z" },
+    { url = "https://files.pythonhosted.org/packages/6f/12/43dc70a0528c59842b04ea1c1ed176f072a9b383190eb015384dd102fb19/sqlalchemy-2.0.45-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:d62e47f5d8a50099b17e2bfc1b0c7d7ecd8ba6b46b1507b58cc4f05eefc3bb1c", size = 3284065, upload-time = "2025-12-09T22:09:59.454Z" },
+    { url = "https://files.pythonhosted.org/packages/cf/9c/563049cf761d9a2ec7bc489f7879e9d94e7b590496bea5bbee9ed7b4cc32/sqlalchemy-2.0.45-cp311-cp311-win32.whl", hash = "sha256:3c5f76216e7b85770d5bb5130ddd11ee89f4d52b11783674a662c7dd57018177", size = 2113480, upload-time = "2025-12-09T21:29:57.03Z" },
+    { url = "https://files.pythonhosted.org/packages/bc/fa/09d0a11fe9f15c7fa5c7f0dd26be3d235b0c0cbf2f9544f43bc42efc8a24/sqlalchemy-2.0.45-cp311-cp311-win_amd64.whl", hash = "sha256:a15b98adb7f277316f2c276c090259129ee4afca783495e212048daf846654b2", size = 2138407, upload-time = "2025-12-09T21:29:58.556Z" },
+    { url = "https://files.pythonhosted.org/packages/2d/c7/1900b56ce19bff1c26f39a4ce427faec7716c81ac792bfac8b6a9f3dca93/sqlalchemy-2.0.45-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:b3ee2aac15169fb0d45822983631466d60b762085bc4535cd39e66bea362df5f", size = 3333760, upload-time = "2025-12-09T22:11:02.66Z" },
+    { url = "https://files.pythonhosted.org/packages/0a/93/3be94d96bb442d0d9a60e55a6bb6e0958dd3457751c6f8502e56ef95fed0/sqlalchemy-2.0.45-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:ba547ac0b361ab4f1608afbc8432db669bd0819b3e12e29fb5fa9529a8bba81d", size = 3348268, upload-time = "2025-12-09T22:13:49.054Z" },
+    { url = "https://files.pythonhosted.org/packages/48/4b/f88ded696e61513595e4a9778f9d3f2bf7332cce4eb0c7cedaabddd6687b/sqlalchemy-2.0.45-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:215f0528b914e5c75ef2559f69dca86878a3beeb0c1be7279d77f18e8d180ed4", size = 3278144, upload-time = "2025-12-09T22:11:04.14Z" },
+    { url = "https://files.pythonhosted.org/packages/ed/6a/310ecb5657221f3e1bd5288ed83aa554923fb5da48d760a9f7622afeb065/sqlalchemy-2.0.45-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:107029bf4f43d076d4011f1afb74f7c3e2ea029ec82eb23d8527d5e909e97aa6", size = 3313907, upload-time = "2025-12-09T22:13:50.598Z" },
+    { url = "https://files.pythonhosted.org/packages/5c/39/69c0b4051079addd57c84a5bfb34920d87456dd4c90cf7ee0df6efafc8ff/sqlalchemy-2.0.45-cp312-cp312-win32.whl", hash = "sha256:0c9f6ada57b58420a2c0277ff853abe40b9e9449f8d7d231763c6bc30f5c4953", size = 2112182, upload-time = "2025-12-09T21:39:30.824Z" },
+    { url = "https://files.pythonhosted.org/packages/f7/4e/510db49dd89fc3a6e994bee51848c94c48c4a00dc905e8d0133c251f41a7/sqlalchemy-2.0.45-cp312-cp312-win_amd64.whl", hash = "sha256:8defe5737c6d2179c7997242d6473587c3beb52e557f5ef0187277009f73e5e1", size = 2139200, upload-time = "2025-12-09T21:39:32.321Z" },
+    { url = "https://files.pythonhosted.org/packages/6a/c8/7cc5221b47a54edc72a0140a1efa56e0a2730eefa4058d7ed0b4c4357ff8/sqlalchemy-2.0.45-cp313-cp313-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:fe187fc31a54d7fd90352f34e8c008cf3ad5d064d08fedd3de2e8df83eb4a1cf", size = 3277082, upload-time = "2025-12-09T22:11:06.167Z" },
+    { url = "https://files.pythonhosted.org/packages/0e/50/80a8d080ac7d3d321e5e5d420c9a522b0aa770ec7013ea91f9a8b7d36e4a/sqlalchemy-2.0.45-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:672c45cae53ba88e0dad74b9027dddd09ef6f441e927786b05bec75d949fbb2e", size = 3293131, upload-time = "2025-12-09T22:13:52.626Z" },
+    { url = "https://files.pythonhosted.org/packages/da/4c/13dab31266fc9904f7609a5dc308a2432a066141d65b857760c3bef97e69/sqlalchemy-2.0.45-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:470daea2c1ce73910f08caf10575676a37159a6d16c4da33d0033546bddebc9b", size = 3225389, upload-time = "2025-12-09T22:11:08.093Z" },
+    { url = "https://files.pythonhosted.org/packages/74/04/891b5c2e9f83589de202e7abaf24cd4e4fa59e1837d64d528829ad6cc107/sqlalchemy-2.0.45-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:9c6378449e0940476577047150fd09e242529b761dc887c9808a9a937fe990c8", size = 3266054, upload-time = "2025-12-09T22:13:54.262Z" },
+    { url = "https://files.pythonhosted.org/packages/f1/24/fc59e7f71b0948cdd4cff7a286210e86b0443ef1d18a23b0d83b87e4b1f7/sqlalchemy-2.0.45-cp313-cp313-win32.whl", hash = "sha256:4b6bec67ca45bc166c8729910bd2a87f1c0407ee955df110d78948f5b5827e8a", size = 2110299, upload-time = "2025-12-09T21:39:33.486Z" },
+    { url = "https://files.pythonhosted.org/packages/c0/c5/d17113020b2d43073412aeca09b60d2009442420372123b8d49cc253f8b8/sqlalchemy-2.0.45-cp313-cp313-win_amd64.whl", hash = "sha256:afbf47dc4de31fa38fd491f3705cac5307d21d4bb828a4f020ee59af412744ee", size = 2136264, upload-time = "2025-12-09T21:39:36.801Z" },
+    { url = "https://files.pythonhosted.org/packages/3d/8d/bb40a5d10e7a5f2195f235c0b2f2c79b0bf6e8f00c0c223130a4fbd2db09/sqlalchemy-2.0.45-cp313-cp313t-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:83d7009f40ce619d483d26ac1b757dfe3167b39921379a8bd1b596cf02dab4a6", size = 3521998, upload-time = "2025-12-09T22:13:28.622Z" },
+    { url = "https://files.pythonhosted.org/packages/75/a5/346128b0464886f036c039ea287b7332a410aa2d3fb0bb5d404cb8861635/sqlalchemy-2.0.45-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:d8a2ca754e5415cde2b656c27900b19d50ba076aa05ce66e2207623d3fe41f5a", size = 3473434, upload-time = "2025-12-09T22:13:30.188Z" },
+    { url = "https://files.pythonhosted.org/packages/cc/64/4e1913772646b060b025d3fc52ce91a58967fe58957df32b455de5a12b4f/sqlalchemy-2.0.45-cp314-cp314-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:7f46ec744e7f51275582e6a24326e10c49fbdd3fc99103e01376841213028774", size = 3272404, upload-time = "2025-12-09T22:11:09.662Z" },
+    { url = "https://files.pythonhosted.org/packages/b3/27/caf606ee924282fe4747ee4fd454b335a72a6e018f97eab5ff7f28199e16/sqlalchemy-2.0.45-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:883c600c345123c033c2f6caca18def08f1f7f4c3ebeb591a63b6fceffc95cce", size = 3277057, upload-time = "2025-12-09T22:13:56.213Z" },
+    { url = "https://files.pythonhosted.org/packages/85/d0/3d64218c9724e91f3d1574d12eb7ff8f19f937643815d8daf792046d88ab/sqlalchemy-2.0.45-cp314-cp314-musllinux_1_2_aarch64.whl", hash = "sha256:2c0b74aa79e2deade948fe8593654c8ef4228c44ba862bb7c9585c8e0db90f33", size = 3222279, upload-time = "2025-12-09T22:11:11.1Z" },
+    { url = "https://files.pythonhosted.org/packages/24/10/dd7688a81c5bc7690c2a3764d55a238c524cd1a5a19487928844cb247695/sqlalchemy-2.0.45-cp314-cp314-musllinux_1_2_x86_64.whl", hash = "sha256:8a420169cef179d4c9064365f42d779f1e5895ad26ca0c8b4c0233920973db74", size = 3244508, upload-time = "2025-12-09T22:13:57.932Z" },
+    { url = "https://files.pythonhosted.org/packages/aa/41/db75756ca49f777e029968d9c9fee338c7907c563267740c6d310a8e3f60/sqlalchemy-2.0.45-cp314-cp314-win32.whl", hash = "sha256:e50dcb81a5dfe4b7b4a4aa8f338116d127cb209559124f3694c70d6cd072b68f", size = 2113204, upload-time = "2025-12-09T21:39:38.365Z" },
+    { url = "https://files.pythonhosted.org/packages/89/a2/0e1590e9adb292b1d576dbcf67ff7df8cf55e56e78d2c927686d01080f4b/sqlalchemy-2.0.45-cp314-cp314-win_amd64.whl", hash = "sha256:4748601c8ea959e37e03d13dcda4a44837afcd1b21338e637f7c935b8da06177", size = 2138785, upload-time = "2025-12-09T21:39:39.503Z" },
+    { url = "https://files.pythonhosted.org/packages/42/39/f05f0ed54d451156bbed0e23eb0516bcad7cbb9f18b3bf219c786371b3f0/sqlalchemy-2.0.45-cp314-cp314t-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:cd337d3526ec5298f67d6a30bbbe4ed7e5e68862f0bf6dd21d289f8d37b7d60b", size = 3522029, upload-time = "2025-12-09T22:13:32.09Z" },
+    { url = "https://files.pythonhosted.org/packages/54/0f/d15398b98b65c2bce288d5ee3f7d0a81f77ab89d9456994d5c7cc8b2a9db/sqlalchemy-2.0.45-cp314-cp314t-musllinux_1_2_x86_64.whl", hash = "sha256:9a62b446b7d86a3909abbcd1cd3cc550a832f99c2bc37c5b22e1925438b9367b", size = 3475142, upload-time = "2025-12-09T22:13:33.739Z" },
+    { url = "https://files.pythonhosted.org/packages/bf/e1/3ccb13c643399d22289c6a9786c1a91e3dcbb68bce4beb44926ac2c557bf/sqlalchemy-2.0.45-py3-none-any.whl", hash = "sha256:5225a288e4c8cc2308dbdd874edad6e7d0fd38eac1e9e5f23503425c8eee20d0", size = 1936672, upload-time = "2025-12-09T21:54:52.608Z" },
+]
+
 [[package]]
 name = "sqlglot"
 version = "28.1.0"
@@ -2925,6 +4004,31 @@ wheels = [
     { url = "https://files.pythonhosted.org/packages/18/67/36e9267722cc04a6b9f15c7f3441c2363321a3ea07da7ae0c0707beb2a9c/typing_extensions-4.15.0-py3-none-any.whl", hash = "sha256:f0fa19c6845758ab08074a0cfa8b7aecb71c999ca73d62883bc25cc018c4e548", size = 44614, upload-time = "2025-08-25T13:49:24.86Z" },
 ]
 
+[[package]]
+name = "typing-inspect"
+version = "0.9.0"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "mypy-extensions" },
+    { name = "typing-extensions" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/dc/74/1789779d91f1961fa9438e9a8710cdae6bd138c80d7303996933d117264a/typing_inspect-0.9.0.tar.gz", hash = "sha256:b23fc42ff6f6ef6954e4852c1fb512cdd18dbea03134f91f856a95ccc9461f78", size = 13825, upload-time = "2023-05-24T20:25:47.612Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/65/f3/107a22063bf27bdccf2024833d3445f4eea42b2e598abfbd46f6a63b6cb0/typing_inspect-0.9.0-py3-none-any.whl", hash = "sha256:9ee6fc59062311ef8547596ab6b955e1b8aa46242d854bfc78f4f6b0eff35f9f", size = 8827, upload-time = "2023-05-24T20:25:45.287Z" },
+]
+
+[[package]]
+name = "typing-inspection"
+version = "0.4.2"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "typing-extensions" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/55/e3/70399cb7dd41c10ac53367ae42139cf4b1ca5f36bb3dc6c9d33acdb43655/typing_inspection-0.4.2.tar.gz", hash = "sha256:ba561c48a67c5958007083d386c3295464928b01faa735ab8547c5692e87f464", size = 75949, upload-time = "2025-10-01T02:14:41.687Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/dc/9b/47798a6c91d8bdb567fe2698fe81e0c6b7cb7ef4d13da4114b41d239f65d/typing_inspection-0.4.2-py3-none-any.whl", hash = "sha256:4ed1cacbdc298c220f1bd249ed5287caa16f34d44ef4e9c3d0cbad5b521545e7", size = 14611, upload-time = "2025-10-01T02:14:40.154Z" },
+]
+
 [[package]]
 name = "tzdata"
 version = "2025.2"
@@ -2943,6 +4047,35 @@ wheels = [
     { url = "https://files.pythonhosted.org/packages/56/1a/9ffe814d317c5224166b23e7c47f606d6e473712a2fad0f704ea9b99f246/urllib3-2.6.0-py3-none-any.whl", hash = "sha256:c90f7a39f716c572c4e3e58509581ebd83f9b59cced005b7db7ad2d22b0db99f", size = 131083, upload-time = "2025-12-05T15:08:45.983Z" },
 ]
 
+[[package]]
+name = "uuid-utils"
+version = "0.12.0"
+source = { registry = "https://pypi.org/simple" }
+sdist = { url = "https://files.pythonhosted.org/packages/0b/0e/512fb221e4970c2f75ca9dae412d320b7d9ddc9f2b15e04ea8e44710396c/uuid_utils-0.12.0.tar.gz", hash = "sha256:252bd3d311b5d6b7f5dfce7a5857e27bb4458f222586bb439463231e5a9cbd64", size = 20889, upload-time = "2025-12-01T17:29:55.494Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/8a/43/de5cd49a57b6293b911b6a9a62fc03e55db9f964da7d5882d9edbee1e9d2/uuid_utils-0.12.0-cp39-abi3-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl", hash = "sha256:3b9b30707659292f207b98f294b0e081f6d77e1fbc760ba5b41331a39045f514", size = 603197, upload-time = "2025-12-01T17:29:30.104Z" },
+    { url = "https://files.pythonhosted.org/packages/02/fa/5fd1d8c9234e44f0c223910808cde0de43bb69f7df1349e49b1afa7f2baa/uuid_utils-0.12.0-cp39-abi3-macosx_10_12_x86_64.whl", hash = "sha256:add3d820c7ec14ed37317375bea30249699c5d08ff4ae4dbee9fc9bce3bfbf65", size = 305168, upload-time = "2025-12-01T17:29:31.384Z" },
+    { url = "https://files.pythonhosted.org/packages/c8/c6/8633ac9942bf9dc97a897b5154e5dcffa58816ec4dd780b3b12b559ff05c/uuid_utils-0.12.0-cp39-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:1b8fce83ecb3b16af29c7809669056c4b6e7cc912cab8c6d07361645de12dd79", size = 340580, upload-time = "2025-12-01T17:29:32.362Z" },
+    { url = "https://files.pythonhosted.org/packages/f3/88/8a61307b04b4da1c576373003e6d857a04dade52ab035151d62cb84d5cb5/uuid_utils-0.12.0-cp39-abi3-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:ec921769afcb905035d785582b0791d02304a7850fbd6ce924c1a8976380dfc6", size = 346771, upload-time = "2025-12-01T17:29:33.708Z" },
+    { url = "https://files.pythonhosted.org/packages/1c/fb/aab2dcf94b991e62aa167457c7825b9b01055b884b888af926562864398c/uuid_utils-0.12.0-cp39-abi3-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:6f3b060330f5899a92d5c723547dc6a95adef42433e9748f14c66859a7396664", size = 474781, upload-time = "2025-12-01T17:29:35.237Z" },
+    { url = "https://files.pythonhosted.org/packages/5a/7a/dbd5e49c91d6c86dba57158bbfa0e559e1ddf377bb46dcfd58aea4f0d567/uuid_utils-0.12.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:908dfef7f0bfcf98d406e5dc570c25d2f2473e49b376de41792b6e96c1d5d291", size = 343685, upload-time = "2025-12-01T17:29:36.677Z" },
+    { url = "https://files.pythonhosted.org/packages/1a/19/8c4b1d9f450159733b8be421a4e1fb03533709b80ed3546800102d085572/uuid_utils-0.12.0-cp39-abi3-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:4c6a24148926bd0ca63e8a2dabf4cc9dc329a62325b3ad6578ecd60fbf926506", size = 366482, upload-time = "2025-12-01T17:29:37.979Z" },
+    { url = "https://files.pythonhosted.org/packages/82/43/c79a6e45687647f80a159c8ba34346f287b065452cc419d07d2212d38420/uuid_utils-0.12.0-cp39-abi3-musllinux_1_2_aarch64.whl", hash = "sha256:64a91e632669f059ef605f1771d28490b1d310c26198e46f754e8846dddf12f4", size = 523132, upload-time = "2025-12-01T17:29:39.293Z" },
+    { url = "https://files.pythonhosted.org/packages/5a/a2/b2d75a621260a40c438aa88593827dfea596d18316520a99e839f7a5fb9d/uuid_utils-0.12.0-cp39-abi3-musllinux_1_2_armv7l.whl", hash = "sha256:93c082212470bb4603ca3975916c205a9d7ef1443c0acde8fbd1e0f5b36673c7", size = 614218, upload-time = "2025-12-01T17:29:40.315Z" },
+    { url = "https://files.pythonhosted.org/packages/13/6b/ba071101626edd5a6dabf8525c9a1537ff3d885dbc210540574a03901fef/uuid_utils-0.12.0-cp39-abi3-musllinux_1_2_i686.whl", hash = "sha256:431b1fb7283ba974811b22abd365f2726f8f821ab33f0f715be389640e18d039", size = 546241, upload-time = "2025-12-01T17:29:41.656Z" },
+    { url = "https://files.pythonhosted.org/packages/01/12/9a942b81c0923268e6d85bf98d8f0a61fcbcd5e432fef94fdf4ce2ef8748/uuid_utils-0.12.0-cp39-abi3-musllinux_1_2_x86_64.whl", hash = "sha256:2ffd7838c40149100299fa37cbd8bab5ee382372e8e65a148002a37d380df7c8", size = 511842, upload-time = "2025-12-01T17:29:43.107Z" },
+    { url = "https://files.pythonhosted.org/packages/a9/a7/c326f5163dd48b79368b87d8a05f5da4668dd228a3f5ca9d79d5fee2fc40/uuid_utils-0.12.0-cp39-abi3-win32.whl", hash = "sha256:487f17c0fee6cbc1d8b90fe811874174a9b1b5683bf2251549e302906a50fed3", size = 179088, upload-time = "2025-12-01T17:29:44.492Z" },
+    { url = "https://files.pythonhosted.org/packages/38/92/41c8734dd97213ee1d5ae435cf4499705dc4f2751e3b957fd12376f61784/uuid_utils-0.12.0-cp39-abi3-win_amd64.whl", hash = "sha256:9598e7c9da40357ae8fffc5d6938b1a7017f09a1acbcc95e14af8c65d48c655a", size = 183003, upload-time = "2025-12-01T17:29:45.47Z" },
+    { url = "https://files.pythonhosted.org/packages/c9/f9/52ab0359618987331a1f739af837d26168a4b16281c9c3ab46519940c628/uuid_utils-0.12.0-cp39-abi3-win_arm64.whl", hash = "sha256:c9bea7c5b2aa6f57937ebebeee4d4ef2baad10f86f1b97b58a3f6f34c14b4e84", size = 182975, upload-time = "2025-12-01T17:29:46.444Z" },
+    { url = "https://files.pythonhosted.org/packages/ef/f7/6c55b7722cede3b424df02ed5cddb25c19543abda2f95fa4cfc34a892ae5/uuid_utils-0.12.0-pp311-pypy311_pp73-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl", hash = "sha256:e2209d361f2996966ab7114f49919eb6aaeabc6041672abbbbf4fdbb8ec1acc0", size = 593065, upload-time = "2025-12-01T17:29:47.507Z" },
+    { url = "https://files.pythonhosted.org/packages/b8/40/ce5fe8e9137dbd5570e0016c2584fca43ad81b11a1cef809a1a1b4952ab7/uuid_utils-0.12.0-pp311-pypy311_pp73-macosx_10_12_x86_64.whl", hash = "sha256:d9636bcdbd6cfcad2b549c352b669412d0d1eb09be72044a2f13e498974863cd", size = 300047, upload-time = "2025-12-01T17:29:48.596Z" },
+    { url = "https://files.pythonhosted.org/packages/fb/9b/31c5d0736d7b118f302c50214e581f40e904305d8872eb0f0c921d50e138/uuid_utils-0.12.0-pp311-pypy311_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:8cd8543a3419251fb78e703ce3b15fdfafe1b7c542cf40caf0775e01db7e7674", size = 335165, upload-time = "2025-12-01T17:29:49.755Z" },
+    { url = "https://files.pythonhosted.org/packages/f6/5c/d80b4d08691c9d7446d0ad58fd41503081a662cfd2c7640faf68c64d8098/uuid_utils-0.12.0-pp311-pypy311_pp73-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:e98db2d8977c052cb307ae1cb5cc37a21715e8d415dbc65863b039397495a013", size = 341437, upload-time = "2025-12-01T17:29:51.112Z" },
+    { url = "https://files.pythonhosted.org/packages/f6/b3/9dccdc6f3c22f6ef5bd381ae559173f8a1ae185ae89ed1f39f499d9d8b02/uuid_utils-0.12.0-pp311-pypy311_pp73-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:f8f2bdf5e4ffeb259ef6d15edae92aed60a1d6f07cbfab465d836f6b12b48da8", size = 469123, upload-time = "2025-12-01T17:29:52.389Z" },
+    { url = "https://files.pythonhosted.org/packages/fd/90/6c35ef65fbc49f8189729839b793a4a74a7dd8c5aa5eb56caa93f8c97732/uuid_utils-0.12.0-pp311-pypy311_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:8c3ec53c0cb15e1835870c139317cc5ec06e35aa22843e3ed7d9c74f23f23898", size = 335892, upload-time = "2025-12-01T17:29:53.44Z" },
+    { url = "https://files.pythonhosted.org/packages/6b/c7/e3f3ce05c5af2bf86a0938d22165affe635f4dcbfd5687b1dacc042d3e0e/uuid_utils-0.12.0-pp311-pypy311_pp73-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:84e5c0eba209356f7f389946a3a47b2cc2effd711b3fc7c7f155ad9f7d45e8a3", size = 360693, upload-time = "2025-12-01T17:29:54.558Z" },
+]
+
 [[package]]
 name = "watchdog"
 version = "6.0.0"
@@ -3050,3 +4183,290 @@ wheels = [
     { url = "https://files.pythonhosted.org/packages/41/99/8a06b8e17dddbf321325ae4eb12465804120f699cd1b8a355718300c62da/wrapt-2.0.1-cp314-cp314t-win_arm64.whl", hash = "sha256:35cdbd478607036fee40273be8ed54a451f5f23121bd9d4be515158f9498f7ad", size = 60634, upload-time = "2025-11-07T00:45:02.087Z" },
     { url = "https://files.pythonhosted.org/packages/15/d1/b51471c11592ff9c012bd3e2f7334a6ff2f42a7aed2caffcf0bdddc9cb89/wrapt-2.0.1-py3-none-any.whl", hash = "sha256:4d2ce1bf1a48c5277d7969259232b57645aae5686dba1eaeade39442277afbca", size = 44046, upload-time = "2025-11-07T00:45:32.116Z" },
 ]
+
+[[package]]
+name = "xxhash"
+version = "3.6.0"
+source = { registry = "https://pypi.org/simple" }
+sdist = { url = "https://files.pythonhosted.org/packages/02/84/30869e01909fb37a6cc7e18688ee8bf1e42d57e7e0777636bd47524c43c7/xxhash-3.6.0.tar.gz", hash = "sha256:f0162a78b13a0d7617b2845b90c763339d1f1d82bb04a4b07f4ab535cc5e05d6", size = 85160, upload-time = "2025-10-02T14:37:08.097Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/17/d4/cc2f0400e9154df4b9964249da78ebd72f318e35ccc425e9f403c392f22a/xxhash-3.6.0-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:b47bbd8cf2d72797f3c2772eaaac0ded3d3af26481a26d7d7d41dc2d3c46b04a", size = 32844, upload-time = "2025-10-02T14:34:14.037Z" },
+    { url = "https://files.pythonhosted.org/packages/5e/ec/1cc11cd13e26ea8bc3cb4af4eaadd8d46d5014aebb67be3f71fb0b68802a/xxhash-3.6.0-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:2b6821e94346f96db75abaa6e255706fb06ebd530899ed76d32cd99f20dc52fa", size = 30809, upload-time = "2025-10-02T14:34:15.484Z" },
+    { url = "https://files.pythonhosted.org/packages/04/5f/19fe357ea348d98ca22f456f75a30ac0916b51c753e1f8b2e0e6fb884cce/xxhash-3.6.0-cp311-cp311-manylinux1_i686.manylinux_2_28_i686.manylinux_2_5_i686.whl", hash = "sha256:d0a9751f71a1a65ce3584e9cae4467651c7e70c9d31017fa57574583a4540248", size = 194665, upload-time = "2025-10-02T14:34:16.541Z" },
+    { url = "https://files.pythonhosted.org/packages/90/3b/d1f1a8f5442a5fd8beedae110c5af7604dc37349a8e16519c13c19a9a2de/xxhash-3.6.0-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:8b29ee68625ab37b04c0b40c3fafdf24d2f75ccd778333cfb698f65f6c463f62", size = 213550, upload-time = "2025-10-02T14:34:17.878Z" },
+    { url = "https://files.pythonhosted.org/packages/c4/ef/3a9b05eb527457d5db13a135a2ae1a26c80fecd624d20f3e8dcc4cb170f3/xxhash-3.6.0-cp311-cp311-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:6812c25fe0d6c36a46ccb002f40f27ac903bf18af9f6dd8f9669cb4d176ab18f", size = 212384, upload-time = "2025-10-02T14:34:19.182Z" },
+    { url = "https://files.pythonhosted.org/packages/0f/18/ccc194ee698c6c623acbf0f8c2969811a8a4b6185af5e824cd27b9e4fd3e/xxhash-3.6.0-cp311-cp311-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:4ccbff013972390b51a18ef1255ef5ac125c92dc9143b2d1909f59abc765540e", size = 445749, upload-time = "2025-10-02T14:34:20.659Z" },
+    { url = "https://files.pythonhosted.org/packages/a5/86/cf2c0321dc3940a7aa73076f4fd677a0fb3e405cb297ead7d864fd90847e/xxhash-3.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:297b7fbf86c82c550e12e8fb71968b3f033d27b874276ba3624ea868c11165a8", size = 193880, upload-time = "2025-10-02T14:34:22.431Z" },
+    { url = "https://files.pythonhosted.org/packages/82/fb/96213c8560e6f948a1ecc9a7613f8032b19ee45f747f4fca4eb31bb6d6ed/xxhash-3.6.0-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:dea26ae1eb293db089798d3973a5fc928a18fdd97cc8801226fae705b02b14b0", size = 210912, upload-time = "2025-10-02T14:34:23.937Z" },
+    { url = "https://files.pythonhosted.org/packages/40/aa/4395e669b0606a096d6788f40dbdf2b819d6773aa290c19e6e83cbfc312f/xxhash-3.6.0-cp311-cp311-musllinux_1_2_i686.whl", hash = "sha256:7a0b169aafb98f4284f73635a8e93f0735f9cbde17bd5ec332480484241aaa77", size = 198654, upload-time = "2025-10-02T14:34:25.644Z" },
+    { url = "https://files.pythonhosted.org/packages/67/74/b044fcd6b3d89e9b1b665924d85d3f400636c23590226feb1eb09e1176ce/xxhash-3.6.0-cp311-cp311-musllinux_1_2_ppc64le.whl", hash = "sha256:08d45aef063a4531b785cd72de4887766d01dc8f362a515693df349fdb825e0c", size = 210867, upload-time = "2025-10-02T14:34:27.203Z" },
+    { url = "https://files.pythonhosted.org/packages/bc/fd/3ce73bf753b08cb19daee1eb14aa0d7fe331f8da9c02dd95316ddfe5275e/xxhash-3.6.0-cp311-cp311-musllinux_1_2_s390x.whl", hash = "sha256:929142361a48ee07f09121fe9e96a84950e8d4df3bb298ca5d88061969f34d7b", size = 414012, upload-time = "2025-10-02T14:34:28.409Z" },
+    { url = "https://files.pythonhosted.org/packages/ba/b3/5a4241309217c5c876f156b10778f3ab3af7ba7e3259e6d5f5c7d0129eb2/xxhash-3.6.0-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:51312c768403d8540487dbbfb557454cfc55589bbde6424456951f7fcd4facb3", size = 191409, upload-time = "2025-10-02T14:34:29.696Z" },
+    { url = "https://files.pythonhosted.org/packages/c0/01/99bfbc15fb9abb9a72b088c1d95219fc4782b7d01fc835bd5744d66dd0b8/xxhash-3.6.0-cp311-cp311-win32.whl", hash = "sha256:d1927a69feddc24c987b337ce81ac15c4720955b667fe9b588e02254b80446fd", size = 30574, upload-time = "2025-10-02T14:34:31.028Z" },
+    { url = "https://files.pythonhosted.org/packages/65/79/9d24d7f53819fe301b231044ea362ce64e86c74f6e8c8e51320de248b3e5/xxhash-3.6.0-cp311-cp311-win_amd64.whl", hash = "sha256:26734cdc2d4ffe449b41d186bbeac416f704a482ed835d375a5c0cb02bc63fef", size = 31481, upload-time = "2025-10-02T14:34:32.062Z" },
+    { url = "https://files.pythonhosted.org/packages/30/4e/15cd0e3e8772071344eab2961ce83f6e485111fed8beb491a3f1ce100270/xxhash-3.6.0-cp311-cp311-win_arm64.whl", hash = "sha256:d72f67ef8bf36e05f5b6c65e8524f265bd61071471cd4cf1d36743ebeeeb06b7", size = 27861, upload-time = "2025-10-02T14:34:33.555Z" },
+    { url = "https://files.pythonhosted.org/packages/9a/07/d9412f3d7d462347e4511181dea65e47e0d0e16e26fbee2ea86a2aefb657/xxhash-3.6.0-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:01362c4331775398e7bb34e3ab403bc9ee9f7c497bc7dee6272114055277dd3c", size = 32744, upload-time = "2025-10-02T14:34:34.622Z" },
+    { url = "https://files.pythonhosted.org/packages/79/35/0429ee11d035fc33abe32dca1b2b69e8c18d236547b9a9b72c1929189b9a/xxhash-3.6.0-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:b7b2df81a23f8cb99656378e72501b2cb41b1827c0f5a86f87d6b06b69f9f204", size = 30816, upload-time = "2025-10-02T14:34:36.043Z" },
+    { url = "https://files.pythonhosted.org/packages/b7/f2/57eb99aa0f7d98624c0932c5b9a170e1806406cdbcdb510546634a1359e0/xxhash-3.6.0-cp312-cp312-manylinux1_i686.manylinux_2_28_i686.manylinux_2_5_i686.whl", hash = "sha256:dc94790144e66b14f67b10ac8ed75b39ca47536bf8800eb7c24b50271ea0c490", size = 194035, upload-time = "2025-10-02T14:34:37.354Z" },
+    { url = "https://files.pythonhosted.org/packages/4c/ed/6224ba353690d73af7a3f1c7cdb1fc1b002e38f783cb991ae338e1eb3d79/xxhash-3.6.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:93f107c673bccf0d592cdba077dedaf52fe7f42dcd7676eba1f6d6f0c3efffd2", size = 212914, upload-time = "2025-10-02T14:34:38.6Z" },
+    { url = "https://files.pythonhosted.org/packages/38/86/fb6b6130d8dd6b8942cc17ab4d90e223653a89aa32ad2776f8af7064ed13/xxhash-3.6.0-cp312-cp312-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:2aa5ee3444c25b69813663c9f8067dcfaa2e126dc55e8dddf40f4d1c25d7effa", size = 212163, upload-time = "2025-10-02T14:34:39.872Z" },
+    { url = "https://files.pythonhosted.org/packages/ee/dc/e84875682b0593e884ad73b2d40767b5790d417bde603cceb6878901d647/xxhash-3.6.0-cp312-cp312-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:f7f99123f0e1194fa59cc69ad46dbae2e07becec5df50a0509a808f90a0f03f0", size = 445411, upload-time = "2025-10-02T14:34:41.569Z" },
+    { url = "https://files.pythonhosted.org/packages/11/4f/426f91b96701ec2f37bb2b8cec664eff4f658a11f3fa9d94f0a887ea6d2b/xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:49e03e6fe2cac4a1bc64952dd250cf0dbc5ef4ebb7b8d96bce82e2de163c82a2", size = 193883, upload-time = "2025-10-02T14:34:43.249Z" },
+    { url = "https://files.pythonhosted.org/packages/53/5a/ddbb83eee8e28b778eacfc5a85c969673e4023cdeedcfcef61f36731610b/xxhash-3.6.0-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:bd17fede52a17a4f9a7bc4472a5867cb0b160deeb431795c0e4abe158bc784e9", size = 210392, upload-time = "2025-10-02T14:34:45.042Z" },
+    { url = "https://files.pythonhosted.org/packages/1e/c2/ff69efd07c8c074ccdf0a4f36fcdd3d27363665bcdf4ba399abebe643465/xxhash-3.6.0-cp312-cp312-musllinux_1_2_i686.whl", hash = "sha256:6fb5f5476bef678f69db04f2bd1efbed3030d2aba305b0fc1773645f187d6a4e", size = 197898, upload-time = "2025-10-02T14:34:46.302Z" },
+    { url = "https://files.pythonhosted.org/packages/58/ca/faa05ac19b3b622c7c9317ac3e23954187516298a091eb02c976d0d3dd45/xxhash-3.6.0-cp312-cp312-musllinux_1_2_ppc64le.whl", hash = "sha256:843b52f6d88071f87eba1631b684fcb4b2068cd2180a0224122fe4ef011a9374", size = 210655, upload-time = "2025-10-02T14:34:47.571Z" },
+    { url = "https://files.pythonhosted.org/packages/d4/7a/06aa7482345480cc0cb597f5c875b11a82c3953f534394f620b0be2f700c/xxhash-3.6.0-cp312-cp312-musllinux_1_2_s390x.whl", hash = "sha256:7d14a6cfaf03b1b6f5f9790f76880601ccc7896aff7ab9cd8978a939c1eb7e0d", size = 414001, upload-time = "2025-10-02T14:34:49.273Z" },
+    { url = "https://files.pythonhosted.org/packages/23/07/63ffb386cd47029aa2916b3d2f454e6cc5b9f5c5ada3790377d5430084e7/xxhash-3.6.0-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:418daf3db71e1413cfe211c2f9a528456936645c17f46b5204705581a45390ae", size = 191431, upload-time = "2025-10-02T14:34:50.798Z" },
+    { url = "https://files.pythonhosted.org/packages/0f/93/14fde614cadb4ddf5e7cebf8918b7e8fac5ae7861c1875964f17e678205c/xxhash-3.6.0-cp312-cp312-win32.whl", hash = "sha256:50fc255f39428a27299c20e280d6193d8b63b8ef8028995323bf834a026b4fbb", size = 30617, upload-time = "2025-10-02T14:34:51.954Z" },
+    { url = "https://files.pythonhosted.org/packages/13/5d/0d125536cbe7565a83d06e43783389ecae0c0f2ed037b48ede185de477c0/xxhash-3.6.0-cp312-cp312-win_amd64.whl", hash = "sha256:c0f2ab8c715630565ab8991b536ecded9416d615538be8ecddce43ccf26cbc7c", size = 31534, upload-time = "2025-10-02T14:34:53.276Z" },
+    { url = "https://files.pythonhosted.org/packages/54/85/6ec269b0952ec7e36ba019125982cf11d91256a778c7c3f98a4c5043d283/xxhash-3.6.0-cp312-cp312-win_arm64.whl", hash = "sha256:eae5c13f3bc455a3bbb68bdc513912dc7356de7e2280363ea235f71f54064829", size = 27876, upload-time = "2025-10-02T14:34:54.371Z" },
+    { url = "https://files.pythonhosted.org/packages/33/76/35d05267ac82f53ae9b0e554da7c5e281ee61f3cad44c743f0fcd354f211/xxhash-3.6.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:599e64ba7f67472481ceb6ee80fa3bd828fd61ba59fb11475572cc5ee52b89ec", size = 32738, upload-time = "2025-10-02T14:34:55.839Z" },
+    { url = "https://files.pythonhosted.org/packages/31/a8/3fbce1cd96534a95e35d5120637bf29b0d7f5d8fa2f6374e31b4156dd419/xxhash-3.6.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:7d8b8aaa30fca4f16f0c84a5c8d7ddee0e25250ec2796c973775373257dde8f1", size = 30821, upload-time = "2025-10-02T14:34:57.219Z" },
+    { url = "https://files.pythonhosted.org/packages/0c/ea/d387530ca7ecfa183cb358027f1833297c6ac6098223fd14f9782cd0015c/xxhash-3.6.0-cp313-cp313-manylinux1_i686.manylinux_2_28_i686.manylinux_2_5_i686.whl", hash = "sha256:d597acf8506d6e7101a4a44a5e428977a51c0fadbbfd3c39650cca9253f6e5a6", size = 194127, upload-time = "2025-10-02T14:34:59.21Z" },
+    { url = "https://files.pythonhosted.org/packages/ba/0c/71435dcb99874b09a43b8d7c54071e600a7481e42b3e3ce1eb5226a5711a/xxhash-3.6.0-cp313-cp313-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:858dc935963a33bc33490128edc1c12b0c14d9c7ebaa4e387a7869ecc4f3e263", size = 212975, upload-time = "2025-10-02T14:35:00.816Z" },
+    { url = "https://files.pythonhosted.org/packages/84/7a/c2b3d071e4bb4a90b7057228a99b10d51744878f4a8a6dd643c8bd897620/xxhash-3.6.0-cp313-cp313-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:ba284920194615cb8edf73bf52236ce2e1664ccd4a38fdb543506413529cc546", size = 212241, upload-time = "2025-10-02T14:35:02.207Z" },
+    { url = "https://files.pythonhosted.org/packages/81/5f/640b6eac0128e215f177df99eadcd0f1b7c42c274ab6a394a05059694c5a/xxhash-3.6.0-cp313-cp313-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:4b54219177f6c6674d5378bd862c6aedf64725f70dd29c472eaae154df1a2e89", size = 445471, upload-time = "2025-10-02T14:35:03.61Z" },
+    { url = "https://files.pythonhosted.org/packages/5e/1e/3c3d3ef071b051cc3abbe3721ffb8365033a172613c04af2da89d5548a87/xxhash-3.6.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:42c36dd7dbad2f5238950c377fcbf6811b1cdb1c444fab447960030cea60504d", size = 193936, upload-time = "2025-10-02T14:35:05.013Z" },
+    { url = "https://files.pythonhosted.org/packages/2c/bd/4a5f68381939219abfe1c22a9e3a5854a4f6f6f3c4983a87d255f21f2e5d/xxhash-3.6.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:f22927652cba98c44639ffdc7aaf35828dccf679b10b31c4ad72a5b530a18eb7", size = 210440, upload-time = "2025-10-02T14:35:06.239Z" },
+    { url = "https://files.pythonhosted.org/packages/eb/37/b80fe3d5cfb9faff01a02121a0f4d565eb7237e9e5fc66e73017e74dcd36/xxhash-3.6.0-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:b45fad44d9c5c119e9c6fbf2e1c656a46dc68e280275007bbfd3d572b21426db", size = 197990, upload-time = "2025-10-02T14:35:07.735Z" },
+    { url = "https://files.pythonhosted.org/packages/d7/fd/2c0a00c97b9e18f72e1f240ad4e8f8a90fd9d408289ba9c7c495ed7dc05c/xxhash-3.6.0-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:6f2580ffab1a8b68ef2b901cde7e55fa8da5e4be0977c68f78fc80f3c143de42", size = 210689, upload-time = "2025-10-02T14:35:09.438Z" },
+    { url = "https://files.pythonhosted.org/packages/93/86/5dd8076a926b9a95db3206aba20d89a7fc14dd5aac16e5c4de4b56033140/xxhash-3.6.0-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:40c391dd3cd041ebc3ffe6f2c862f402e306eb571422e0aa918d8070ba31da11", size = 414068, upload-time = "2025-10-02T14:35:11.162Z" },
+    { url = "https://files.pythonhosted.org/packages/af/3c/0bb129170ee8f3650f08e993baee550a09593462a5cddd8e44d0011102b1/xxhash-3.6.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:f205badabde7aafd1a31e8ca2a3e5a763107a71c397c4481d6a804eb5063d8bd", size = 191495, upload-time = "2025-10-02T14:35:12.971Z" },
+    { url = "https://files.pythonhosted.org/packages/e9/3a/6797e0114c21d1725e2577508e24006fd7ff1d8c0c502d3b52e45c1771d8/xxhash-3.6.0-cp313-cp313-win32.whl", hash = "sha256:2577b276e060b73b73a53042ea5bd5203d3e6347ce0d09f98500f418a9fcf799", size = 30620, upload-time = "2025-10-02T14:35:14.129Z" },
+    { url = "https://files.pythonhosted.org/packages/86/15/9bc32671e9a38b413a76d24722a2bf8784a132c043063a8f5152d390b0f9/xxhash-3.6.0-cp313-cp313-win_amd64.whl", hash = "sha256:757320d45d2fbcce8f30c42a6b2f47862967aea7bf458b9625b4bbe7ee390392", size = 31542, upload-time = "2025-10-02T14:35:15.21Z" },
+    { url = "https://files.pythonhosted.org/packages/39/c5/cc01e4f6188656e56112d6a8e0dfe298a16934b8c47a247236549a3f7695/xxhash-3.6.0-cp313-cp313-win_arm64.whl", hash = "sha256:457b8f85dec5825eed7b69c11ae86834a018b8e3df5e77783c999663da2f96d6", size = 27880, upload-time = "2025-10-02T14:35:16.315Z" },
+    { url = "https://files.pythonhosted.org/packages/f3/30/25e5321c8732759e930c555176d37e24ab84365482d257c3b16362235212/xxhash-3.6.0-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:a42e633d75cdad6d625434e3468126c73f13f7584545a9cf34e883aa1710e702", size = 32956, upload-time = "2025-10-02T14:35:17.413Z" },
+    { url = "https://files.pythonhosted.org/packages/9f/3c/0573299560d7d9f8ab1838f1efc021a280b5ae5ae2e849034ef3dee18810/xxhash-3.6.0-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:568a6d743219e717b07b4e03b0a828ce593833e498c3b64752e0f5df6bfe84db", size = 31072, upload-time = "2025-10-02T14:35:18.844Z" },
+    { url = "https://files.pythonhosted.org/packages/7a/1c/52d83a06e417cd9d4137722693424885cc9878249beb3a7c829e74bf7ce9/xxhash-3.6.0-cp313-cp313t-manylinux1_i686.manylinux_2_28_i686.manylinux_2_5_i686.whl", hash = "sha256:bec91b562d8012dae276af8025a55811b875baace6af510412a5e58e3121bc54", size = 196409, upload-time = "2025-10-02T14:35:20.31Z" },
+    { url = "https://files.pythonhosted.org/packages/e3/8e/c6d158d12a79bbd0b878f8355432075fc82759e356ab5a111463422a239b/xxhash-3.6.0-cp313-cp313t-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:78e7f2f4c521c30ad5e786fdd6bae89d47a32672a80195467b5de0480aa97b1f", size = 215736, upload-time = "2025-10-02T14:35:21.616Z" },
+    { url = "https://files.pythonhosted.org/packages/bc/68/c4c80614716345d55071a396cf03d06e34b5f4917a467faf43083c995155/xxhash-3.6.0-cp313-cp313t-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:3ed0df1b11a79856df5ffcab572cbd6b9627034c1c748c5566fa79df9048a7c5", size = 214833, upload-time = "2025-10-02T14:35:23.32Z" },
+    { url = "https://files.pythonhosted.org/packages/7e/e9/ae27c8ffec8b953efa84c7c4a6c6802c263d587b9fc0d6e7cea64e08c3af/xxhash-3.6.0-cp313-cp313t-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:0e4edbfc7d420925b0dd5e792478ed393d6e75ff8fc219a6546fb446b6a417b1", size = 448348, upload-time = "2025-10-02T14:35:25.111Z" },
+    { url = "https://files.pythonhosted.org/packages/d7/6b/33e21afb1b5b3f46b74b6bd1913639066af218d704cc0941404ca717fc57/xxhash-3.6.0-cp313-cp313t-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:fba27a198363a7ef87f8c0f6b171ec36b674fe9053742c58dd7e3201c1ab30ee", size = 196070, upload-time = "2025-10-02T14:35:26.586Z" },
+    { url = "https://files.pythonhosted.org/packages/96/b6/fcabd337bc5fa624e7203aa0fa7d0c49eed22f72e93229431752bddc83d9/xxhash-3.6.0-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:794fe9145fe60191c6532fa95063765529770edcdd67b3d537793e8004cabbfd", size = 212907, upload-time = "2025-10-02T14:35:28.087Z" },
+    { url = "https://files.pythonhosted.org/packages/4b/d3/9ee6160e644d660fcf176c5825e61411c7f62648728f69c79ba237250143/xxhash-3.6.0-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:6105ef7e62b5ac73a837778efc331a591d8442f8ef5c7e102376506cb4ae2729", size = 200839, upload-time = "2025-10-02T14:35:29.857Z" },
+    { url = "https://files.pythonhosted.org/packages/0d/98/e8de5baa5109394baf5118f5e72ab21a86387c4f89b0e77ef3e2f6b0327b/xxhash-3.6.0-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:f01375c0e55395b814a679b3eea205db7919ac2af213f4a6682e01220e5fe292", size = 213304, upload-time = "2025-10-02T14:35:31.222Z" },
+    { url = "https://files.pythonhosted.org/packages/7b/1d/71056535dec5c3177eeb53e38e3d367dd1d16e024e63b1cee208d572a033/xxhash-3.6.0-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:d706dca2d24d834a4661619dcacf51a75c16d65985718d6a7d73c1eeeb903ddf", size = 416930, upload-time = "2025-10-02T14:35:32.517Z" },
+    { url = "https://files.pythonhosted.org/packages/dc/6c/5cbde9de2cd967c322e651c65c543700b19e7ae3e0aae8ece3469bf9683d/xxhash-3.6.0-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:5f059d9faeacd49c0215d66f4056e1326c80503f51a1532ca336a385edadd033", size = 193787, upload-time = "2025-10-02T14:35:33.827Z" },
+    { url = "https://files.pythonhosted.org/packages/19/fa/0172e350361d61febcea941b0cc541d6e6c8d65d153e85f850a7b256ff8a/xxhash-3.6.0-cp313-cp313t-win32.whl", hash = "sha256:1244460adc3a9be84731d72b8e80625788e5815b68da3da8b83f78115a40a7ec", size = 30916, upload-time = "2025-10-02T14:35:35.107Z" },
+    { url = "https://files.pythonhosted.org/packages/ad/e6/e8cf858a2b19d6d45820f072eff1bea413910592ff17157cabc5f1227a16/xxhash-3.6.0-cp313-cp313t-win_amd64.whl", hash = "sha256:b1e420ef35c503869c4064f4a2f2b08ad6431ab7b229a05cce39d74268bca6b8", size = 31799, upload-time = "2025-10-02T14:35:36.165Z" },
+    { url = "https://files.pythonhosted.org/packages/56/15/064b197e855bfb7b343210e82490ae672f8bc7cdf3ddb02e92f64304ee8a/xxhash-3.6.0-cp313-cp313t-win_arm64.whl", hash = "sha256:ec44b73a4220623235f67a996c862049f375df3b1052d9899f40a6382c32d746", size = 28044, upload-time = "2025-10-02T14:35:37.195Z" },
+    { url = "https://files.pythonhosted.org/packages/7e/5e/0138bc4484ea9b897864d59fce9be9086030825bc778b76cb5a33a906d37/xxhash-3.6.0-cp314-cp314-macosx_10_13_x86_64.whl", hash = "sha256:a40a3d35b204b7cc7643cbcf8c9976d818cb47befcfac8bbefec8038ac363f3e", size = 32754, upload-time = "2025-10-02T14:35:38.245Z" },
+    { url = "https://files.pythonhosted.org/packages/18/d7/5dac2eb2ec75fd771957a13e5dda560efb2176d5203f39502a5fc571f899/xxhash-3.6.0-cp314-cp314-macosx_11_0_arm64.whl", hash = "sha256:a54844be970d3fc22630b32d515e79a90d0a3ddb2644d8d7402e3c4c8da61405", size = 30846, upload-time = "2025-10-02T14:35:39.6Z" },
+    { url = "https://files.pythonhosted.org/packages/fe/71/8bc5be2bb00deb5682e92e8da955ebe5fa982da13a69da5a40a4c8db12fb/xxhash-3.6.0-cp314-cp314-manylinux1_i686.manylinux_2_28_i686.manylinux_2_5_i686.whl", hash = "sha256:016e9190af8f0a4e3741343777710e3d5717427f175adfdc3e72508f59e2a7f3", size = 194343, upload-time = "2025-10-02T14:35:40.69Z" },
+    { url = "https://files.pythonhosted.org/packages/e7/3b/52badfb2aecec2c377ddf1ae75f55db3ba2d321c5e164f14461c90837ef3/xxhash-3.6.0-cp314-cp314-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:4f6f72232f849eb9d0141e2ebe2677ece15adfd0fa599bc058aad83c714bb2c6", size = 213074, upload-time = "2025-10-02T14:35:42.29Z" },
+    { url = "https://files.pythonhosted.org/packages/a2/2b/ae46b4e9b92e537fa30d03dbc19cdae57ed407e9c26d163895e968e3de85/xxhash-3.6.0-cp314-cp314-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:63275a8aba7865e44b1813d2177e0f5ea7eadad3dd063a21f7cf9afdc7054063", size = 212388, upload-time = "2025-10-02T14:35:43.929Z" },
+    { url = "https://files.pythonhosted.org/packages/f5/80/49f88d3afc724b4ac7fbd664c8452d6db51b49915be48c6982659e0e7942/xxhash-3.6.0-cp314-cp314-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:3cd01fa2aa00d8b017c97eb46b9a794fbdca53fc14f845f5a328c71254b0abb7", size = 445614, upload-time = "2025-10-02T14:35:45.216Z" },
+    { url = "https://files.pythonhosted.org/packages/ed/ba/603ce3961e339413543d8cd44f21f2c80e2a7c5cfe692a7b1f2cccf58f3c/xxhash-3.6.0-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:0226aa89035b62b6a86d3c68df4d7c1f47a342b8683da2b60cedcddb46c4d95b", size = 194024, upload-time = "2025-10-02T14:35:46.959Z" },
+    { url = "https://files.pythonhosted.org/packages/78/d1/8e225ff7113bf81545cfdcd79eef124a7b7064a0bba53605ff39590b95c2/xxhash-3.6.0-cp314-cp314-musllinux_1_2_aarch64.whl", hash = "sha256:c6e193e9f56e4ca4923c61238cdaced324f0feac782544eb4c6d55ad5cc99ddd", size = 210541, upload-time = "2025-10-02T14:35:48.301Z" },
+    { url = "https://files.pythonhosted.org/packages/6f/58/0f89d149f0bad89def1a8dd38feb50ccdeb643d9797ec84707091d4cb494/xxhash-3.6.0-cp314-cp314-musllinux_1_2_i686.whl", hash = "sha256:9176dcaddf4ca963d4deb93866d739a343c01c969231dbe21680e13a5d1a5bf0", size = 198305, upload-time = "2025-10-02T14:35:49.584Z" },
+    { url = "https://files.pythonhosted.org/packages/11/38/5eab81580703c4df93feb5f32ff8fa7fe1e2c51c1f183ee4e48d4bb9d3d7/xxhash-3.6.0-cp314-cp314-musllinux_1_2_ppc64le.whl", hash = "sha256:c1ce4009c97a752e682b897aa99aef84191077a9433eb237774689f14f8ec152", size = 210848, upload-time = "2025-10-02T14:35:50.877Z" },
+    { url = "https://files.pythonhosted.org/packages/5e/6b/953dc4b05c3ce678abca756416e4c130d2382f877a9c30a20d08ee6a77c0/xxhash-3.6.0-cp314-cp314-musllinux_1_2_s390x.whl", hash = "sha256:8cb2f4f679b01513b7adbb9b1b2f0f9cdc31b70007eaf9d59d0878809f385b11", size = 414142, upload-time = "2025-10-02T14:35:52.15Z" },
+    { url = "https://files.pythonhosted.org/packages/08/a9/238ec0d4e81a10eb5026d4a6972677cbc898ba6c8b9dbaec12ae001b1b35/xxhash-3.6.0-cp314-cp314-musllinux_1_2_x86_64.whl", hash = "sha256:653a91d7c2ab54a92c19ccf43508b6a555440b9be1bc8be553376778be7f20b5", size = 191547, upload-time = "2025-10-02T14:35:53.547Z" },
+    { url = "https://files.pythonhosted.org/packages/f1/ee/3cf8589e06c2164ac77c3bf0aa127012801128f1feebf2a079272da5737c/xxhash-3.6.0-cp314-cp314-win32.whl", hash = "sha256:a756fe893389483ee8c394d06b5ab765d96e68fbbfe6fde7aa17e11f5720559f", size = 31214, upload-time = "2025-10-02T14:35:54.746Z" },
+    { url = "https://files.pythonhosted.org/packages/02/5d/a19552fbc6ad4cb54ff953c3908bbc095f4a921bc569433d791f755186f1/xxhash-3.6.0-cp314-cp314-win_amd64.whl", hash = "sha256:39be8e4e142550ef69629c9cd71b88c90e9a5db703fecbcf265546d9536ca4ad", size = 32290, upload-time = "2025-10-02T14:35:55.791Z" },
+    { url = "https://files.pythonhosted.org/packages/b1/11/dafa0643bc30442c887b55baf8e73353a344ee89c1901b5a5c54a6c17d39/xxhash-3.6.0-cp314-cp314-win_arm64.whl", hash = "sha256:25915e6000338999236f1eb68a02a32c3275ac338628a7eaa5a269c401995679", size = 28795, upload-time = "2025-10-02T14:35:57.162Z" },
+    { url = "https://files.pythonhosted.org/packages/2c/db/0e99732ed7f64182aef4a6fb145e1a295558deec2a746265dcdec12d191e/xxhash-3.6.0-cp314-cp314t-macosx_10_13_x86_64.whl", hash = "sha256:c5294f596a9017ca5a3e3f8884c00b91ab2ad2933cf288f4923c3fd4346cf3d4", size = 32955, upload-time = "2025-10-02T14:35:58.267Z" },
+    { url = "https://files.pythonhosted.org/packages/55/f4/2a7c3c68e564a099becfa44bb3d398810cc0ff6749b0d3cb8ccb93f23c14/xxhash-3.6.0-cp314-cp314t-macosx_11_0_arm64.whl", hash = "sha256:1cf9dcc4ab9cff01dfbba78544297a3a01dafd60f3bde4e2bfd016cf7e4ddc67", size = 31072, upload-time = "2025-10-02T14:35:59.382Z" },
+    { url = "https://files.pythonhosted.org/packages/c6/d9/72a29cddc7250e8a5819dad5d466facb5dc4c802ce120645630149127e73/xxhash-3.6.0-cp314-cp314t-manylinux1_i686.manylinux_2_28_i686.manylinux_2_5_i686.whl", hash = "sha256:01262da8798422d0685f7cef03b2bd3f4f46511b02830861df548d7def4402ad", size = 196579, upload-time = "2025-10-02T14:36:00.838Z" },
+    { url = "https://files.pythonhosted.org/packages/63/93/b21590e1e381040e2ca305a884d89e1c345b347404f7780f07f2cdd47ef4/xxhash-3.6.0-cp314-cp314t-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:51a73fb7cb3a3ead9f7a8b583ffd9b8038e277cdb8cb87cf890e88b3456afa0b", size = 215854, upload-time = "2025-10-02T14:36:02.207Z" },
+    { url = "https://files.pythonhosted.org/packages/ce/b8/edab8a7d4fa14e924b29be877d54155dcbd8b80be85ea00d2be3413a9ed4/xxhash-3.6.0-cp314-cp314t-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:b9c6df83594f7df8f7f708ce5ebeacfc69f72c9fbaaababf6cf4758eaada0c9b", size = 214965, upload-time = "2025-10-02T14:36:03.507Z" },
+    { url = "https://files.pythonhosted.org/packages/27/67/dfa980ac7f0d509d54ea0d5a486d2bb4b80c3f1bb22b66e6a05d3efaf6c0/xxhash-3.6.0-cp314-cp314t-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:627f0af069b0ea56f312fd5189001c24578868643203bca1abbc2c52d3a6f3ca", size = 448484, upload-time = "2025-10-02T14:36:04.828Z" },
+    { url = "https://files.pythonhosted.org/packages/8c/63/8ffc2cc97e811c0ca5d00ab36604b3ea6f4254f20b7bc658ca825ce6c954/xxhash-3.6.0-cp314-cp314t-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:aa912c62f842dfd013c5f21a642c9c10cd9f4c4e943e0af83618b4a404d9091a", size = 196162, upload-time = "2025-10-02T14:36:06.182Z" },
+    { url = "https://files.pythonhosted.org/packages/4b/77/07f0e7a3edd11a6097e990f6e5b815b6592459cb16dae990d967693e6ea9/xxhash-3.6.0-cp314-cp314t-musllinux_1_2_aarch64.whl", hash = "sha256:b465afd7909db30168ab62afe40b2fcf79eedc0b89a6c0ab3123515dc0df8b99", size = 213007, upload-time = "2025-10-02T14:36:07.733Z" },
+    { url = "https://files.pythonhosted.org/packages/ae/d8/bc5fa0d152837117eb0bef6f83f956c509332ce133c91c63ce07ee7c4873/xxhash-3.6.0-cp314-cp314t-musllinux_1_2_i686.whl", hash = "sha256:a881851cf38b0a70e7c4d3ce81fc7afd86fbc2a024f4cfb2a97cf49ce04b75d3", size = 200956, upload-time = "2025-10-02T14:36:09.106Z" },
+    { url = "https://files.pythonhosted.org/packages/26/a5/d749334130de9411783873e9b98ecc46688dad5db64ca6e04b02acc8b473/xxhash-3.6.0-cp314-cp314t-musllinux_1_2_ppc64le.whl", hash = "sha256:9b3222c686a919a0f3253cfc12bb118b8b103506612253b5baeaac10d8027cf6", size = 213401, upload-time = "2025-10-02T14:36:10.585Z" },
+    { url = "https://files.pythonhosted.org/packages/89/72/abed959c956a4bfc72b58c0384bb7940663c678127538634d896b1195c10/xxhash-3.6.0-cp314-cp314t-musllinux_1_2_s390x.whl", hash = "sha256:c5aa639bc113e9286137cec8fadc20e9cd732b2cc385c0b7fa673b84fc1f2a93", size = 417083, upload-time = "2025-10-02T14:36:12.276Z" },
+    { url = "https://files.pythonhosted.org/packages/0c/b3/62fd2b586283b7d7d665fb98e266decadf31f058f1cf6c478741f68af0cb/xxhash-3.6.0-cp314-cp314t-musllinux_1_2_x86_64.whl", hash = "sha256:5c1343d49ac102799905e115aee590183c3921d475356cb24b4de29a4bc56518", size = 193913, upload-time = "2025-10-02T14:36:14.025Z" },
+    { url = "https://files.pythonhosted.org/packages/9a/9a/c19c42c5b3f5a4aad748a6d5b4f23df3bed7ee5445accc65a0fb3ff03953/xxhash-3.6.0-cp314-cp314t-win32.whl", hash = "sha256:5851f033c3030dd95c086b4a36a2683c2ff4a799b23af60977188b057e467119", size = 31586, upload-time = "2025-10-02T14:36:15.603Z" },
+    { url = "https://files.pythonhosted.org/packages/03/d6/4cc450345be9924fd5dc8c590ceda1db5b43a0a889587b0ae81a95511360/xxhash-3.6.0-cp314-cp314t-win_amd64.whl", hash = "sha256:0444e7967dac37569052d2409b00a8860c2135cff05502df4da80267d384849f", size = 32526, upload-time = "2025-10-02T14:36:16.708Z" },
+    { url = "https://files.pythonhosted.org/packages/0f/c9/7243eb3f9eaabd1a88a5a5acadf06df2d83b100c62684b7425c6a11bcaa8/xxhash-3.6.0-cp314-cp314t-win_arm64.whl", hash = "sha256:bb79b1e63f6fd84ec778a4b1916dfe0a7c3fdb986c06addd5db3a0d413819d95", size = 28898, upload-time = "2025-10-02T14:36:17.843Z" },
+    { url = "https://files.pythonhosted.org/packages/93/1e/8aec23647a34a249f62e2398c42955acd9b4c6ed5cf08cbea94dc46f78d2/xxhash-3.6.0-pp311-pypy311_pp73-macosx_10_15_x86_64.whl", hash = "sha256:0f7b7e2ec26c1666ad5fc9dbfa426a6a3367ceaf79db5dd76264659d509d73b0", size = 30662, upload-time = "2025-10-02T14:37:01.743Z" },
+    { url = "https://files.pythonhosted.org/packages/b8/0b/b14510b38ba91caf43006209db846a696ceea6a847a0c9ba0a5b1adc53d6/xxhash-3.6.0-pp311-pypy311_pp73-manylinux1_i686.manylinux_2_28_i686.manylinux_2_5_i686.whl", hash = "sha256:5dc1e14d14fa0f5789ec29a7062004b5933964bb9b02aae6622b8f530dc40296", size = 41056, upload-time = "2025-10-02T14:37:02.879Z" },
+    { url = "https://files.pythonhosted.org/packages/50/55/15a7b8a56590e66ccd374bbfa3f9ffc45b810886c8c3b614e3f90bd2367c/xxhash-3.6.0-pp311-pypy311_pp73-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:881b47fc47e051b37d94d13e7455131054b56749b91b508b0907eb07900d1c13", size = 36251, upload-time = "2025-10-02T14:37:04.44Z" },
+    { url = "https://files.pythonhosted.org/packages/62/b2/5ac99a041a29e58e95f907876b04f7067a0242cb85b5f39e726153981503/xxhash-3.6.0-pp311-pypy311_pp73-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:c6dc31591899f5e5666f04cc2e529e69b4072827085c1ef15294d91a004bc1bd", size = 32481, upload-time = "2025-10-02T14:37:05.869Z" },
+    { url = "https://files.pythonhosted.org/packages/7b/d9/8d95e906764a386a3d3b596f3c68bb63687dfca806373509f51ce8eea81f/xxhash-3.6.0-pp311-pypy311_pp73-win_amd64.whl", hash = "sha256:15e0dac10eb9309508bfc41f7f9deaa7755c69e35af835db9cb10751adebc35d", size = 31565, upload-time = "2025-10-02T14:37:06.966Z" },
+]
+
+[[package]]
+name = "yarl"
+version = "1.22.0"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "idna" },
+    { name = "multidict" },
+    { name = "propcache" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/57/63/0c6ebca57330cd313f6102b16dd57ffaf3ec4c83403dcb45dbd15c6f3ea1/yarl-1.22.0.tar.gz", hash = "sha256:bebf8557577d4401ba8bd9ff33906f1376c877aa78d1fe216ad01b4d6745af71", size = 187169, upload-time = "2025-10-06T14:12:55.963Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/4d/27/5ab13fc84c76a0250afd3d26d5936349a35be56ce5785447d6c423b26d92/yarl-1.22.0-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:1ab72135b1f2db3fed3997d7e7dc1b80573c67138023852b6efb336a5eae6511", size = 141607, upload-time = "2025-10-06T14:09:16.298Z" },
+    { url = "https://files.pythonhosted.org/packages/6a/a1/d065d51d02dc02ce81501d476b9ed2229d9a990818332242a882d5d60340/yarl-1.22.0-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:669930400e375570189492dc8d8341301578e8493aec04aebc20d4717f899dd6", size = 94027, upload-time = "2025-10-06T14:09:17.786Z" },
+    { url = "https://files.pythonhosted.org/packages/c1/da/8da9f6a53f67b5106ffe902c6fa0164e10398d4e150d85838b82f424072a/yarl-1.22.0-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:792a2af6d58177ef7c19cbf0097aba92ca1b9cb3ffdd9c7470e156c8f9b5e028", size = 94963, upload-time = "2025-10-06T14:09:19.662Z" },
+    { url = "https://files.pythonhosted.org/packages/68/fe/2c1f674960c376e29cb0bec1249b117d11738db92a6ccc4a530b972648db/yarl-1.22.0-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:3ea66b1c11c9150f1372f69afb6b8116f2dd7286f38e14ea71a44eee9ec51b9d", size = 368406, upload-time = "2025-10-06T14:09:21.402Z" },
+    { url = "https://files.pythonhosted.org/packages/95/26/812a540e1c3c6418fec60e9bbd38e871eaba9545e94fa5eff8f4a8e28e1e/yarl-1.22.0-cp311-cp311-manylinux2014_armv7l.manylinux_2_17_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:3e2daa88dc91870215961e96a039ec73e4937da13cf77ce17f9cad0c18df3503", size = 336581, upload-time = "2025-10-06T14:09:22.98Z" },
+    { url = "https://files.pythonhosted.org/packages/0b/f5/5777b19e26fdf98563985e481f8be3d8a39f8734147a6ebf459d0dab5a6b/yarl-1.22.0-cp311-cp311-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:ba440ae430c00eee41509353628600212112cd5018d5def7e9b05ea7ac34eb65", size = 388924, upload-time = "2025-10-06T14:09:24.655Z" },
+    { url = "https://files.pythonhosted.org/packages/86/08/24bd2477bd59c0bbd994fe1d93b126e0472e4e3df5a96a277b0a55309e89/yarl-1.22.0-cp311-cp311-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:e6438cc8f23a9c1478633d216b16104a586b9761db62bfacb6425bac0a36679e", size = 392890, upload-time = "2025-10-06T14:09:26.617Z" },
+    { url = "https://files.pythonhosted.org/packages/46/00/71b90ed48e895667ecfb1eaab27c1523ee2fa217433ed77a73b13205ca4b/yarl-1.22.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:4c52a6e78aef5cf47a98ef8e934755abf53953379b7d53e68b15ff4420e6683d", size = 365819, upload-time = "2025-10-06T14:09:28.544Z" },
+    { url = "https://files.pythonhosted.org/packages/30/2d/f715501cae832651d3282387c6a9236cd26bd00d0ff1e404b3dc52447884/yarl-1.22.0-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:3b06bcadaac49c70f4c88af4ffcfbe3dc155aab3163e75777818092478bcbbe7", size = 363601, upload-time = "2025-10-06T14:09:30.568Z" },
+    { url = "https://files.pythonhosted.org/packages/f8/f9/a678c992d78e394e7126ee0b0e4e71bd2775e4334d00a9278c06a6cce96a/yarl-1.22.0-cp311-cp311-musllinux_1_2_armv7l.whl", hash = "sha256:6944b2dc72c4d7f7052683487e3677456050ff77fcf5e6204e98caf785ad1967", size = 358072, upload-time = "2025-10-06T14:09:32.528Z" },
+    { url = "https://files.pythonhosted.org/packages/2c/d1/b49454411a60edb6fefdcad4f8e6dbba7d8019e3a508a1c5836cba6d0781/yarl-1.22.0-cp311-cp311-musllinux_1_2_ppc64le.whl", hash = "sha256:d5372ca1df0f91a86b047d1277c2aaf1edb32d78bbcefffc81b40ffd18f027ed", size = 385311, upload-time = "2025-10-06T14:09:34.634Z" },
+    { url = "https://files.pythonhosted.org/packages/87/e5/40d7a94debb8448c7771a916d1861d6609dddf7958dc381117e7ba36d9e8/yarl-1.22.0-cp311-cp311-musllinux_1_2_s390x.whl", hash = "sha256:51af598701f5299012b8416486b40fceef8c26fc87dc6d7d1f6fc30609ea0aa6", size = 381094, upload-time = "2025-10-06T14:09:36.268Z" },
+    { url = "https://files.pythonhosted.org/packages/35/d8/611cc282502381ad855448643e1ad0538957fc82ae83dfe7762c14069e14/yarl-1.22.0-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:b266bd01fedeffeeac01a79ae181719ff848a5a13ce10075adbefc8f1daee70e", size = 370944, upload-time = "2025-10-06T14:09:37.872Z" },
+    { url = "https://files.pythonhosted.org/packages/2d/df/fadd00fb1c90e1a5a8bd731fa3d3de2e165e5a3666a095b04e31b04d9cb6/yarl-1.22.0-cp311-cp311-win32.whl", hash = "sha256:a9b1ba5610a4e20f655258d5a1fdc7ebe3d837bb0e45b581398b99eb98b1f5ca", size = 81804, upload-time = "2025-10-06T14:09:39.359Z" },
+    { url = "https://files.pythonhosted.org/packages/b5/f7/149bb6f45f267cb5c074ac40c01c6b3ea6d8a620d34b337f6321928a1b4d/yarl-1.22.0-cp311-cp311-win_amd64.whl", hash = "sha256:078278b9b0b11568937d9509b589ee83ef98ed6d561dfe2020e24a9fd08eaa2b", size = 86858, upload-time = "2025-10-06T14:09:41.068Z" },
+    { url = "https://files.pythonhosted.org/packages/2b/13/88b78b93ad3f2f0b78e13bfaaa24d11cbc746e93fe76d8c06bf139615646/yarl-1.22.0-cp311-cp311-win_arm64.whl", hash = "sha256:b6a6f620cfe13ccec221fa312139135166e47ae169f8253f72a0abc0dae94376", size = 81637, upload-time = "2025-10-06T14:09:42.712Z" },
+    { url = "https://files.pythonhosted.org/packages/75/ff/46736024fee3429b80a165a732e38e5d5a238721e634ab41b040d49f8738/yarl-1.22.0-cp312-cp312-macosx_10_13_universal2.whl", hash = "sha256:e340382d1afa5d32b892b3ff062436d592ec3d692aeea3bef3a5cfe11bbf8c6f", size = 142000, upload-time = "2025-10-06T14:09:44.631Z" },
+    { url = "https://files.pythonhosted.org/packages/5a/9a/b312ed670df903145598914770eb12de1bac44599549b3360acc96878df8/yarl-1.22.0-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:f1e09112a2c31ffe8d80be1b0988fa6a18c5d5cad92a9ffbb1c04c91bfe52ad2", size = 94338, upload-time = "2025-10-06T14:09:46.372Z" },
+    { url = "https://files.pythonhosted.org/packages/ba/f5/0601483296f09c3c65e303d60c070a5c19fcdbc72daa061e96170785bc7d/yarl-1.22.0-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:939fe60db294c786f6b7c2d2e121576628468f65453d86b0fe36cb52f987bd74", size = 94909, upload-time = "2025-10-06T14:09:48.648Z" },
+    { url = "https://files.pythonhosted.org/packages/60/41/9a1fe0b73dbcefce72e46cf149b0e0a67612d60bfc90fb59c2b2efdfbd86/yarl-1.22.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:e1651bf8e0398574646744c1885a41198eba53dc8a9312b954073f845c90a8df", size = 372940, upload-time = "2025-10-06T14:09:50.089Z" },
+    { url = "https://files.pythonhosted.org/packages/17/7a/795cb6dfee561961c30b800f0ed616b923a2ec6258b5def2a00bf8231334/yarl-1.22.0-cp312-cp312-manylinux2014_armv7l.manylinux_2_17_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:b8a0588521a26bf92a57a1705b77b8b59044cdceccac7151bd8d229e66b8dedb", size = 345825, upload-time = "2025-10-06T14:09:52.142Z" },
+    { url = "https://files.pythonhosted.org/packages/d7/93/a58f4d596d2be2ae7bab1a5846c4d270b894958845753b2c606d666744d3/yarl-1.22.0-cp312-cp312-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:42188e6a615c1a75bcaa6e150c3fe8f3e8680471a6b10150c5f7e83f47cc34d2", size = 386705, upload-time = "2025-10-06T14:09:54.128Z" },
+    { url = "https://files.pythonhosted.org/packages/61/92/682279d0e099d0e14d7fd2e176bd04f48de1484f56546a3e1313cd6c8e7c/yarl-1.22.0-cp312-cp312-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:f6d2cb59377d99718913ad9a151030d6f83ef420a2b8f521d94609ecc106ee82", size = 396518, upload-time = "2025-10-06T14:09:55.762Z" },
+    { url = "https://files.pythonhosted.org/packages/db/0f/0d52c98b8a885aeda831224b78f3be7ec2e1aa4a62091f9f9188c3c65b56/yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:50678a3b71c751d58d7908edc96d332af328839eea883bb554a43f539101277a", size = 377267, upload-time = "2025-10-06T14:09:57.958Z" },
+    { url = "https://files.pythonhosted.org/packages/22/42/d2685e35908cbeaa6532c1fc73e89e7f2efb5d8a7df3959ea8e37177c5a3/yarl-1.22.0-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:1e8fbaa7cec507aa24ea27a01456e8dd4b6fab829059b69844bd348f2d467124", size = 365797, upload-time = "2025-10-06T14:09:59.527Z" },
+    { url = "https://files.pythonhosted.org/packages/a2/83/cf8c7bcc6355631762f7d8bdab920ad09b82efa6b722999dfb05afa6cfac/yarl-1.22.0-cp312-cp312-musllinux_1_2_armv7l.whl", hash = "sha256:433885ab5431bc3d3d4f2f9bd15bfa1614c522b0f1405d62c4f926ccd69d04fa", size = 365535, upload-time = "2025-10-06T14:10:01.139Z" },
+    { url = "https://files.pythonhosted.org/packages/25/e1/5302ff9b28f0c59cac913b91fe3f16c59a033887e57ce9ca5d41a3a94737/yarl-1.22.0-cp312-cp312-musllinux_1_2_ppc64le.whl", hash = "sha256:b790b39c7e9a4192dc2e201a282109ed2985a1ddbd5ac08dc56d0e121400a8f7", size = 382324, upload-time = "2025-10-06T14:10:02.756Z" },
+    { url = "https://files.pythonhosted.org/packages/bf/cd/4617eb60f032f19ae3a688dc990d8f0d89ee0ea378b61cac81ede3e52fae/yarl-1.22.0-cp312-cp312-musllinux_1_2_s390x.whl", hash = "sha256:31f0b53913220599446872d757257be5898019c85e7971599065bc55065dc99d", size = 383803, upload-time = "2025-10-06T14:10:04.552Z" },
+    { url = "https://files.pythonhosted.org/packages/59/65/afc6e62bb506a319ea67b694551dab4a7e6fb7bf604e9bd9f3e11d575fec/yarl-1.22.0-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:a49370e8f711daec68d09b821a34e1167792ee2d24d405cbc2387be4f158b520", size = 374220, upload-time = "2025-10-06T14:10:06.489Z" },
+    { url = "https://files.pythonhosted.org/packages/e7/3d/68bf18d50dc674b942daec86a9ba922d3113d8399b0e52b9897530442da2/yarl-1.22.0-cp312-cp312-win32.whl", hash = "sha256:70dfd4f241c04bd9239d53b17f11e6ab672b9f1420364af63e8531198e3f5fe8", size = 81589, upload-time = "2025-10-06T14:10:09.254Z" },
+    { url = "https://files.pythonhosted.org/packages/c8/9a/6ad1a9b37c2f72874f93e691b2e7ecb6137fb2b899983125db4204e47575/yarl-1.22.0-cp312-cp312-win_amd64.whl", hash = "sha256:8884d8b332a5e9b88e23f60bb166890009429391864c685e17bd73a9eda9105c", size = 87213, upload-time = "2025-10-06T14:10:11.369Z" },
+    { url = "https://files.pythonhosted.org/packages/44/c5/c21b562d1680a77634d748e30c653c3ca918beb35555cff24986fff54598/yarl-1.22.0-cp312-cp312-win_arm64.whl", hash = "sha256:ea70f61a47f3cc93bdf8b2f368ed359ef02a01ca6393916bc8ff877427181e74", size = 81330, upload-time = "2025-10-06T14:10:13.112Z" },
+    { url = "https://files.pythonhosted.org/packages/ea/f3/d67de7260456ee105dc1d162d43a019ecad6b91e2f51809d6cddaa56690e/yarl-1.22.0-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:8dee9c25c74997f6a750cd317b8ca63545169c098faee42c84aa5e506c819b53", size = 139980, upload-time = "2025-10-06T14:10:14.601Z" },
+    { url = "https://files.pythonhosted.org/packages/01/88/04d98af0b47e0ef42597b9b28863b9060bb515524da0a65d5f4db160b2d5/yarl-1.22.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:01e73b85a5434f89fc4fe27dcda2aff08ddf35e4d47bbbea3bdcd25321af538a", size = 93424, upload-time = "2025-10-06T14:10:16.115Z" },
+    { url = "https://files.pythonhosted.org/packages/18/91/3274b215fd8442a03975ce6bee5fe6aa57a8326b29b9d3d56234a1dca244/yarl-1.22.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:22965c2af250d20c873cdbee8ff958fb809940aeb2e74ba5f20aaf6b7ac8c70c", size = 93821, upload-time = "2025-10-06T14:10:17.993Z" },
+    { url = "https://files.pythonhosted.org/packages/61/3a/caf4e25036db0f2da4ca22a353dfeb3c9d3c95d2761ebe9b14df8fc16eb0/yarl-1.22.0-cp313-cp313-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:b4f15793aa49793ec8d1c708ab7f9eded1aa72edc5174cae703651555ed1b601", size = 373243, upload-time = "2025-10-06T14:10:19.44Z" },
+    { url = "https://files.pythonhosted.org/packages/6e/9e/51a77ac7516e8e7803b06e01f74e78649c24ee1021eca3d6a739cb6ea49c/yarl-1.22.0-cp313-cp313-manylinux2014_armv7l.manylinux_2_17_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:e5542339dcf2747135c5c85f68680353d5cb9ffd741c0f2e8d832d054d41f35a", size = 342361, upload-time = "2025-10-06T14:10:21.124Z" },
+    { url = "https://files.pythonhosted.org/packages/d4/f8/33b92454789dde8407f156c00303e9a891f1f51a0330b0fad7c909f87692/yarl-1.22.0-cp313-cp313-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:5c401e05ad47a75869c3ab3e35137f8468b846770587e70d71e11de797d113df", size = 387036, upload-time = "2025-10-06T14:10:22.902Z" },
+    { url = "https://files.pythonhosted.org/packages/d9/9a/c5db84ea024f76838220280f732970aa4ee154015d7f5c1bfb60a267af6f/yarl-1.22.0-cp313-cp313-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:243dda95d901c733f5b59214d28b0120893d91777cb8aa043e6ef059d3cddfe2", size = 397671, upload-time = "2025-10-06T14:10:24.523Z" },
+    { url = "https://files.pythonhosted.org/packages/11/c9/cd8538dc2e7727095e0c1d867bad1e40c98f37763e6d995c1939f5fdc7b1/yarl-1.22.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:bec03d0d388060058f5d291a813f21c011041938a441c593374da6077fe21b1b", size = 377059, upload-time = "2025-10-06T14:10:26.406Z" },
+    { url = "https://files.pythonhosted.org/packages/a1/b9/ab437b261702ced75122ed78a876a6dec0a1b0f5e17a4ac7a9a2482d8abe/yarl-1.22.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:b0748275abb8c1e1e09301ee3cf90c8a99678a4e92e4373705f2a2570d581273", size = 365356, upload-time = "2025-10-06T14:10:28.461Z" },
+    { url = "https://files.pythonhosted.org/packages/b2/9d/8e1ae6d1d008a9567877b08f0ce4077a29974c04c062dabdb923ed98e6fe/yarl-1.22.0-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:47fdb18187e2a4e18fda2c25c05d8251a9e4a521edaed757fef033e7d8498d9a", size = 361331, upload-time = "2025-10-06T14:10:30.541Z" },
+    { url = "https://files.pythonhosted.org/packages/ca/5a/09b7be3905962f145b73beb468cdd53db8aa171cf18c80400a54c5b82846/yarl-1.22.0-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:c7044802eec4524fde550afc28edda0dd5784c4c45f0be151a2d3ba017daca7d", size = 382590, upload-time = "2025-10-06T14:10:33.352Z" },
+    { url = "https://files.pythonhosted.org/packages/aa/7f/59ec509abf90eda5048b0bc3e2d7b5099dffdb3e6b127019895ab9d5ef44/yarl-1.22.0-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:139718f35149ff544caba20fce6e8a2f71f1e39b92c700d8438a0b1d2a631a02", size = 385316, upload-time = "2025-10-06T14:10:35.034Z" },
+    { url = "https://files.pythonhosted.org/packages/e5/84/891158426bc8036bfdfd862fabd0e0fa25df4176ec793e447f4b85cf1be4/yarl-1.22.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:e1b51bebd221006d3d2f95fbe124b22b247136647ae5dcc8c7acafba66e5ee67", size = 374431, upload-time = "2025-10-06T14:10:37.76Z" },
+    { url = "https://files.pythonhosted.org/packages/bb/49/03da1580665baa8bef5e8ed34c6df2c2aca0a2f28bf397ed238cc1bbc6f2/yarl-1.22.0-cp313-cp313-win32.whl", hash = "sha256:d3e32536234a95f513bd374e93d717cf6b2231a791758de6c509e3653f234c95", size = 81555, upload-time = "2025-10-06T14:10:39.649Z" },
+    { url = "https://files.pythonhosted.org/packages/9a/ee/450914ae11b419eadd067c6183ae08381cfdfcb9798b90b2b713bbebddda/yarl-1.22.0-cp313-cp313-win_amd64.whl", hash = "sha256:47743b82b76d89a1d20b83e60d5c20314cbd5ba2befc9cda8f28300c4a08ed4d", size = 86965, upload-time = "2025-10-06T14:10:41.313Z" },
+    { url = "https://files.pythonhosted.org/packages/98/4d/264a01eae03b6cf629ad69bae94e3b0e5344741e929073678e84bf7a3e3b/yarl-1.22.0-cp313-cp313-win_arm64.whl", hash = "sha256:5d0fcda9608875f7d052eff120c7a5da474a6796fe4d83e152e0e4d42f6d1a9b", size = 81205, upload-time = "2025-10-06T14:10:43.167Z" },
+    { url = "https://files.pythonhosted.org/packages/88/fc/6908f062a2f77b5f9f6d69cecb1747260831ff206adcbc5b510aff88df91/yarl-1.22.0-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:719ae08b6972befcba4310e49edb1161a88cdd331e3a694b84466bd938a6ab10", size = 146209, upload-time = "2025-10-06T14:10:44.643Z" },
+    { url = "https://files.pythonhosted.org/packages/65/47/76594ae8eab26210b4867be6f49129861ad33da1f1ebdf7051e98492bf62/yarl-1.22.0-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:47d8a5c446df1c4db9d21b49619ffdba90e77c89ec6e283f453856c74b50b9e3", size = 95966, upload-time = "2025-10-06T14:10:46.554Z" },
+    { url = "https://files.pythonhosted.org/packages/ab/ce/05e9828a49271ba6b5b038b15b3934e996980dd78abdfeb52a04cfb9467e/yarl-1.22.0-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:cfebc0ac8333520d2d0423cbbe43ae43c8838862ddb898f5ca68565e395516e9", size = 97312, upload-time = "2025-10-06T14:10:48.007Z" },
+    { url = "https://files.pythonhosted.org/packages/d1/c5/7dffad5e4f2265b29c9d7ec869c369e4223166e4f9206fc2243ee9eea727/yarl-1.22.0-cp313-cp313t-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:4398557cbf484207df000309235979c79c4356518fd5c99158c7d38203c4da4f", size = 361967, upload-time = "2025-10-06T14:10:49.997Z" },
+    { url = "https://files.pythonhosted.org/packages/50/b2/375b933c93a54bff7fc041e1a6ad2c0f6f733ffb0c6e642ce56ee3b39970/yarl-1.22.0-cp313-cp313t-manylinux2014_armv7l.manylinux_2_17_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:2ca6fd72a8cd803be290d42f2dec5cdcd5299eeb93c2d929bf060ad9efaf5de0", size = 323949, upload-time = "2025-10-06T14:10:52.004Z" },
+    { url = "https://files.pythonhosted.org/packages/66/50/bfc2a29a1d78644c5a7220ce2f304f38248dc94124a326794e677634b6cf/yarl-1.22.0-cp313-cp313t-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:ca1f59c4e1ab6e72f0a23c13fca5430f889634166be85dbf1013683e49e3278e", size = 361818, upload-time = "2025-10-06T14:10:54.078Z" },
+    { url = "https://files.pythonhosted.org/packages/46/96/f3941a46af7d5d0f0498f86d71275696800ddcdd20426298e572b19b91ff/yarl-1.22.0-cp313-cp313t-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:6c5010a52015e7c70f86eb967db0f37f3c8bd503a695a49f8d45700144667708", size = 372626, upload-time = "2025-10-06T14:10:55.767Z" },
+    { url = "https://files.pythonhosted.org/packages/c1/42/8b27c83bb875cd89448e42cd627e0fb971fa1675c9ec546393d18826cb50/yarl-1.22.0-cp313-cp313t-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:9d7672ecf7557476642c88497c2f8d8542f8e36596e928e9bcba0e42e1e7d71f", size = 341129, upload-time = "2025-10-06T14:10:57.985Z" },
+    { url = "https://files.pythonhosted.org/packages/49/36/99ca3122201b382a3cf7cc937b95235b0ac944f7e9f2d5331d50821ed352/yarl-1.22.0-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:3b7c88eeef021579d600e50363e0b6ee4f7f6f728cd3486b9d0f3ee7b946398d", size = 346776, upload-time = "2025-10-06T14:10:59.633Z" },
+    { url = "https://files.pythonhosted.org/packages/85/b4/47328bf996acd01a4c16ef9dcd2f59c969f495073616586f78cd5f2efb99/yarl-1.22.0-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:f4afb5c34f2c6fecdcc182dfcfc6af6cccf1aa923eed4d6a12e9d96904e1a0d8", size = 334879, upload-time = "2025-10-06T14:11:01.454Z" },
+    { url = "https://files.pythonhosted.org/packages/c2/ad/b77d7b3f14a4283bffb8e92c6026496f6de49751c2f97d4352242bba3990/yarl-1.22.0-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:59c189e3e99a59cf8d83cbb31d4db02d66cda5a1a4374e8a012b51255341abf5", size = 350996, upload-time = "2025-10-06T14:11:03.452Z" },
+    { url = "https://files.pythonhosted.org/packages/81/c8/06e1d69295792ba54d556f06686cbd6a7ce39c22307100e3fb4a2c0b0a1d/yarl-1.22.0-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:5a3bf7f62a289fa90f1990422dc8dff5a458469ea71d1624585ec3a4c8d6960f", size = 356047, upload-time = "2025-10-06T14:11:05.115Z" },
+    { url = "https://files.pythonhosted.org/packages/4b/b8/4c0e9e9f597074b208d18cef227d83aac36184bfbc6eab204ea55783dbc5/yarl-1.22.0-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:de6b9a04c606978fdfe72666fa216ffcf2d1a9f6a381058d4378f8d7b1e5de62", size = 342947, upload-time = "2025-10-06T14:11:08.137Z" },
+    { url = "https://files.pythonhosted.org/packages/e0/e5/11f140a58bf4c6ad7aca69a892bff0ee638c31bea4206748fc0df4ebcb3a/yarl-1.22.0-cp313-cp313t-win32.whl", hash = "sha256:1834bb90991cc2999f10f97f5f01317f99b143284766d197e43cd5b45eb18d03", size = 86943, upload-time = "2025-10-06T14:11:10.284Z" },
+    { url = "https://files.pythonhosted.org/packages/31/74/8b74bae38ed7fe6793d0c15a0c8207bbb819cf287788459e5ed230996cdd/yarl-1.22.0-cp313-cp313t-win_amd64.whl", hash = "sha256:ff86011bd159a9d2dfc89c34cfd8aff12875980e3bd6a39ff097887520e60249", size = 93715, upload-time = "2025-10-06T14:11:11.739Z" },
+    { url = "https://files.pythonhosted.org/packages/69/66/991858aa4b5892d57aef7ee1ba6b4d01ec3b7eb3060795d34090a3ca3278/yarl-1.22.0-cp313-cp313t-win_arm64.whl", hash = "sha256:7861058d0582b847bc4e3a4a4c46828a410bca738673f35a29ba3ca5db0b473b", size = 83857, upload-time = "2025-10-06T14:11:13.586Z" },
+    { url = "https://files.pythonhosted.org/packages/46/b3/e20ef504049f1a1c54a814b4b9bed96d1ac0e0610c3b4da178f87209db05/yarl-1.22.0-cp314-cp314-macosx_10_13_universal2.whl", hash = "sha256:34b36c2c57124530884d89d50ed2c1478697ad7473efd59cfd479945c95650e4", size = 140520, upload-time = "2025-10-06T14:11:15.465Z" },
+    { url = "https://files.pythonhosted.org/packages/e4/04/3532d990fdbab02e5ede063676b5c4260e7f3abea2151099c2aa745acc4c/yarl-1.22.0-cp314-cp314-macosx_10_13_x86_64.whl", hash = "sha256:0dd9a702591ca2e543631c2a017e4a547e38a5c0f29eece37d9097e04a7ac683", size = 93504, upload-time = "2025-10-06T14:11:17.106Z" },
+    { url = "https://files.pythonhosted.org/packages/11/63/ff458113c5c2dac9a9719ac68ee7c947cb621432bcf28c9972b1c0e83938/yarl-1.22.0-cp314-cp314-macosx_11_0_arm64.whl", hash = "sha256:594fcab1032e2d2cc3321bb2e51271e7cd2b516c7d9aee780ece81b07ff8244b", size = 94282, upload-time = "2025-10-06T14:11:19.064Z" },
+    { url = "https://files.pythonhosted.org/packages/a7/bc/315a56aca762d44a6aaaf7ad253f04d996cb6b27bad34410f82d76ea8038/yarl-1.22.0-cp314-cp314-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:f3d7a87a78d46a2e3d5b72587ac14b4c16952dd0887dbb051451eceac774411e", size = 372080, upload-time = "2025-10-06T14:11:20.996Z" },
+    { url = "https://files.pythonhosted.org/packages/3f/3f/08e9b826ec2e099ea6e7c69a61272f4f6da62cb5b1b63590bb80ca2e4a40/yarl-1.22.0-cp314-cp314-manylinux2014_armv7l.manylinux_2_17_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:852863707010316c973162e703bddabec35e8757e67fcb8ad58829de1ebc8590", size = 338696, upload-time = "2025-10-06T14:11:22.847Z" },
+    { url = "https://files.pythonhosted.org/packages/e3/9f/90360108e3b32bd76789088e99538febfea24a102380ae73827f62073543/yarl-1.22.0-cp314-cp314-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:131a085a53bfe839a477c0845acf21efc77457ba2bcf5899618136d64f3303a2", size = 387121, upload-time = "2025-10-06T14:11:24.889Z" },
+    { url = "https://files.pythonhosted.org/packages/98/92/ab8d4657bd5b46a38094cfaea498f18bb70ce6b63508fd7e909bd1f93066/yarl-1.22.0-cp314-cp314-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:078a8aefd263f4d4f923a9677b942b445a2be970ca24548a8102689a3a8ab8da", size = 394080, upload-time = "2025-10-06T14:11:27.307Z" },
+    { url = "https://files.pythonhosted.org/packages/f5/e7/d8c5a7752fef68205296201f8ec2bf718f5c805a7a7e9880576c67600658/yarl-1.22.0-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:bca03b91c323036913993ff5c738d0842fc9c60c4648e5c8d98331526df89784", size = 372661, upload-time = "2025-10-06T14:11:29.387Z" },
+    { url = "https://files.pythonhosted.org/packages/b6/2e/f4d26183c8db0bb82d491b072f3127fb8c381a6206a3a56332714b79b751/yarl-1.22.0-cp314-cp314-musllinux_1_2_aarch64.whl", hash = "sha256:68986a61557d37bb90d3051a45b91fa3d5c516d177dfc6dd6f2f436a07ff2b6b", size = 364645, upload-time = "2025-10-06T14:11:31.423Z" },
+    { url = "https://files.pythonhosted.org/packages/80/7c/428e5812e6b87cd00ee8e898328a62c95825bf37c7fa87f0b6bb2ad31304/yarl-1.22.0-cp314-cp314-musllinux_1_2_armv7l.whl", hash = "sha256:4792b262d585ff0dff6bcb787f8492e40698443ec982a3568c2096433660c694", size = 355361, upload-time = "2025-10-06T14:11:33.055Z" },
+    { url = "https://files.pythonhosted.org/packages/ec/2a/249405fd26776f8b13c067378ef4d7dd49c9098d1b6457cdd152a99e96a9/yarl-1.22.0-cp314-cp314-musllinux_1_2_ppc64le.whl", hash = "sha256:ebd4549b108d732dba1d4ace67614b9545b21ece30937a63a65dd34efa19732d", size = 381451, upload-time = "2025-10-06T14:11:35.136Z" },
+    { url = "https://files.pythonhosted.org/packages/67/a8/fb6b1adbe98cf1e2dd9fad71003d3a63a1bc22459c6e15f5714eb9323b93/yarl-1.22.0-cp314-cp314-musllinux_1_2_s390x.whl", hash = "sha256:f87ac53513d22240c7d59203f25cc3beac1e574c6cd681bbfd321987b69f95fd", size = 383814, upload-time = "2025-10-06T14:11:37.094Z" },
+    { url = "https://files.pythonhosted.org/packages/d9/f9/3aa2c0e480fb73e872ae2814c43bc1e734740bb0d54e8cb2a95925f98131/yarl-1.22.0-cp314-cp314-musllinux_1_2_x86_64.whl", hash = "sha256:22b029f2881599e2f1b06f8f1db2ee63bd309e2293ba2d566e008ba12778b8da", size = 370799, upload-time = "2025-10-06T14:11:38.83Z" },
+    { url = "https://files.pythonhosted.org/packages/50/3c/af9dba3b8b5eeb302f36f16f92791f3ea62e3f47763406abf6d5a4a3333b/yarl-1.22.0-cp314-cp314-win32.whl", hash = "sha256:6a635ea45ba4ea8238463b4f7d0e721bad669f80878b7bfd1f89266e2ae63da2", size = 82990, upload-time = "2025-10-06T14:11:40.624Z" },
+    { url = "https://files.pythonhosted.org/packages/ac/30/ac3a0c5bdc1d6efd1b41fa24d4897a4329b3b1e98de9449679dd327af4f0/yarl-1.22.0-cp314-cp314-win_amd64.whl", hash = "sha256:0d6e6885777af0f110b0e5d7e5dda8b704efed3894da26220b7f3d887b839a79", size = 88292, upload-time = "2025-10-06T14:11:42.578Z" },
+    { url = "https://files.pythonhosted.org/packages/df/0a/227ab4ff5b998a1b7410abc7b46c9b7a26b0ca9e86c34ba4b8d8bc7c63d5/yarl-1.22.0-cp314-cp314-win_arm64.whl", hash = "sha256:8218f4e98d3c10d683584cb40f0424f4b9fd6e95610232dd75e13743b070ee33", size = 82888, upload-time = "2025-10-06T14:11:44.863Z" },
+    { url = "https://files.pythonhosted.org/packages/06/5e/a15eb13db90abd87dfbefb9760c0f3f257ac42a5cac7e75dbc23bed97a9f/yarl-1.22.0-cp314-cp314t-macosx_10_13_universal2.whl", hash = "sha256:45c2842ff0e0d1b35a6bf1cd6c690939dacb617a70827f715232b2e0494d55d1", size = 146223, upload-time = "2025-10-06T14:11:46.796Z" },
+    { url = "https://files.pythonhosted.org/packages/18/82/9665c61910d4d84f41a5bf6837597c89e665fa88aa4941080704645932a9/yarl-1.22.0-cp314-cp314t-macosx_10_13_x86_64.whl", hash = "sha256:d947071e6ebcf2e2bee8fce76e10faca8f7a14808ca36a910263acaacef08eca", size = 95981, upload-time = "2025-10-06T14:11:48.845Z" },
+    { url = "https://files.pythonhosted.org/packages/5d/9a/2f65743589809af4d0a6d3aa749343c4b5f4c380cc24a8e94a3c6625a808/yarl-1.22.0-cp314-cp314t-macosx_11_0_arm64.whl", hash = "sha256:334b8721303e61b00019474cc103bdac3d7b1f65e91f0bfedeec2d56dfe74b53", size = 97303, upload-time = "2025-10-06T14:11:50.897Z" },
+    { url = "https://files.pythonhosted.org/packages/b0/ab/5b13d3e157505c43c3b43b5a776cbf7b24a02bc4cccc40314771197e3508/yarl-1.22.0-cp314-cp314t-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:1e7ce67c34138a058fd092f67d07a72b8e31ff0c9236e751957465a24b28910c", size = 361820, upload-time = "2025-10-06T14:11:52.549Z" },
+    { url = "https://files.pythonhosted.org/packages/fb/76/242a5ef4677615cf95330cfc1b4610e78184400699bdda0acb897ef5e49a/yarl-1.22.0-cp314-cp314t-manylinux2014_armv7l.manylinux_2_17_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:d77e1b2c6d04711478cb1c4ab90db07f1609ccf06a287d5607fcd90dc9863acf", size = 323203, upload-time = "2025-10-06T14:11:54.225Z" },
+    { url = "https://files.pythonhosted.org/packages/8c/96/475509110d3f0153b43d06164cf4195c64d16999e0c7e2d8a099adcd6907/yarl-1.22.0-cp314-cp314t-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:c4647674b6150d2cae088fc07de2738a84b8bcedebef29802cf0b0a82ab6face", size = 363173, upload-time = "2025-10-06T14:11:56.069Z" },
+    { url = "https://files.pythonhosted.org/packages/c9/66/59db471aecfbd559a1fd48aedd954435558cd98c7d0da8b03cc6c140a32c/yarl-1.22.0-cp314-cp314t-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:efb07073be061c8f79d03d04139a80ba33cbd390ca8f0297aae9cce6411e4c6b", size = 373562, upload-time = "2025-10-06T14:11:58.783Z" },
+    { url = "https://files.pythonhosted.org/packages/03/1f/c5d94abc91557384719da10ff166b916107c1b45e4d0423a88457071dd88/yarl-1.22.0-cp314-cp314t-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:e51ac5435758ba97ad69617e13233da53908beccc6cfcd6c34bbed8dcbede486", size = 339828, upload-time = "2025-10-06T14:12:00.686Z" },
+    { url = "https://files.pythonhosted.org/packages/5f/97/aa6a143d3afba17b6465733681c70cf175af89f76ec8d9286e08437a7454/yarl-1.22.0-cp314-cp314t-musllinux_1_2_aarch64.whl", hash = "sha256:33e32a0dd0c8205efa8e83d04fc9f19313772b78522d1bdc7d9aed706bfd6138", size = 347551, upload-time = "2025-10-06T14:12:02.628Z" },
+    { url = "https://files.pythonhosted.org/packages/43/3c/45a2b6d80195959239a7b2a8810506d4eea5487dce61c2a3393e7fc3c52e/yarl-1.22.0-cp314-cp314t-musllinux_1_2_armv7l.whl", hash = "sha256:bf4a21e58b9cde0e401e683ebd00f6ed30a06d14e93f7c8fd059f8b6e8f87b6a", size = 334512, upload-time = "2025-10-06T14:12:04.871Z" },
+    { url = "https://files.pythonhosted.org/packages/86/a0/c2ab48d74599c7c84cb104ebd799c5813de252bea0f360ffc29d270c2caa/yarl-1.22.0-cp314-cp314t-musllinux_1_2_ppc64le.whl", hash = "sha256:e4b582bab49ac33c8deb97e058cd67c2c50dac0dd134874106d9c774fd272529", size = 352400, upload-time = "2025-10-06T14:12:06.624Z" },
+    { url = "https://files.pythonhosted.org/packages/32/75/f8919b2eafc929567d3d8411f72bdb1a2109c01caaab4ebfa5f8ffadc15b/yarl-1.22.0-cp314-cp314t-musllinux_1_2_s390x.whl", hash = "sha256:0b5bcc1a9c4839e7e30b7b30dd47fe5e7e44fb7054ec29b5bb8d526aa1041093", size = 357140, upload-time = "2025-10-06T14:12:08.362Z" },
+    { url = "https://files.pythonhosted.org/packages/cf/72/6a85bba382f22cf78add705d8c3731748397d986e197e53ecc7835e76de7/yarl-1.22.0-cp314-cp314t-musllinux_1_2_x86_64.whl", hash = "sha256:c0232bce2170103ec23c454e54a57008a9a72b5d1c3105dc2496750da8cfa47c", size = 341473, upload-time = "2025-10-06T14:12:10.994Z" },
+    { url = "https://files.pythonhosted.org/packages/35/18/55e6011f7c044dc80b98893060773cefcfdbf60dfefb8cb2f58b9bacbd83/yarl-1.22.0-cp314-cp314t-win32.whl", hash = "sha256:8009b3173bcd637be650922ac455946197d858b3630b6d8787aa9e5c4564533e", size = 89056, upload-time = "2025-10-06T14:12:13.317Z" },
+    { url = "https://files.pythonhosted.org/packages/f9/86/0f0dccb6e59a9e7f122c5afd43568b1d31b8ab7dda5f1b01fb5c7025c9a9/yarl-1.22.0-cp314-cp314t-win_amd64.whl", hash = "sha256:9fb17ea16e972c63d25d4a97f016d235c78dd2344820eb35bc034bc32012ee27", size = 96292, upload-time = "2025-10-06T14:12:15.398Z" },
+    { url = "https://files.pythonhosted.org/packages/48/b7/503c98092fb3b344a179579f55814b613c1fbb1c23b3ec14a7b008a66a6e/yarl-1.22.0-cp314-cp314t-win_arm64.whl", hash = "sha256:9f6d73c1436b934e3f01df1e1b21ff765cd1d28c77dfb9ace207f746d4610ee1", size = 85171, upload-time = "2025-10-06T14:12:16.935Z" },
+    { url = "https://files.pythonhosted.org/packages/73/ae/b48f95715333080afb75a4504487cbe142cae1268afc482d06692d605ae6/yarl-1.22.0-py3-none-any.whl", hash = "sha256:1380560bdba02b6b6c90de54133c81c9f2a453dee9912fe58c1dcced1edb7cff", size = 46814, upload-time = "2025-10-06T14:12:53.872Z" },
+]
+
+[[package]]
+name = "zstandard"
+version = "0.25.0"
+source = { registry = "https://pypi.org/simple" }
+sdist = { url = "https://files.pythonhosted.org/packages/fd/aa/3e0508d5a5dd96529cdc5a97011299056e14c6505b678fd58938792794b1/zstandard-0.25.0.tar.gz", hash = "sha256:7713e1179d162cf5c7906da876ec2ccb9c3a9dcbdffef0cc7f70c3667a205f0b", size = 711513, upload-time = "2025-09-14T22:15:54.002Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/2a/83/c3ca27c363d104980f1c9cee1101cc8ba724ac8c28a033ede6aab89585b1/zstandard-0.25.0-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:933b65d7680ea337180733cf9e87293cc5500cc0eb3fc8769f4d3c88d724ec5c", size = 795254, upload-time = "2025-09-14T22:16:26.137Z" },
+    { url = "https://files.pythonhosted.org/packages/ac/4d/e66465c5411a7cf4866aeadc7d108081d8ceba9bc7abe6b14aa21c671ec3/zstandard-0.25.0-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:a3f79487c687b1fc69f19e487cd949bf3aae653d181dfb5fde3bf6d18894706f", size = 640559, upload-time = "2025-09-14T22:16:27.973Z" },
+    { url = "https://files.pythonhosted.org/packages/12/56/354fe655905f290d3b147b33fe946b0f27e791e4b50a5f004c802cb3eb7b/zstandard-0.25.0-cp311-cp311-manylinux2010_i686.manylinux2014_i686.manylinux_2_12_i686.manylinux_2_17_i686.whl", hash = "sha256:0bbc9a0c65ce0eea3c34a691e3c4b6889f5f3909ba4822ab385fab9057099431", size = 5348020, upload-time = "2025-09-14T22:16:29.523Z" },
+    { url = "https://files.pythonhosted.org/packages/3b/13/2b7ed68bd85e69a2069bcc72141d378f22cae5a0f3b353a2c8f50ef30c1b/zstandard-0.25.0-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.whl", hash = "sha256:01582723b3ccd6939ab7b3a78622c573799d5d8737b534b86d0e06ac18dbde4a", size = 5058126, upload-time = "2025-09-14T22:16:31.811Z" },
+    { url = "https://files.pythonhosted.org/packages/c9/dd/fdaf0674f4b10d92cb120ccff58bbb6626bf8368f00ebfd2a41ba4a0dc99/zstandard-0.25.0-cp311-cp311-manylinux2014_ppc64le.manylinux_2_17_ppc64le.whl", hash = "sha256:5f1ad7bf88535edcf30038f6919abe087f606f62c00a87d7e33e7fc57cb69fcc", size = 5405390, upload-time = "2025-09-14T22:16:33.486Z" },
+    { url = "https://files.pythonhosted.org/packages/0f/67/354d1555575bc2490435f90d67ca4dd65238ff2f119f30f72d5cde09c2ad/zstandard-0.25.0-cp311-cp311-manylinux2014_s390x.manylinux_2_17_s390x.whl", hash = "sha256:06acb75eebeedb77b69048031282737717a63e71e4ae3f77cc0c3b9508320df6", size = 5452914, upload-time = "2025-09-14T22:16:35.277Z" },
+    { url = "https://files.pythonhosted.org/packages/bb/1f/e9cfd801a3f9190bf3e759c422bbfd2247db9d7f3d54a56ecde70137791a/zstandard-0.25.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl", hash = "sha256:9300d02ea7c6506f00e627e287e0492a5eb0371ec1670ae852fefffa6164b072", size = 5559635, upload-time = "2025-09-14T22:16:37.141Z" },
+    { url = "https://files.pythonhosted.org/packages/21/88/5ba550f797ca953a52d708c8e4f380959e7e3280af029e38fbf47b55916e/zstandard-0.25.0-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:bfd06b1c5584b657a2892a6014c2f4c20e0db0208c159148fa78c65f7e0b0277", size = 5048277, upload-time = "2025-09-14T22:16:38.807Z" },
+    { url = "https://files.pythonhosted.org/packages/46/c0/ca3e533b4fa03112facbe7fbe7779cb1ebec215688e5df576fe5429172e0/zstandard-0.25.0-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:f373da2c1757bb7f1acaf09369cdc1d51d84131e50d5fa9863982fd626466313", size = 5574377, upload-time = "2025-09-14T22:16:40.523Z" },
+    { url = "https://files.pythonhosted.org/packages/12/9b/3fb626390113f272abd0799fd677ea33d5fc3ec185e62e6be534493c4b60/zstandard-0.25.0-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:6c0e5a65158a7946e7a7affa6418878ef97ab66636f13353b8502d7ea03c8097", size = 4961493, upload-time = "2025-09-14T22:16:43.3Z" },
+    { url = "https://files.pythonhosted.org/packages/cb/d3/23094a6b6a4b1343b27ae68249daa17ae0651fcfec9ed4de09d14b940285/zstandard-0.25.0-cp311-cp311-musllinux_1_2_i686.whl", hash = "sha256:c8e167d5adf59476fa3e37bee730890e389410c354771a62e3c076c86f9f7778", size = 5269018, upload-time = "2025-09-14T22:16:45.292Z" },
+    { url = "https://files.pythonhosted.org/packages/8c/a7/bb5a0c1c0f3f4b5e9d5b55198e39de91e04ba7c205cc46fcb0f95f0383c1/zstandard-0.25.0-cp311-cp311-musllinux_1_2_ppc64le.whl", hash = "sha256:98750a309eb2f020da61e727de7d7ba3c57c97cf6213f6f6277bb7fb42a8e065", size = 5443672, upload-time = "2025-09-14T22:16:47.076Z" },
+    { url = "https://files.pythonhosted.org/packages/27/22/503347aa08d073993f25109c36c8d9f029c7d5949198050962cb568dfa5e/zstandard-0.25.0-cp311-cp311-musllinux_1_2_s390x.whl", hash = "sha256:22a086cff1b6ceca18a8dd6096ec631e430e93a8e70a9ca5efa7561a00f826fa", size = 5822753, upload-time = "2025-09-14T22:16:49.316Z" },
+    { url = "https://files.pythonhosted.org/packages/e2/be/94267dc6ee64f0f8ba2b2ae7c7a2df934a816baaa7291db9e1aa77394c3c/zstandard-0.25.0-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:72d35d7aa0bba323965da807a462b0966c91608ef3a48ba761678cb20ce5d8b7", size = 5366047, upload-time = "2025-09-14T22:16:51.328Z" },
+    { url = "https://files.pythonhosted.org/packages/7b/a3/732893eab0a3a7aecff8b99052fecf9f605cf0fb5fb6d0290e36beee47a4/zstandard-0.25.0-cp311-cp311-win32.whl", hash = "sha256:f5aeea11ded7320a84dcdd62a3d95b5186834224a9e55b92ccae35d21a8b63d4", size = 436484, upload-time = "2025-09-14T22:16:55.005Z" },
+    { url = "https://files.pythonhosted.org/packages/43/a3/c6155f5c1cce691cb80dfd38627046e50af3ee9ddc5d0b45b9b063bfb8c9/zstandard-0.25.0-cp311-cp311-win_amd64.whl", hash = "sha256:daab68faadb847063d0c56f361a289c4f268706b598afbf9ad113cbe5c38b6b2", size = 506183, upload-time = "2025-09-14T22:16:52.753Z" },
+    { url = "https://files.pythonhosted.org/packages/8c/3e/8945ab86a0820cc0e0cdbf38086a92868a9172020fdab8a03ac19662b0e5/zstandard-0.25.0-cp311-cp311-win_arm64.whl", hash = "sha256:22a06c5df3751bb7dc67406f5374734ccee8ed37fc5981bf1ad7041831fa1137", size = 462533, upload-time = "2025-09-14T22:16:53.878Z" },
+    { url = "https://files.pythonhosted.org/packages/82/fc/f26eb6ef91ae723a03e16eddb198abcfce2bc5a42e224d44cc8b6765e57e/zstandard-0.25.0-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:7b3c3a3ab9daa3eed242d6ecceead93aebbb8f5f84318d82cee643e019c4b73b", size = 795738, upload-time = "2025-09-14T22:16:56.237Z" },
+    { url = "https://files.pythonhosted.org/packages/aa/1c/d920d64b22f8dd028a8b90e2d756e431a5d86194caa78e3819c7bf53b4b3/zstandard-0.25.0-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:913cbd31a400febff93b564a23e17c3ed2d56c064006f54efec210d586171c00", size = 640436, upload-time = "2025-09-14T22:16:57.774Z" },
+    { url = "https://files.pythonhosted.org/packages/53/6c/288c3f0bd9fcfe9ca41e2c2fbfd17b2097f6af57b62a81161941f09afa76/zstandard-0.25.0-cp312-cp312-manylinux2010_i686.manylinux2014_i686.manylinux_2_12_i686.manylinux_2_17_i686.whl", hash = "sha256:011d388c76b11a0c165374ce660ce2c8efa8e5d87f34996aa80f9c0816698b64", size = 5343019, upload-time = "2025-09-14T22:16:59.302Z" },
+    { url = "https://files.pythonhosted.org/packages/1e/15/efef5a2f204a64bdb5571e6161d49f7ef0fffdbca953a615efbec045f60f/zstandard-0.25.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.whl", hash = "sha256:6dffecc361d079bb48d7caef5d673c88c8988d3d33fb74ab95b7ee6da42652ea", size = 5063012, upload-time = "2025-09-14T22:17:01.156Z" },
+    { url = "https://files.pythonhosted.org/packages/b7/37/a6ce629ffdb43959e92e87ebdaeebb5ac81c944b6a75c9c47e300f85abdf/zstandard-0.25.0-cp312-cp312-manylinux2014_ppc64le.manylinux_2_17_ppc64le.whl", hash = "sha256:7149623bba7fdf7e7f24312953bcf73cae103db8cae49f8154dd1eadc8a29ecb", size = 5394148, upload-time = "2025-09-14T22:17:03.091Z" },
+    { url = "https://files.pythonhosted.org/packages/e3/79/2bf870b3abeb5c070fe2d670a5a8d1057a8270f125ef7676d29ea900f496/zstandard-0.25.0-cp312-cp312-manylinux2014_s390x.manylinux_2_17_s390x.whl", hash = "sha256:6a573a35693e03cf1d67799fd01b50ff578515a8aeadd4595d2a7fa9f3ec002a", size = 5451652, upload-time = "2025-09-14T22:17:04.979Z" },
+    { url = "https://files.pythonhosted.org/packages/53/60/7be26e610767316c028a2cbedb9a3beabdbe33e2182c373f71a1c0b88f36/zstandard-0.25.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl", hash = "sha256:5a56ba0db2d244117ed744dfa8f6f5b366e14148e00de44723413b2f3938a902", size = 5546993, upload-time = "2025-09-14T22:17:06.781Z" },
+    { url = "https://files.pythonhosted.org/packages/85/c7/3483ad9ff0662623f3648479b0380d2de5510abf00990468c286c6b04017/zstandard-0.25.0-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:10ef2a79ab8e2974e2075fb984e5b9806c64134810fac21576f0668e7ea19f8f", size = 5046806, upload-time = "2025-09-14T22:17:08.415Z" },
+    { url = "https://files.pythonhosted.org/packages/08/b3/206883dd25b8d1591a1caa44b54c2aad84badccf2f1de9e2d60a446f9a25/zstandard-0.25.0-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:aaf21ba8fb76d102b696781bddaa0954b782536446083ae3fdaa6f16b25a1c4b", size = 5576659, upload-time = "2025-09-14T22:17:10.164Z" },
+    { url = "https://files.pythonhosted.org/packages/9d/31/76c0779101453e6c117b0ff22565865c54f48f8bd807df2b00c2c404b8e0/zstandard-0.25.0-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:1869da9571d5e94a85a5e8d57e4e8807b175c9e4a6294e3b66fa4efb074d90f6", size = 4953933, upload-time = "2025-09-14T22:17:11.857Z" },
+    { url = "https://files.pythonhosted.org/packages/18/e1/97680c664a1bf9a247a280a053d98e251424af51f1b196c6d52f117c9720/zstandard-0.25.0-cp312-cp312-musllinux_1_2_i686.whl", hash = "sha256:809c5bcb2c67cd0ed81e9229d227d4ca28f82d0f778fc5fea624a9def3963f91", size = 5268008, upload-time = "2025-09-14T22:17:13.627Z" },
+    { url = "https://files.pythonhosted.org/packages/1e/73/316e4010de585ac798e154e88fd81bb16afc5c5cb1a72eeb16dd37e8024a/zstandard-0.25.0-cp312-cp312-musllinux_1_2_ppc64le.whl", hash = "sha256:f27662e4f7dbf9f9c12391cb37b4c4c3cb90ffbd3b1fb9284dadbbb8935fa708", size = 5433517, upload-time = "2025-09-14T22:17:16.103Z" },
+    { url = "https://files.pythonhosted.org/packages/5b/60/dd0f8cfa8129c5a0ce3ea6b7f70be5b33d2618013a161e1ff26c2b39787c/zstandard-0.25.0-cp312-cp312-musllinux_1_2_s390x.whl", hash = "sha256:99c0c846e6e61718715a3c9437ccc625de26593fea60189567f0118dc9db7512", size = 5814292, upload-time = "2025-09-14T22:17:17.827Z" },
+    { url = "https://files.pythonhosted.org/packages/fc/5f/75aafd4b9d11b5407b641b8e41a57864097663699f23e9ad4dbb91dc6bfe/zstandard-0.25.0-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:474d2596a2dbc241a556e965fb76002c1ce655445e4e3bf38e5477d413165ffa", size = 5360237, upload-time = "2025-09-14T22:17:19.954Z" },
+    { url = "https://files.pythonhosted.org/packages/ff/8d/0309daffea4fcac7981021dbf21cdb2e3427a9e76bafbcdbdf5392ff99a4/zstandard-0.25.0-cp312-cp312-win32.whl", hash = "sha256:23ebc8f17a03133b4426bcc04aabd68f8236eb78c3760f12783385171b0fd8bd", size = 436922, upload-time = "2025-09-14T22:17:24.398Z" },
+    { url = "https://files.pythonhosted.org/packages/79/3b/fa54d9015f945330510cb5d0b0501e8253c127cca7ebe8ba46a965df18c5/zstandard-0.25.0-cp312-cp312-win_amd64.whl", hash = "sha256:ffef5a74088f1e09947aecf91011136665152e0b4b359c42be3373897fb39b01", size = 506276, upload-time = "2025-09-14T22:17:21.429Z" },
+    { url = "https://files.pythonhosted.org/packages/ea/6b/8b51697e5319b1f9ac71087b0af9a40d8a6288ff8025c36486e0c12abcc4/zstandard-0.25.0-cp312-cp312-win_arm64.whl", hash = "sha256:181eb40e0b6a29b3cd2849f825e0fa34397f649170673d385f3598ae17cca2e9", size = 462679, upload-time = "2025-09-14T22:17:23.147Z" },
+    { url = "https://files.pythonhosted.org/packages/35/0b/8df9c4ad06af91d39e94fa96cc010a24ac4ef1378d3efab9223cc8593d40/zstandard-0.25.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:ec996f12524f88e151c339688c3897194821d7f03081ab35d31d1e12ec975e94", size = 795735, upload-time = "2025-09-14T22:17:26.042Z" },
+    { url = "https://files.pythonhosted.org/packages/3f/06/9ae96a3e5dcfd119377ba33d4c42a7d89da1efabd5cb3e366b156c45ff4d/zstandard-0.25.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:a1a4ae2dec3993a32247995bdfe367fc3266da832d82f8438c8570f989753de1", size = 640440, upload-time = "2025-09-14T22:17:27.366Z" },
+    { url = "https://files.pythonhosted.org/packages/d9/14/933d27204c2bd404229c69f445862454dcc101cd69ef8c6068f15aaec12c/zstandard-0.25.0-cp313-cp313-manylinux2010_i686.manylinux2014_i686.manylinux_2_12_i686.manylinux_2_17_i686.whl", hash = "sha256:e96594a5537722fdfb79951672a2a63aec5ebfb823e7560586f7484819f2a08f", size = 5343070, upload-time = "2025-09-14T22:17:28.896Z" },
+    { url = "https://files.pythonhosted.org/packages/6d/db/ddb11011826ed7db9d0e485d13df79b58586bfdec56e5c84a928a9a78c1c/zstandard-0.25.0-cp313-cp313-manylinux2014_aarch64.manylinux_2_17_aarch64.whl", hash = "sha256:bfc4e20784722098822e3eee42b8e576b379ed72cca4a7cb856ae733e62192ea", size = 5063001, upload-time = "2025-09-14T22:17:31.044Z" },
+    { url = "https://files.pythonhosted.org/packages/db/00/87466ea3f99599d02a5238498b87bf84a6348290c19571051839ca943777/zstandard-0.25.0-cp313-cp313-manylinux2014_ppc64le.manylinux_2_17_ppc64le.whl", hash = "sha256:457ed498fc58cdc12fc48f7950e02740d4f7ae9493dd4ab2168a47c93c31298e", size = 5394120, upload-time = "2025-09-14T22:17:32.711Z" },
+    { url = "https://files.pythonhosted.org/packages/2b/95/fc5531d9c618a679a20ff6c29e2b3ef1d1f4ad66c5e161ae6ff847d102a9/zstandard-0.25.0-cp313-cp313-manylinux2014_s390x.manylinux_2_17_s390x.whl", hash = "sha256:fd7a5004eb1980d3cefe26b2685bcb0b17989901a70a1040d1ac86f1d898c551", size = 5451230, upload-time = "2025-09-14T22:17:34.41Z" },
+    { url = "https://files.pythonhosted.org/packages/63/4b/e3678b4e776db00f9f7b2fe58e547e8928ef32727d7a1ff01dea010f3f13/zstandard-0.25.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl", hash = "sha256:8e735494da3db08694d26480f1493ad2cf86e99bdd53e8e9771b2752a5c0246a", size = 5547173, upload-time = "2025-09-14T22:17:36.084Z" },
+    { url = "https://files.pythonhosted.org/packages/4e/d5/ba05ed95c6b8ec30bd468dfeab20589f2cf709b5c940483e31d991f2ca58/zstandard-0.25.0-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:3a39c94ad7866160a4a46d772e43311a743c316942037671beb264e395bdd611", size = 5046736, upload-time = "2025-09-14T22:17:37.891Z" },
+    { url = "https://files.pythonhosted.org/packages/50/d5/870aa06b3a76c73eced65c044b92286a3c4e00554005ff51962deef28e28/zstandard-0.25.0-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:172de1f06947577d3a3005416977cce6168f2261284c02080e7ad0185faeced3", size = 5576368, upload-time = "2025-09-14T22:17:40.206Z" },
+    { url = "https://files.pythonhosted.org/packages/5d/35/398dc2ffc89d304d59bc12f0fdd931b4ce455bddf7038a0a67733a25f550/zstandard-0.25.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:3c83b0188c852a47cd13ef3bf9209fb0a77fa5374958b8c53aaa699398c6bd7b", size = 4954022, upload-time = "2025-09-14T22:17:41.879Z" },
+    { url = "https://files.pythonhosted.org/packages/9a/5c/36ba1e5507d56d2213202ec2b05e8541734af5f2ce378c5d1ceaf4d88dc4/zstandard-0.25.0-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:1673b7199bbe763365b81a4f3252b8e80f44c9e323fc42940dc8843bfeaf9851", size = 5267889, upload-time = "2025-09-14T22:17:43.577Z" },
+    { url = "https://files.pythonhosted.org/packages/70/e8/2ec6b6fb7358b2ec0113ae202647ca7c0e9d15b61c005ae5225ad0995df5/zstandard-0.25.0-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:0be7622c37c183406f3dbf0cba104118eb16a4ea7359eeb5752f0794882fc250", size = 5433952, upload-time = "2025-09-14T22:17:45.271Z" },
+    { url = "https://files.pythonhosted.org/packages/7b/01/b5f4d4dbc59ef193e870495c6f1275f5b2928e01ff5a81fecb22a06e22fb/zstandard-0.25.0-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:5f5e4c2a23ca271c218ac025bd7d635597048b366d6f31f420aaeb715239fc98", size = 5814054, upload-time = "2025-09-14T22:17:47.08Z" },
+    { url = "https://files.pythonhosted.org/packages/b2/e5/fbd822d5c6f427cf158316d012c5a12f233473c2f9c5fe5ab1ae5d21f3d8/zstandard-0.25.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:4f187a0bb61b35119d1926aee039524d1f93aaf38a9916b8c4b78ac8514a0aaf", size = 5360113, upload-time = "2025-09-14T22:17:48.893Z" },
+    { url = "https://files.pythonhosted.org/packages/8e/e0/69a553d2047f9a2c7347caa225bb3a63b6d7704ad74610cb7823baa08ed7/zstandard-0.25.0-cp313-cp313-win32.whl", hash = "sha256:7030defa83eef3e51ff26f0b7bfb229f0204b66fe18e04359ce3474ac33cbc09", size = 436936, upload-time = "2025-09-14T22:17:52.658Z" },
+    { url = "https://files.pythonhosted.org/packages/d9/82/b9c06c870f3bd8767c201f1edbdf9e8dc34be5b0fbc5682c4f80fe948475/zstandard-0.25.0-cp313-cp313-win_amd64.whl", hash = "sha256:1f830a0dac88719af0ae43b8b2d6aef487d437036468ef3c2ea59c51f9d55fd5", size = 506232, upload-time = "2025-09-14T22:17:50.402Z" },
+    { url = "https://files.pythonhosted.org/packages/d4/57/60c3c01243bb81d381c9916e2a6d9e149ab8627c0c7d7abb2d73384b3c0c/zstandard-0.25.0-cp313-cp313-win_arm64.whl", hash = "sha256:85304a43f4d513f5464ceb938aa02c1e78c2943b29f44a750b48b25ac999a049", size = 462671, upload-time = "2025-09-14T22:17:51.533Z" },
+    { url = "https://files.pythonhosted.org/packages/3d/5c/f8923b595b55fe49e30612987ad8bf053aef555c14f05bb659dd5dbe3e8a/zstandard-0.25.0-cp314-cp314-macosx_10_13_x86_64.whl", hash = "sha256:e29f0cf06974c899b2c188ef7f783607dbef36da4c242eb6c82dcd8b512855e3", size = 795887, upload-time = "2025-09-14T22:17:54.198Z" },
+    { url = "https://files.pythonhosted.org/packages/8d/09/d0a2a14fc3439c5f874042dca72a79c70a532090b7ba0003be73fee37ae2/zstandard-0.25.0-cp314-cp314-macosx_11_0_arm64.whl", hash = "sha256:05df5136bc5a011f33cd25bc9f506e7426c0c9b3f9954f056831ce68f3b6689f", size = 640658, upload-time = "2025-09-14T22:17:55.423Z" },
+    { url = "https://files.pythonhosted.org/packages/5d/7c/8b6b71b1ddd517f68ffb55e10834388d4f793c49c6b83effaaa05785b0b4/zstandard-0.25.0-cp314-cp314-manylinux2010_i686.manylinux_2_12_i686.manylinux_2_28_i686.whl", hash = "sha256:f604efd28f239cc21b3adb53eb061e2a205dc164be408e553b41ba2ffe0ca15c", size = 5379849, upload-time = "2025-09-14T22:17:57.372Z" },
+    { url = "https://files.pythonhosted.org/packages/a4/86/a48e56320d0a17189ab7a42645387334fba2200e904ee47fc5a26c1fd8ca/zstandard-0.25.0-cp314-cp314-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:223415140608d0f0da010499eaa8ccdb9af210a543fac54bce15babbcfc78439", size = 5058095, upload-time = "2025-09-14T22:17:59.498Z" },
+    { url = "https://files.pythonhosted.org/packages/f8/ad/eb659984ee2c0a779f9d06dbfe45e2dc39d99ff40a319895df2d3d9a48e5/zstandard-0.25.0-cp314-cp314-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:2e54296a283f3ab5a26fc9b8b5d4978ea0532f37b231644f367aa588930aa043", size = 5551751, upload-time = "2025-09-14T22:18:01.618Z" },
+    { url = "https://files.pythonhosted.org/packages/61/b3/b637faea43677eb7bd42ab204dfb7053bd5c4582bfe6b1baefa80ac0c47b/zstandard-0.25.0-cp314-cp314-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:ca54090275939dc8ec5dea2d2afb400e0f83444b2fc24e07df7fdef677110859", size = 6364818, upload-time = "2025-09-14T22:18:03.769Z" },
+    { url = "https://files.pythonhosted.org/packages/31/dc/cc50210e11e465c975462439a492516a73300ab8caa8f5e0902544fd748b/zstandard-0.25.0-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:e09bb6252b6476d8d56100e8147b803befa9a12cea144bbe629dd508800d1ad0", size = 5560402, upload-time = "2025-09-14T22:18:05.954Z" },
+    { url = "https://files.pythonhosted.org/packages/c9/ae/56523ae9c142f0c08efd5e868a6da613ae76614eca1305259c3bf6a0ed43/zstandard-0.25.0-cp314-cp314-musllinux_1_2_aarch64.whl", hash = "sha256:a9ec8c642d1ec73287ae3e726792dd86c96f5681eb8df274a757bf62b750eae7", size = 4955108, upload-time = "2025-09-14T22:18:07.68Z" },
+    { url = "https://files.pythonhosted.org/packages/98/cf/c899f2d6df0840d5e384cf4c4121458c72802e8bda19691f3b16619f51e9/zstandard-0.25.0-cp314-cp314-musllinux_1_2_i686.whl", hash = "sha256:a4089a10e598eae6393756b036e0f419e8c1d60f44a831520f9af41c14216cf2", size = 5269248, upload-time = "2025-09-14T22:18:09.753Z" },
+    { url = "https://files.pythonhosted.org/packages/1b/c0/59e912a531d91e1c192d3085fc0f6fb2852753c301a812d856d857ea03c6/zstandard-0.25.0-cp314-cp314-musllinux_1_2_ppc64le.whl", hash = "sha256:f67e8f1a324a900e75b5e28ffb152bcac9fbed1cc7b43f99cd90f395c4375344", size = 5430330, upload-time = "2025-09-14T22:18:11.966Z" },
+    { url = "https://files.pythonhosted.org/packages/a0/1d/7e31db1240de2df22a58e2ea9a93fc6e38cc29353e660c0272b6735d6669/zstandard-0.25.0-cp314-cp314-musllinux_1_2_s390x.whl", hash = "sha256:9654dbc012d8b06fc3d19cc825af3f7bf8ae242226df5f83936cb39f5fdc846c", size = 5811123, upload-time = "2025-09-14T22:18:13.907Z" },
+    { url = "https://files.pythonhosted.org/packages/f6/49/fac46df5ad353d50535e118d6983069df68ca5908d4d65b8c466150a4ff1/zstandard-0.25.0-cp314-cp314-musllinux_1_2_x86_64.whl", hash = "sha256:4203ce3b31aec23012d3a4cf4a2ed64d12fea5269c49aed5e4c3611b938e4088", size = 5359591, upload-time = "2025-09-14T22:18:16.465Z" },
+    { url = "https://files.pythonhosted.org/packages/c2/38/f249a2050ad1eea0bb364046153942e34abba95dd5520af199aed86fbb49/zstandard-0.25.0-cp314-cp314-win32.whl", hash = "sha256:da469dc041701583e34de852d8634703550348d5822e66a0c827d39b05365b12", size = 444513, upload-time = "2025-09-14T22:18:20.61Z" },
+    { url = "https://files.pythonhosted.org/packages/3a/43/241f9615bcf8ba8903b3f0432da069e857fc4fd1783bd26183db53c4804b/zstandard-0.25.0-cp314-cp314-win_amd64.whl", hash = "sha256:c19bcdd826e95671065f8692b5a4aa95c52dc7a02a4c5a0cac46deb879a017a2", size = 516118, upload-time = "2025-09-14T22:18:17.849Z" },
+    { url = "https://files.pythonhosted.org/packages/f0/ef/da163ce2450ed4febf6467d77ccb4cd52c4c30ab45624bad26ca0a27260c/zstandard-0.25.0-cp314-cp314-win_arm64.whl", hash = "sha256:d7541afd73985c630bafcd6338d2518ae96060075f9463d7dc14cfb33514383d", size = 476940, upload-time = "2025-09-14T22:18:19.088Z" },
+]
