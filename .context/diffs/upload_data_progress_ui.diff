diff --git "a/src/clinical_analytics/ui/pages/1_\360\237\223\244_Upload_Data.py" "b/src/clinical_analytics/ui/pages/1_\360\237\223\244_Upload_Data.py"
index 2e3577b..9167c49 100644
--- "a/src/clinical_analytics/ui/pages/1_\360\237\223\244_Upload_Data.py"
+++ "b/src/clinical_analytics/ui/pages/1_\360\237\223\244_Upload_Data.py"
@@ -5,23 +5,33 @@ Self-service data upload for clinicians.
 Upload CSV, Excel, or SPSS files without code or YAML configuration.
 """
 
-import streamlit as st
-import pandas as pd
+import logging
 import sys
 from pathlib import Path
-from typing import Optional
+
+import pandas as pd
+import streamlit as st
+
+# Configure logging for verbose output
+logging.basicConfig(
+    level=logging.INFO,
+    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
+    handlers=[
+        logging.StreamHandler(sys.stdout)
+    ]
+)
+
+# Set specific loggers to INFO/DEBUG for visibility
+logging.getLogger('clinical_analytics.core.multi_table_handler').setLevel(logging.INFO)
+logging.getLogger('clinical_analytics.ui.storage.user_datasets').setLevel(logging.INFO)
 
 # Add src to path
 sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))
 
-from clinical_analytics.ui.storage.user_datasets import (
-    UserDatasetStorage,
-    UploadSecurityValidator
-)
+from clinical_analytics.ui.components.data_validator import DataQualityValidator
 from clinical_analytics.ui.components.variable_detector import VariableTypeDetector
 from clinical_analytics.ui.components.variable_mapper import VariableMappingWizard
-from clinical_analytics.ui.components.data_validator import DataQualityValidator
-
+from clinical_analytics.ui.storage.user_datasets import UploadSecurityValidator, UserDatasetStorage
 
 # Page configuration
 st.set_page_config(
@@ -38,23 +48,29 @@ def render_upload_step():
     """Step 1: File Upload"""
     st.markdown("## ğŸ“¤ Upload Your Data")
     st.markdown("""
-    Upload your clinical dataset in CSV, Excel, or SPSS format.
+    Upload your clinical dataset in CSV, Excel, SPSS, or ZIP format.
 
     **Supported formats:**
-    - CSV (`.csv`)
-    - Excel (`.xlsx`, `.xls`)
-    - SPSS (`.sav`)
+    - CSV (`.csv`) - Single table
+    - Excel (`.xlsx`, `.xls`) - Single table
+    - SPSS (`.sav`) - Single table
+    - **ZIP (`.zip`) - Multi-table datasets** â­ NEW!
 
     **Requirements:**
     - File size: 1KB - 100MB
     - Must include patient ID column
     - Must include outcome variable
+
+    **Multi-Table ZIP Format:**
+    - ZIP file containing multiple CSV files
+    - Tables will be automatically joined (e.g., MIMIC-IV demo)
+    - Relationships detected via foreign keys
     """)
 
     uploaded_file = st.file_uploader(
         "Choose a file",
-        type=['csv', 'xlsx', 'xls', 'sav'],
-        help="Maximum file size: 100MB",
+        type=['csv', 'xlsx', 'xls', 'sav', 'zip'],
+        help="Maximum file size: 100MB. ZIP files for multi-table datasets.",
         key="file_uploader"
     )
 
@@ -80,10 +96,30 @@ def render_upload_step():
 
         st.success("âœ… File validation passed")
 
-        # Try to load preview
-        try:
-            file_ext = Path(uploaded_file.name).suffix.lower()
+        # Check if ZIP file (multi-table)
+        file_ext = Path(uploaded_file.name).suffix.lower()
+
+        if file_ext == '.zip':
+            # Handle multi-table ZIP upload
+            st.info("ğŸ—‚ï¸ **Multi-table dataset detected!** ZIP file validated.")
 
+            # Store ZIP upload details
+            st.session_state['is_zip_upload'] = True
+            st.session_state['uploaded_filename'] = uploaded_file.name
+            st.session_state['uploaded_bytes'] = file_bytes
+
+            st.success("âœ… ZIP file ready for processing")
+            st.info("ğŸ’¡ **Next:** Click 'Continue to Review' to process tables, detect relationships, and build unified cohort.")
+
+            # Button to proceed to review step
+            if st.button("Continue to Review â¡ï¸", type="primary"):
+                st.session_state['upload_step'] = 5
+                st.rerun()
+
+            return uploaded_file
+
+        # Try to load preview for single-table files
+        try:
             if file_ext == '.csv':
                 df = pd.read_csv(uploaded_file)
             elif file_ext in {'.xlsx', '.xls'}:
@@ -95,7 +131,8 @@ def render_upload_step():
                 st.error(f"Unsupported file type: {file_ext}")
                 return None
 
-            # Store in session state
+            # Store in session state (single-table)
+            st.session_state['is_zip_upload'] = False
             st.session_state['uploaded_df'] = df
             st.session_state['uploaded_filename'] = uploaded_file.name
             st.session_state['uploaded_bytes'] = file_bytes
@@ -309,10 +346,24 @@ def render_mapping_step(df: pd.DataFrame, variable_info: dict, suggestions: dict
                 st.rerun()
 
 
-def render_review_step(df: pd.DataFrame, mapping: dict, variable_info: dict):
+def render_review_step(df: pd.DataFrame = None, mapping: dict = None, variable_info: dict = None):
     """Step 5: Final Review & Save"""
     st.markdown("## âœ… Review & Save Dataset")
 
+    # Check if this is a ZIP upload (multi-table)
+    is_zip = st.session_state.get('is_zip_upload', False)
+
+    if is_zip:
+        # Handle multi-table ZIP upload
+        return render_zip_review_step()
+
+    # Single-table workflow (existing logic)
+    if df is None or mapping is None or variable_info is None:
+        st.warning("Missing required data. Please start over.")
+        st.session_state['upload_step'] = 1
+        st.rerun()
+        return
+
     # Run final validation with mapping
     patient_id_col = mapping['patient_id']
     outcome_col = mapping['outcome']
@@ -416,6 +467,159 @@ def render_review_step(df: pd.DataFrame, mapping: dict, variable_info: dict):
                 st.error(f"âŒ {message}")
 
 
+def render_zip_review_step():
+    """Step 5: Review & Save for Multi-Table ZIP Upload"""
+    st.markdown("### ğŸ—‚ï¸ Multi-Table Dataset Processing")
+
+    # Dataset name input
+    st.markdown("### ğŸ“ Dataset Name")
+    default_name = Path(st.session_state.get('uploaded_filename', 'dataset')).stem
+    dataset_name = st.text_input(
+        "Enter a name for this dataset",
+        value=default_name,
+        help="This name will be used to identify your dataset in the analysis interface"
+    )
+
+    # Process ZIP file
+    if st.button("ğŸš€ Process & Save Multi-Table Dataset", type="primary", disabled=not dataset_name.strip()):
+        # Create progress tracking UI elements
+        progress_bar = st.progress(0)
+        status_text = st.empty()
+        log_expander = st.expander("ğŸ“‹ Processing Log", expanded=True)
+        log_container = log_expander.container()
+
+
+        def progress_callback(step, total_steps, message, details):
+            """Update progress UI with current step information."""
+            progress = step / total_steps if total_steps > 0 else 0
+            progress_bar.progress(progress)
+            status_text.info(f"ğŸ”„ {message}")
+
+            # Add to log
+            with log_container:
+                if details:
+                    if 'table_name' in details:
+                        table_info = f"**{details['table_name']}**"
+                        if 'rows' in details:
+                            table_info += f" - {details['rows']:,} rows, {details['cols']} cols"
+                        if 'progress' in details:
+                            table_info += f" ({details['progress']})"
+                        st.text(f"âœ“ {table_info}")
+                    elif 'tables_found' in details:
+                        st.text(f"ğŸ“¦ Found {details['tables_found']} tables in ZIP")
+                        if 'table_names' in details:
+                            st.text(f"   Tables: {', '.join(details['table_names'][:5])}" +
+                                   (f" ... and {len(details['table_names']) - 5} more"
+                                    if len(details['table_names']) > 5 else ""))
+                    elif 'relationships' in details:
+                        st.text(f"ğŸ”— Detected {len(details['relationships'])} relationships")
+                        for rel in details['relationships'][:3]:  # Show first 3
+                            st.text(f"   â€¢ {rel}")
+                        if len(details['relationships']) > 3:
+                            st.text(f"   ... and {len(details['relationships']) - 3} more")
+                    else:
+                        st.text(f"â†’ {message}")
+                else:
+                    st.text(f"â†’ {message}")
+
+        # Prepare metadata
+        metadata = {
+            'dataset_name': dataset_name
+        }
+
+        # Save ZIP upload (this processes everything)
+        try:
+            success, message, upload_id = storage.save_zip_upload(
+                file_bytes=st.session_state['uploaded_bytes'],
+                original_filename=st.session_state['uploaded_filename'],
+                metadata=metadata,
+                progress_callback=progress_callback
+            )
+        except Exception as e:
+            import traceback
+            with log_container:
+                st.error(f"âŒ Error during processing: {str(e)}")
+                st.code(traceback.format_exc())
+            status_text.error(f"âŒ Processing failed: {str(e)}")
+            success = False
+            message = str(e)
+            upload_id = None
+
+        if success:
+            progress_bar.progress(1.0)
+            status_text.success(f"âœ… {message}")
+            with log_container:
+                st.success("âœ… Processing complete!")
+            st.balloons()
+
+            # Load metadata to show details
+            upload_metadata = storage.get_upload_metadata(upload_id)
+            if upload_metadata:
+                st.markdown("### ğŸ“Š Processing Summary")
+
+                col1, col2, col3 = st.columns(3)
+                with col1:
+                    st.metric("Tables Joined", upload_metadata.get('tables', []) and len(upload_metadata.get('tables', [])) or 0)
+                with col2:
+                    st.metric("Unified Rows", upload_metadata.get('row_count', 0))
+                with col3:
+                    st.metric("Total Columns", upload_metadata.get('column_count', 0))
+
+                # Show detected relationships
+                relationships = upload_metadata.get('relationships', [])
+                if relationships:
+                    with st.expander(f"ğŸ”— Detected Relationships ({len(relationships)})"):
+                        for rel in relationships:
+                            st.code(rel)
+
+                # Show tables
+                tables = upload_metadata.get('tables', [])
+                table_counts = upload_metadata.get('table_counts', {})
+                if tables:
+                    with st.expander(f"ğŸ“‹ Tables ({len(tables)})"):
+                        for table in tables:
+                            count = table_counts.get(table, 0)
+                            st.markdown(f"- **{table}**: {count:,} rows")
+
+                # Show inferred schema
+                inferred_schema = upload_metadata.get('inferred_schema', {})
+                if inferred_schema:
+                    with st.expander("ğŸ”¬ Inferred Schema"):
+                        if inferred_schema.get('column_mapping'):
+                            st.markdown("**Column Mappings:**")
+                            for col, role in inferred_schema['column_mapping'].items():
+                                st.markdown(f"- `{col}` â†’ {role}")
+
+                        if inferred_schema.get('outcomes'):
+                            st.markdown("**Outcomes:**")
+                            for outcome, config in inferred_schema['outcomes'].items():
+                                st.markdown(f"- `{outcome}` ({config.get('type', 'unknown')})")
+
+            st.markdown(f"""
+            **Dataset saved successfully!**
+
+            - **Upload ID:** `{upload_id}`
+            - **Name:** {dataset_name}
+            - **Format:** Multi-table (ZIP)
+
+            You can now use this dataset in the main analysis interface.
+            """)
+
+            # Clear session state
+            if st.button("Upload Another Dataset"):
+                for key in list(st.session_state.keys()):
+                    if key.startswith('upload') or key == 'is_zip_upload':
+                        del st.session_state[key]
+                st.rerun()
+        else:
+            st.error(f"âŒ {message}")
+
+    # Back button
+    if st.button("â¬…ï¸ Back to Upload"):
+        st.session_state['upload_step'] = 1
+        st.rerun()
+
+
 def main():
     """Main upload page logic"""
     st.title("ğŸ“¤ Upload Clinical Data")
@@ -474,7 +678,13 @@ def main():
             st.rerun()
 
     elif current_step == 5:
-        if all(k in st.session_state for k in ['uploaded_df', 'variable_mapping', 'variable_info']):
+        # Check if ZIP upload (skip to review directly)
+        is_zip = st.session_state.get('is_zip_upload', False)
+
+        if is_zip:
+            # For ZIP files, go directly to review (skip preview/detection/mapping)
+            render_review_step()
+        elif all(k in st.session_state for k in ['uploaded_df', 'variable_mapping', 'variable_info']):
             render_review_step(
                 st.session_state['uploaded_df'],
                 st.session_state['variable_mapping'],
