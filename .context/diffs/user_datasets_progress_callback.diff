diff --git a/src/clinical_analytics/ui/storage/user_datasets.py b/src/clinical_analytics/ui/storage/user_datasets.py
index 06e53ea..501550c 100644
--- a/src/clinical_analytics/ui/storage/user_datasets.py
+++ b/src/clinical_analytics/ui/storage/user_datasets.py
@@ -4,11 +4,13 @@ User Dataset Storage Manager
 Handles secure storage, metadata management, and persistence of uploaded datasets.
 """
 
-import json
 import hashlib
+import json
+from collections.abc import Callable
 from datetime import datetime
 from pathlib import Path
-from typing import Dict, List, Optional, Any
+from typing import Any
+
 import pandas as pd
 import polars as pl
 
@@ -137,7 +139,7 @@ class UserDatasetStorage:
     - Integration with existing registry system
     """
 
-    def __init__(self, upload_dir: Optional[Path] = None):
+    def __init__(self, upload_dir: Path | None = None):
         """
         Initialize storage manager.
 
@@ -177,8 +179,8 @@ class UserDatasetStorage:
         self,
         file_bytes: bytes,
         original_filename: str,
-        metadata: Dict[str, Any]
-    ) -> tuple[bool, str, Optional[str]]:
+        metadata: dict[str, Any]
+    ) -> tuple[bool, str, str | None]:
         """
         Save uploaded file with security validation.
 
@@ -221,6 +223,7 @@ class UserDatasetStorage:
             elif file_ext == '.sav':
                 # SPSS file - convert to CSV
                 import io
+
                 import pyreadstat
                 df, meta = pyreadstat.read_sav(io.BytesIO(file_bytes))
                 csv_path = self.raw_dir / f"{upload_id}.csv"
@@ -251,7 +254,7 @@ class UserDatasetStorage:
         except Exception as e:
             return False, f"Error saving upload: {str(e)}", None
 
-    def get_upload_metadata(self, upload_id: str) -> Optional[Dict[str, Any]]:
+    def get_upload_metadata(self, upload_id: str) -> dict[str, Any] | None:
         """
         Retrieve metadata for an upload.
 
@@ -266,10 +269,10 @@ class UserDatasetStorage:
         if not metadata_path.exists():
             return None
 
-        with open(metadata_path, 'r') as f:
+        with open(metadata_path) as f:
             return json.load(f)
 
-    def get_upload_data(self, upload_id: str) -> Optional[pd.DataFrame]:
+    def get_upload_data(self, upload_id: str) -> pd.DataFrame | None:
         """
         Load uploaded dataset.
 
@@ -286,7 +289,7 @@ class UserDatasetStorage:
 
         return pd.read_csv(csv_path)
 
-    def list_uploads(self) -> List[Dict[str, Any]]:
+    def list_uploads(self) -> list[dict[str, Any]]:
         """
         List all uploaded datasets.
 
@@ -296,7 +299,7 @@ class UserDatasetStorage:
         uploads = []
 
         for metadata_file in self.metadata_dir.glob("*.json"):
-            with open(metadata_file, 'r') as f:
+            with open(metadata_file) as f:
                 metadata = json.load(f)
                 uploads.append(metadata)
 
@@ -337,7 +340,7 @@ class UserDatasetStorage:
         except Exception as e:
             return False, f"Error deleting upload: {str(e)}"
 
-    def update_metadata(self, upload_id: str, metadata_updates: Dict[str, Any]) -> tuple[bool, str]:
+    def update_metadata(self, upload_id: str, metadata_updates: dict[str, Any]) -> tuple[bool, str]:
         """
         Update metadata for an upload.
 
@@ -355,7 +358,7 @@ class UserDatasetStorage:
 
         try:
             # Load existing metadata
-            with open(metadata_path, 'r') as f:
+            with open(metadata_path) as f:
                 metadata = json.load(f)
 
             # Update with new fields
@@ -374,8 +377,9 @@ class UserDatasetStorage:
         self,
         file_bytes: bytes,
         original_filename: str,
-        metadata: Dict[str, Any]
-    ) -> tuple[bool, str, Optional[str]]:
+        metadata: dict[str, Any],
+        progress_callback: Callable[[int, int, str, dict], None] | None = None
+    ) -> tuple[bool, str, str | None]:
         """
         Save uploaded ZIP file containing multiple CSV files.
 
@@ -386,14 +390,16 @@ class UserDatasetStorage:
             file_bytes: ZIP file content
             original_filename: Original filename
             metadata: Upload metadata
+            progress_callback: Optional callback function(step, total_steps, message, details)
 
         Returns:
             Tuple of (success, message, upload_id)
         """
+        import io
+        import zipfile
+
         from clinical_analytics.core.multi_table_handler import MultiTableHandler
         from clinical_analytics.core.schema_inference import SchemaInferenceEngine
-        import zipfile
-        import io
 
         # Security validation
         valid, error = UploadSecurityValidator.validate(original_filename, file_bytes)
@@ -404,41 +410,158 @@ class UserDatasetStorage:
         upload_id = self.generate_upload_id(original_filename)
 
         try:
+            import logging
+            logger = logging.getLogger(__name__)
+            logger.info(f"Starting ZIP upload processing: {original_filename}")
+
             # Extract ZIP contents
             zip_buffer = io.BytesIO(file_bytes)
-            tables: Dict[str, pl.DataFrame] = {}
+            tables: dict[str, pl.DataFrame] = {}
 
             with zipfile.ZipFile(zip_buffer, 'r') as zip_file:
-                # Get list of CSV files in ZIP
-                csv_files = [f for f in zip_file.namelist()
-                           if f.endswith('.csv') and not f.startswith('__MACOSX')]
+                # Get list of CSV files in ZIP (including .csv.gz in subdirectories)
+                csv_files = [
+                    f for f in zip_file.namelist()
+                    if (f.endswith('.csv') or f.endswith('.csv.gz'))
+                    and not f.startswith('__MACOSX')
+                    and not f.endswith('/')  # Skip directory entries
+                ]
 
                 if not csv_files:
                     return False, "No CSV files found in ZIP archive", None
 
+                logger.info(f"Found {len(csv_files)} CSV files in ZIP archive")
+
+                # Calculate total steps now that we know number of files
+                # 1 (found tables) + len(csv_files) (loading) + 4 (detect, build, save, infer)
+                total_steps = 1 + len(csv_files) + 4
+
+                if progress_callback:
+                    progress_callback(0, total_steps, "Initializing ZIP extraction...", {})
+                    progress_callback(1, total_steps, f"Found {len(csv_files)} tables to load", {
+                        'tables_found': len(csv_files),
+                        'table_names': [Path(f).stem for f in csv_files]
+                    })
+
                 # Load each CSV as a table
-                for csv_filename in csv_files:
+                for idx, csv_filename in enumerate(csv_files, start=1):
+                    # Extract table name (without path and extension)
                     table_name = Path(csv_filename).stem
+                    if table_name.endswith('.csv'):
+                        # Handle .csv.gz case where stem gives us "filename.csv"
+                        table_name = Path(table_name).stem
+
+                    logger.info(f"Loading table {idx}/{len(csv_files)}: {table_name} from {csv_filename}")
+
+                    if progress_callback:
+                        progress_callback(
+                            1 + idx,
+                            total_steps,
+                            f"Loading table: {table_name}",
+                            {
+                                'table_name': table_name,
+                                'file': csv_filename,
+                                'progress': f"{idx}/{len(csv_files)}"
+                            }
+                        )
+
+                    # Read file content
                     csv_content = zip_file.read(csv_filename)
 
-                    # Load as Polars DataFrame
-                    df = pl.read_csv(io.BytesIO(csv_content))
+                    # Handle gzip compression
+                    if csv_filename.endswith('.gz'):
+                        import gzip
+                        logger.debug(f"Decompressing gzip file: {csv_filename}")
+                        csv_content = gzip.decompress(csv_content)
+
+                    # Load as Polars DataFrame with robust schema inference
+                    # Use larger infer_schema_length to handle mixed-type columns (e.g., ICD codes)
+                    try:
+                        logger.debug(f"Reading CSV with schema inference for {table_name}")
+                        df = pl.read_csv(
+                            io.BytesIO(csv_content),
+                            infer_schema_length=10000,  # Scan more rows for better type inference
+                            try_parse_dates=True
+                        )
+                    except Exception as e:
+                        logger.warning(f"Schema inference failed for {table_name}, falling back to string types: {e}")
+                        # Fallback: read with all columns as strings, let DuckDB handle types
+                        df = pl.read_csv(
+                            io.BytesIO(csv_content),
+                            infer_schema_length=0  # Treat all as strings
+                        )
+
                     tables[table_name] = df
+                    logger.info(f"Loaded table '{table_name}': {df.height:,} rows, {df.width} cols")
+                    logger.debug(f"Schema for {table_name}: {dict(df.schema)}")
+
+                    if progress_callback:
+                        progress_callback(
+                            1 + idx,
+                            total_steps,
+                            f"Loaded {table_name}: {df.height:,} rows, {df.width} cols",
+                            {
+                                'table_name': table_name,
+                                'rows': df.height,
+                                'cols': df.width,
+                                'status': 'loaded'
+                            }
+                        )
+
+            logger.info(f"Extracted {len(tables)} tables from ZIP: {list(tables.keys())}")
 
             # Detect relationships between tables
+            # Step calculation: 1 (init) + len(csv_files) (loading) = current step
+            step_num = 1 + len(csv_files)
+            logger.info(f"Detecting relationships for {len(tables)} tables")
+
+            if progress_callback:
+                progress_callback(step_num, total_steps, "Detecting table relationships...", {
+                    'tables': list(tables.keys()),
+                    'table_counts': {name: df.height for name, df in tables.items()}
+                })
+
             handler = MultiTableHandler(tables)
             relationships = handler.detect_relationships()
+            logger.info(f"Detected {len(relationships)} relationships")
+
+            if relationships:
+                for rel in relationships:
+                    logger.info(f"Relationship: {rel}")
+
+            if progress_callback:
+                progress_callback(step_num + 1, total_steps, f"Detected {len(relationships)} relationships", {
+                    'relationships': [str(rel) for rel in relationships]
+                })
 
             # Build unified cohort
+            logger.info("Building unified cohort from detected relationships")
+            if progress_callback:
+                progress_callback(step_num + 2, total_steps, "Building unified cohort...", {})
+
             unified_df = handler.build_unified_cohort()
+            logger.info(f"Unified cohort created: {unified_df.height:,} rows, {unified_df.width} cols")
 
             # Save unified cohort as CSV
             csv_path = self.raw_dir / f"{upload_id}.csv"
+            logger.info(f"Saving unified cohort to {csv_path}")
+            if progress_callback:
+                progress_callback(
+                    step_num + 3,
+                    total_steps,
+                    f"Saving unified cohort ({unified_df.height:,} rows)...",
+                    {}
+                )
             unified_df.write_csv(csv_path)
 
             # Infer schema for unified cohort
+            logger.info("Inferring schema for unified cohort")
+            if progress_callback:
+                progress_callback(step_num + 4, total_steps, "Inferring schema...", {})
+
             engine = SchemaInferenceEngine()
             schema = engine.infer_schema(unified_df)
+            logger.info("Schema inference complete")
 
             # Save metadata
             full_metadata = {
@@ -463,7 +586,20 @@ class UserDatasetStorage:
 
             handler.close()
 
-            return True, f"Multi-table upload successful: {len(tables)} tables joined into {unified_df.height} rows", upload_id
+            if progress_callback:
+                progress_callback(step_num + 4, total_steps, "Processing complete!", {
+                    'tables': len(tables),
+                    'rows': unified_df.height,
+                    'cols': unified_df.width
+                })
+
+            logger.info(f"Multi-table upload successful: {len(tables)} tables joined into {unified_df.height:,} rows")
+            return True, f"Multi-table upload successful: {len(tables)} tables joined into {unified_df.height:,} rows", upload_id
 
         except Exception as e:
+            import logging
+            import traceback
+            logger = logging.getLogger(__name__)
+            logger.error(f"Error processing ZIP upload: {type(e).__name__}: {str(e)}")
+            logger.error(f"Traceback: {traceback.format_exc()}")
             return False, f"Error processing ZIP upload: {str(e)}", None
