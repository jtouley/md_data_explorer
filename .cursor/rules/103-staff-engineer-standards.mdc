---
description: Staff-level data engineering standards. Enforces production-grade patterns for error handling, observability, idempotency, and operational excellence. Always use Makefile commands for code quality checks.
globs: ["**/*.py"]
alwaysApply: true
---

# Staff Engineer Standards

## The Staff Mindset

Staff engineers build **systems that build systems**. Your code is read by others, maintained for years, and runs at 3 AM when you're asleep. Design accordingly.

## Error Handling

### Fail Fast, Fail Loud

```python
# WRONG: Silent failures
def load_data(path: str) -> pl.DataFrame | None:
    try:
        return pl.read_parquet(path)
    except Exception:
        return None  # Caller has no idea what happened

# CORRECT: Explicit, typed failures
from dataclasses import dataclass

@dataclass
class LoadError:
    path: str
    reason: str
    original_error: Exception | None = None

def load_data(path: str) -> pl.DataFrame:
    """Load parquet file. Raises LoadError with context on failure."""
    if not Path(path).exists():
        raise LoadError(path, "File not found")
    
    try:
        return pl.read_parquet(path)
    except pl.exceptions.ComputeError as e:
        raise LoadError(path, "Corrupted parquet file", e) from e
```

### Exception Hierarchy

```python
# Define domain-specific exceptions
class PipelineError(Exception):
    """Base exception for pipeline failures."""
    pass

class SchemaValidationError(PipelineError):
    """Data does not match expected schema."""
    def __init__(self, missing: set, type_mismatches: dict):
        self.missing = missing
        self.type_mismatches = type_mismatches
        super().__init__(f"Missing: {missing}, Type mismatches: {type_mismatches}")

class DataQualityError(PipelineError):
    """Data fails quality checks."""
    def __init__(self, check_name: str, failure_count: int, sample: pl.DataFrame):
        self.check_name = check_name
        self.failure_count = failure_count
        self.sample = sample
        super().__init__(f"{check_name} failed: {failure_count} rows")
```

### Never Catch Exception

```python
# WRONG: Catches everything, including bugs
try:
    result = complex_transform(df)
except Exception as e:
    logger.error(f"Transform failed: {e}")
    return fallback_value

# CORRECT: Catch specific, expected failures
try:
    result = complex_transform(df)
except pl.exceptions.SchemaError as e:
    logger.error(f"Schema mismatch: {e}")
    raise
except pl.exceptions.ComputeError as e:
    logger.error(f"Compute error during transform: {e}")
    raise
# Let unexpected exceptions propagate
```

## Observability

### Structured Logging

```python
import structlog

logger = structlog.get_logger()

def process_batch(batch_id: str, source_path: str) -> int:
    log = logger.bind(batch_id=batch_id, source_path=source_path)
    
    log.info("batch_processing_started")
    
    df = pl.read_parquet(source_path)
    row_count = len(df)
    log.info("batch_loaded", row_count=row_count)
    
    result = transform(df)
    output_count = len(result)
    log.info(
        "batch_processing_completed",
        input_rows=row_count,
        output_rows=output_count,
        delta=output_count - row_count,
    )
    
    return output_count
```

### Metrics That Matter

```python
from prometheus_client import Counter, Histogram, Gauge

# Counters for events
rows_processed = Counter(
    "pipeline_rows_processed_total",
    "Total rows processed",
    ["pipeline", "stage"],
)

# Histograms for latency
stage_duration = Histogram(
    "pipeline_stage_duration_seconds",
    "Time spent in each pipeline stage",
    ["pipeline", "stage"],
    buckets=[0.1, 0.5, 1.0, 5.0, 10.0, 30.0, 60.0],
)

# Gauges for current state
backlog_size = Gauge(
    "pipeline_backlog_files",
    "Number of files waiting to be processed",
    ["pipeline"],
)

# Usage
with stage_duration.labels(pipeline="customers", stage="transform").time():
    result = transform(df)
    rows_processed.labels(pipeline="customers", stage="transform").inc(len(result))
```

## Idempotency

### Deterministic Processing

```python
def process_file(file_path: str, processing_date: date) -> pl.DataFrame:
    """
    Idempotent file processing.
    
    Same inputs always produce same outputs:
    - No random values
    - No current timestamps (use processing_date)
    - Deterministic ordering
    """
    return (
        pl.scan_parquet(file_path)
        .with_columns(
            pl.lit(processing_date).alias("processing_date"),  # Deterministic
            pl.lit(file_path).alias("source_file"),
        )
        .sort("id")  # Deterministic order
        .collect()
    )
```

### Idempotent Writes

```python
def upsert_to_delta(
    df: pl.DataFrame,
    table_path: str,
    merge_keys: list[str],
) -> None:
    """
    Idempotent upsert: running twice with same data = no change.
    """
    from deltalake import DeltaTable, write_deltalake
    
    if not DeltaTable.is_deltatable(table_path):
        write_deltalake(table_path, df.to_arrow())
        return
    
    dt = DeltaTable(table_path)
    merge_predicate = " AND ".join(
        f"source.{k} = target.{k}" for k in merge_keys
    )
    
    (
        dt.merge(
            source=df.to_arrow(),
            predicate=merge_predicate,
            source_alias="source",
            target_alias="target",
        )
        .when_matched_update_all()
        .when_not_matched_insert_all()
        .execute()
    )
```

## Defensive Programming

### Validate at Boundaries

```python
def ingest_external_data(api_response: dict) -> pl.DataFrame:
    """Validate untrusted external data at ingestion boundary."""
    
    # Validate structure
    required_fields = {"records", "metadata"}
    missing = required_fields - set(api_response.keys())
    if missing:
        raise ValueError(f"API response missing fields: {missing}")
    
    # Validate types
    if not isinstance(api_response["records"], list):
        raise TypeError("records must be a list")
    
    # Parse and validate schema
    df = pl.DataFrame(api_response["records"])
    
    # Validate business rules
    if (df["amount"] < 0).any():
        raise DataQualityError("negative_amounts", (df["amount"] < 0).sum(), df)
    
    return df
```

### Assertion-Based Invariants

```python
def merge_datasets(
    left: pl.DataFrame,
    right: pl.DataFrame,
    join_key: str,
) -> pl.DataFrame:
    """Merge with invariant checks."""
    
    # Pre-conditions
    assert join_key in left.columns, f"{join_key} not in left"
    assert join_key in right.columns, f"{join_key} not in right"
    assert left[join_key].null_count() == 0, "Nulls in left join key"
    assert right[join_key].null_count() == 0, "Nulls in right join key"
    
    left_count = len(left)
    result = left.join(right, on=join_key, how="left")
    
    # Post-conditions: left join should preserve left row count
    assert len(result) == left_count, (
        f"Row count changed: {left_count} -> {len(result)}. "
        "Right side has duplicates on join key."
    )
    
    return result
```

## Performance Discipline

### Profile Before Optimizing

```python
# Add timing to understand bottlenecks
import time
from contextlib import contextmanager

@contextmanager
def timed_stage(stage_name: str):
    start = time.perf_counter()
    yield
    elapsed = time.perf_counter() - start
    logger.info(f"{stage_name} completed", duration_seconds=elapsed)

# Usage
with timed_stage("transform"):
    df = heavy_transform(raw_data)

with timed_stage("write"):
    df.write_parquet(output_path)
```

### Memory Awareness

```python
def process_large_dataset(path_pattern: str, chunk_size: int = 100_000) -> None:
    """
    Process large datasets without OOM.
    Uses streaming when possible, chunking when not.
    """
    lf = pl.scan_parquet(path_pattern)
    
    # Try streaming first
    try:
        result = lf.collect(streaming=True)
        result.write_parquet("output.parquet")
        return
    except pl.exceptions.ComputeError:
        logger.warning("Streaming failed, falling back to chunked processing")
    
    # Chunked fallback
    total_rows = lf.select(pl.len()).collect().item()
    
    for offset in range(0, total_rows, chunk_size):
        chunk = lf.slice(offset, chunk_size).collect()
        mode = "overwrite" if offset == 0 else "append"
        chunk.write_parquet(f"output/chunk_{offset}.parquet")
```

## Documentation Standards

### Docstrings with Purpose

```python
def calculate_customer_ltv(
    transactions: pl.DataFrame,
    as_of_date: date,
    discount_rate: float = 0.1,
) -> pl.DataFrame:
    """
    Calculate customer lifetime value using discounted cash flow.
    
    Uses a simplified DCF model where future value is projected based on
    historical transaction patterns and discounted to present value.
    
    Args:
        transactions: Must contain columns: customer_id, amount, transaction_date
        as_of_date: Reference date for calculations
        discount_rate: Annual discount rate (default 10%)
    
    Returns:
        DataFrame with columns: customer_id, ltv, confidence
        
    Raises:
        SchemaValidationError: If required columns are missing
        
    Example:
        >>> ltv = calculate_customer_ltv(txns, date(2024, 1, 1))
        >>> top_customers = ltv.filter(pl.col("ltv") > 10000)
        
    Note:
        LTV calculations require at least 3 months of transaction history.
        Customers with insufficient history will have confidence='low'.
    """
```

## Code Review Checklist

Before approving any PR:

1. **Error handling**: Are failures explicit and recoverable?
2. **Observability**: Can we debug this at 3 AM?
3. **Idempotency**: Is it safe to run twice?
4. **Boundaries**: Is external data validated at entry?
5. **Performance**: Any unbounded operations?
6. **Testing**: Are edge cases covered?
7. **Documentation**: Can someone new understand the why?
8. **Code quality**: Run `make check` - all checks must pass

## Pre-Commit Verification

**Before committing code, always run:**
```bash
make format        # Auto-format code
make lint-fix      # Auto-fix linting issues
make check         # Full check: lint, format-check, type-check, test
```

**If checks fail:**
- Fix issues before committing
- Never commit code that fails `make check`
- Use `make check-fast` for quicker feedback during development

See `.cursor/rules/000-project-setup-and-makefile.mdc` for full Makefile reference.
