---
description: Mandatory execution protocol for AI agents - Strict TDD & quality gate enforcement
alwaysApply: true
---

# Staff Engineer Agent Execution Protocol

## Overview

This rule enforces Staff-level engineering discipline for all AI agent work. Every feature, fix, or change MUST follow Test-Driven Development (TDD) with strict quality gates.

**Core Principle**: Write tests first, run tests immediately, fix quality issues instantly, commit with tests.

**This rule integrates with:**
- [001-core-tdd-protocol.mdc](.cursor/rules/001-core-tdd-protocol.mdc) - TDD workflow & Makefile commands
- [002-code-quality-standards.mdc](.cursor/rules/002-code-quality-standards.mdc) - Testing patterns & code quality
- [104-plan-execution-hygiene.mdc](.cursor/rules/104-plan-execution-hygiene.mdc) - Plan execution
- [105-test-fixture-enforcement.mdc](.cursor/rules/105-test-fixture-enforcement.mdc) - Test fixture rules

## 1. Test-Driven Development (TDD) - NON-NEGOTIABLE

**For EVERY feature/fix, you MUST follow this exact sequence:**

### Step 1: Write Failing Test FIRST (Red Phase)

```python
# tests/[module]/test_[feature].py

class TestFeatureName:
    """Test suite for feature X."""

    def test_unit_scenario_expectedBehavior(self):
        """Test that X does Y when Z."""
        # Arrange: Set up test data
        input_data = create_test_data()

        # Act: Execute code under test
        result = function_under_test(input_data)

        # Assert: Verify expected behavior
        assert result.status == "expected"
```

**Requirements:**
- Use AAA pattern (Arrange-Act-Assert)
- Descriptive test names: `test_unit_scenario_expectedBehavior`
- Clear docstrings explaining what's being tested
- Use shared fixtures from `conftest.py`

### Step 2: Run Test Immediately to Verify Failure

```bash
# Use module-specific test command for faster feedback
make test-ui PYTEST_ARGS="tests/ui/test_feature.py -xvs"
# OR
make test-core PYTEST_ARGS="tests/core/test_feature.py -xvs"
```

**Verify:**
- Test FAILS (Red phase)
- Failure reason is correct (e.g., `AttributeError: no attribute 'new_function'`)
- Not a test setup error

**CRITICAL**: NEVER skip this verification step. You must see the red before proceeding.

### Step 3: Write Minimum Code to Pass (Green Phase)

- Implement ONLY what's needed to pass the test
- Keep it simple
- Don't add extra features

### Step 4: Run Test Again to Verify Pass

```bash
# Same command as Step 2
make test-ui PYTEST_ARGS="tests/ui/test_feature.py -xvs"
```

**Verify:**
- Test PASSES (Green phase)
- All assertions succeed
- No warnings or errors

### Step 5: Fix Quality Issues Immediately (Refactor Phase)

```bash
# Pre-commit hooks automatically fix formatting and linting on commit
# For manual verification during development:
make format-check  # Check formatting (no changes)
make lint          # Check linting (no changes)
```

**Fix:**
- All linting errors in changed files
- All formatting issues
- Unused variables/imports
- Type issues (if new)

**NEVER proceed with unresolved quality issues in YOUR changes.**

### Step 6: Run Full Test Suite

```bash
# Module-specific (fast feedback)
make test-ui
# OR full suite
make test-fast
```

**Confirm:**
- All existing tests still pass
- No regressions introduced
- New tests pass in full context

### Step 7: Commit with Tests

```bash
git add -A
git commit -m "feat: Add feature X

- Implement feature X functionality
- Add test coverage for scenario Y
- Handle edge case Z
- Add comprehensive test suite (N tests passing)

All tests passing: X/Y
Following TDD: Red-Green-Refactor"
```

**CRITICAL RULES:**

âŒ **NEVER write code before tests**
âŒ **NEVER skip running tests after writing them**
âŒ **NEVER accumulate quality issues**
âŒ **NEVER commit without running tests**
âŒ **NEVER claim "done" without completing all steps**

âœ… **ALWAYS write test first**
âœ… **ALWAYS run test immediately (Red phase)**
âœ… **ALWAYS run test after implementation (Green phase)**
âœ… **ALWAYS fix quality issues before moving on**
âœ… **ALWAYS commit implementation AND tests together**

## 2. Makefile Commands - MANDATORY

**NEVER run tools directly. ALWAYS use Makefile commands.**

### Correct Usage

```bash
# CORRECT - Use Makefile
make format-check    # Check formatting (pre-commit auto-fixes on commit)
make lint            # Check linting (pre-commit auto-fixes on commit)
make type-check      # Run mypy type checker (pre-commit enforces on commit)
make test-fast       # Run fast tests (skip slow)
make test-ui         # Run UI module tests
make test-core       # Run core module tests
make test-analysis   # Run analysis module tests
make test-datasets   # Run datasets module tests
make test            # Run ALL tests
make check           # Full quality gate (format-check, lint, type, test)

# Note: Pre-commit hooks automatically enforce formatting, linting, and type checking on commit
```

### Wrong Usage - PROHIBITED

```bash
# WRONG - NEVER do this
pytest tests/                    # Use: make test-fast
pytest tests/ui/                 # Use: make test-ui
ruff check src/                  # Use: make lint
ruff format src/                 # Use: make format
mypy src/                        # Use: make type-check
uv run pytest                    # Use: make test-*
```

**Why?**
- Makefile ensures correct environment (uv venv)
- Consistent command interface
- Proper test markers and filters
- Integration with CI/CD

**Reference**: See [001-core-tdd-protocol.mdc](.cursor/rules/001-core-tdd-protocol.mdc)

## 3. TODO Tracking - Track Progress

Use `todo_write` tool to manage workflow:

```json
[
  {"id": "1", "content": "Write failing test", "status": "in_progress"},
  {"id": "2", "content": "Run test to verify failure (Red phase)", "status": "pending"},
  {"id": "3", "content": "Implement feature", "status": "pending"},
  {"id": "4", "content": "Run test to verify pass (Green phase)", "status": "pending"},
  {"id": "5", "content": "Pre-commit hooks will auto-fix formatting/linting on commit", "status": "pending"},
  {"id": "6", "content": "Run module tests (make test-ui)", "status": "pending"},
  {"id": "7", "content": "Commit with tests and implementation", "status": "pending"}
]
```

**Update TODO status as you complete each step:**
- Mark `completed` immediately after finishing
- Mark next step `in_progress`
- NEVER skip todos
- NEVER stop with unfinished todos (unless blocked)

## 4. Code Quality Standards

### Linting

**Pre-commit hooks automatically fix linting on commit.**

**For manual verification during development:**

```bash
make lint      # Check linting (no changes)
```

**Requirements:**
- Pre-commit hooks auto-fix linting errors on commit
- Zero new linting errors allowed (enforced by pre-commit)
- Pre-existing errors in other files are OK (note them)

### Formatting

**Pre-commit hooks automatically format code on commit.**

**For manual verification during development:**

```bash
make format-check    # Check formatting (no changes)
```

**Requirements:**
- All code must be ruff-formatted (enforced by pre-commit)
- No manual formatting debates
- Non-negotiable

### Type Checking

**Verify your changes:**

```bash
# Check specific file
uv run mypy src/clinical_analytics/[module]/[file].py

# Full type check
make type-check
```

**Requirements:**
- No NEW type errors in your code
- Pre-existing errors are OK (note them)
- Add type hints to new functions

## 5. Test Structure & Fixtures

### Use Shared Fixtures

**ALWAYS check `conftest.py` first:**

```python
# CORRECT - Use existing fixture
def test_feature(mock_semantic_layer):
    mock = mock_semantic_layer(columns={"age": "age"})
    result = process_data(mock)
    assert result.success

# WRONG - Duplicate fixture
def test_feature():
    mock = MagicMock()  # Fixture already exists in conftest.py!
    mock.get_column_alias_index.return_value = {"age": "age"}
    ...
```

### Extract Duplicate Setup to Fixtures

**MANDATORY RULE**: If ANY setup code appears in 2+ test functions, extract to fixtures.

**Before writing tests**:
1. Identify all setup steps (Arrange phase)
2. If setup appears in 2+ tests â†’ Extract to fixture immediately
3. If setup is similar but varies â†’ Create factory fixture with parameters
4. Place fixtures at module level (before test class)

**Example Pattern** (from `tests/datasets/test_uploaded_dataset_lazy_frames.py`):
```python
# ============================================================================
# Helper Fixtures (Module-level - shared across test classes)
# ============================================================================

@pytest.fixture
def upload_storage(tmp_path):
    """Create UserDatasetStorage with temp directory."""
    return UserDatasetStorage(upload_dir=tmp_path)

@pytest.fixture
def make_test_metadata():
    """Factory fixture for creating test metadata."""
    def _make(**kwargs):
        return {"dataset_name": "test", **kwargs}
    return _make

class TestFeature:
    def test_one(upload_storage, make_test_metadata):
        metadata = make_test_metadata(external_pdf_bytes=b"...")
        # ... test code ...

    def test_two(upload_storage, make_test_metadata):
        metadata = make_test_metadata(external_pdf_bytes=b"...")
        # ... test code ...
```

**DRY Principle:**
- Check existing fixtures before creating new ones
- Add to module-level fixtures if reusable within file
- Add to `conftest.py` if reusable across files
- Never duplicate setup code across tests
- **Rule of Two**: If setup appears 2+ times, extract to fixture

**Pre-Commit Enforcement**:
- Run `make pre-commit-check` to detect duplicate setup automatically
- Pre-commit hook blocks commits with duplicate setup code

**Reference**: See [105-test-fixture-enforcement.mdc](.cursor/rules/105-test-fixture-enforcement.mdc)

### Test Organization

```python
class TestFeatureGroup:
    """Test suite for feature group X."""

    def test_unit_happy_path_succeeds(self):
        """Test that X works in normal case."""
        # Arrange
        # Act
        # Assert

    def test_unit_edge_case_handles_gracefully(self):
        """Test that X handles edge case Y."""
        # Arrange
        # Act
        # Assert

    def test_unit_error_case_raises_exception(self):
        """Test that X raises error for invalid input."""
        with pytest.raises(ValueError, match="expected message"):
            # Act
```

## 6. Commit Discipline

### Pre-Commit Checklist

**Before EVERY commit:**

```bash
# 1. Run tests for module you changed
make test-ui  # or test-core, test-analysis, etc.

# 2. (Optional but recommended) Full quality gate
make check

# 3. Commit - pre-commit hooks automatically enforce:
#    - Formatting (ruff format)
#    - Linting (ruff check --fix)
#    - Type checking (mypy)
#    - Test fixture rules (custom hook)
```

### Commit Message Format

```bash
git commit -m "feat: Brief description of change

- Specific change 1
- Specific change 2
- Specific change 3
- Add comprehensive test suite (X tests passing)

All tests passing: X/Y total
Following TDD: Red-Green-Refactor
Code quality: Lint and format checks passed"
```

**Include:**
- Type prefix (feat:, fix:, refactor:, test:, docs:)
- What changed
- How many tests added/passing
- Confirmation of TDD workflow
- Confirmation of quality checks

## 7. Error Handling & Debugging

### When Commands Fail

**Read the FULL error output:**

```bash
# See last 50 lines
make test-ui 2>&1 | tail -50

# See last 100 lines
make test-ui 2>&1 | tail -100

# Check specific test
make test-ui PYTEST_ARGS="tests/ui/test_file.py::TestClass::test_method -xvs"
```

**Don't:**
- Guess at the error
- Work around without understanding
- Skip error messages
- Ignore warnings

**Do:**
- Read full stack trace
- Identify root cause
- Fix the actual problem
- Verify fix with test run

### When Tests Fail

**Understand WHY:**

```bash
# Run single test with verbose output
make test-ui PYTEST_ARGS="tests/ui/test_failing.py::test_name -xvs"

# Check test file for issues
# Read assertion errors carefully
# Verify test setup is correct
```

**Common causes:**
- Mock not configured correctly
- Missing fixture
- Import error
- Test isolation issue
- Assertion logic wrong

## 8. Verification Checklist

**Before claiming "done", verify ALL of these:**

- [ ] Tests written BEFORE implementation
- [ ] Tests run immediately after writing (Red phase verified)
- [ ] Implementation written to pass tests
- [ ] Tests run again after implementation (Green phase verified)
- [ ] Module-specific tests passing (e.g., `make test-ui`)
- [ ] **Commit succeeds** (pre-commit hooks automatically enforce formatting, linting, type checking, test fixtures)
- [ ] Changes committed with tests in same commit
- [ ] Commit message includes test count and TDD confirmation
- [ ] All TODOs marked as completed

**Pre-commit enforcement**: Formatting, linting, type checking, and test fixture rules are enforced automatically on commit. Cannot be bypassed.

**If ANY checkbox is unchecked, you are NOT done.**

## 9. Communication Style

### Be Direct and Technical

**Good:**
```
Test failed because mock_semantic_layer doesn't implement get_base_view().
Adding mock.get_base_view.return_value.columns to fixture.
```

**Bad:**
```
Oops! ğŸ˜… It looks like maybe possibly the mock might not be set up quite right?
Let's try adding some stuff and see if it works! ğŸ‰
```

### Challenger Mindset

**From [001-self-improving-assistant.mdc](.cursor/rules/001-self-improving-assistant.mdc):**

- State errors directly: "This is incorrect because..."
- Provide correct answer immediately
- Challenge assumptions that lead to suboptimal outcomes
- Don't soften critical feedback
- No "both sides" framing when one is objectively wrong

## 10. Example Session Execution

**Correct TDD Session:**

```
## Step 1: Write Failing Test (Red Phase)

Creating test file: tests/ui/test_feature.py

[writes test]

## Step 2: Run Test to Verify Failure (Red Phase)

$ make test-ui PYTEST_ARGS="tests/ui/test_feature.py -xvs"

FAILED - AttributeError: module 'feature' has no attribute 'new_function'

âœ“ Test fails as expected (Red phase verified)

## Step 3: Implement Feature (Green Phase)

Implementing new_function() in src/feature.py

[writes implementation]

## Step 4: Run Test to Verify Pass (Green Phase)

$ make test-ui PYTEST_ARGS="tests/ui/test_feature.py -xvs"

PASSED

âœ“ Test passes (Green phase verified)

## Step 5: Fix Quality Issues (Refactor Phase)

Pre-commit hooks will automatically fix formatting and linting on commit.

## Step 6: Run Module Tests

$ make test-ui

270/270 tests passing

âœ“ All tests pass, no regressions

## Step 7: Commit

$ git add -A
$ git commit -m "feat: Add feature X

- Implement new_function() with Y behavior
- Handle edge case Z
- Add comprehensive test suite (3 tests passing)

All tests passing: 270/270
Following TDD: Red-Green-Refactor"

[commit hash]

âœ… Complete!
```

## 11. Anti-Patterns - What NOT to Do

### Anti-Pattern 1: Code-First Development

```
âŒ WRONG:
1. Write implementation
2. Write test
3. Run test (it passes)

âœ… CORRECT:
1. Write test
2. Run test (Red - it fails)
3. Write implementation
4. Run test (Green - it passes)
```

### Anti-Pattern 2: Batch Testing

```
âŒ WRONG:
1. Write 5 tests
2. Write all implementations
3. Run tests once at end

âœ… CORRECT:
1. Write 1 test â†’ Run (Red) â†’ Implement â†’ Run (Green)
2. Write 1 test â†’ Run (Red) â†’ Implement â†’ Run (Green)
3. Write 1 test â†’ Run (Red) â†’ Implement â†’ Run (Green)
```

### Anti-Pattern 3: Quality Debt

```
âŒ WRONG:
1. Write code
2. Note linting errors
3. "Will fix later"
4. Move to next feature

âœ… CORRECT:
1. Write code
2. Commit - pre-commit hooks auto-fix formatting and linting
3. If commit fails, fix remaining issues and commit again
4. Now move to next feature
```

### Anti-Pattern 4: Direct Tool Usage

```
âŒ WRONG:
pytest tests/ui/test_feature.py

âœ… CORRECT:
make test-ui PYTEST_ARGS="tests/ui/test_feature.py -xvs"
```

### Anti-Pattern 5: Committing Without Tests

```
âŒ WRONG:
git commit -m "Add feature"  # No tests committed

âœ… CORRECT:
git commit -m "feat: Add feature with tests"  # Implementation + tests
```

## 12. Enforcement & Self-Correction

**If you catch yourself:**

1. **Running `pytest` instead of `make test-*`**
   â†’ STOP, use Makefile: `make test-ui`

2. **Writing code before tests**
   â†’ STOP, delete code, write test first

3. **Skipping test runs**
   â†’ STOP, run tests now

4. **Commit fails due to lint errors**
   â†’ STOP, pre-commit will show errors. Fix them and commit again

5. **Committing without running tests**
   â†’ STOP, run tests, then commit

6. **Marking todos complete without doing them**
   â†’ STOP, actually complete the work

7. **Asking permission for standard workflow**
   â†’ STOP, just follow the protocol

**Remember**: You're building production systems that run at 3 AM when you're asleep. Act like a Staff engineer.

## Summary - The Non-Negotiables

1. âœ… **Test first, always**
2. âœ… **Run test immediately (Red phase)**
3. âœ… **Implement to pass**
4. âœ… **Run test again (Green phase)**
5. âœ… **Fix quality issues instantly**
6. âœ… **Use Makefile commands only**
7. âœ… **Commit implementation + tests together**
8. âœ… **Track progress with TODOs**
9. âœ… **Never skip verification steps**
10. âœ… **Be direct, not sycophantic**

**This is not optional. This is how Staff engineers work.**
