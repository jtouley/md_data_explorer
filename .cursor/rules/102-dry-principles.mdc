---
description: DRY principles and code organization for data engineering. Enforces single source of truth, configuration-driven patterns, and abstraction discipline.
globs: ["**/*.py"]
alwaysApply: true
---

# DRY and Code Organization

## The DRY Mandate

Don't Repeat Yourself is not about typing less. It's about **single source of truth**. When a business rule exists in two places, they will diverge.

## Configuration Over Code

### Extract Magic Values

```python
# WRONG: Magic values scattered in code
def filter_active_customers(df):
    return df.filter(
        (pl.col("status") == "active") &
        (pl.col("days_since_last_order") < 90) &
        (pl.col("total_spend") > 100)
    )

# CORRECT: Configuration-driven
from dataclasses import dataclass

@dataclass(frozen=True)
class ActiveCustomerCriteria:
    status: str = "active"
    max_days_inactive: int = 90
    min_total_spend: float = 100.0

def filter_active_customers(
    df: pl.DataFrame,
    criteria: ActiveCustomerCriteria = ActiveCustomerCriteria(),
) -> pl.DataFrame:
    return df.filter(
        (pl.col("status") == criteria.status) &
        (pl.col("days_since_last_order") < criteria.max_days_inactive) &
        (pl.col("total_spend") > criteria.min_total_spend)
    )
```

### Centralize Column Definitions

```python
# columns.py - Single source of truth for column names
class Columns:
    # Raw layer
    CUSTOMER_ID = "customer_id"
    TRANSACTION_DATE = "transaction_date"
    AMOUNT = "amount"
    
    # Derived
    TOTAL_SPEND = "total_spend"
    DAYS_SINCE_LAST_ORDER = "days_since_last_order"
    
    # Standardized audit columns
    LOADED_AT = "_loaded_at"
    SOURCE_FILE = "_source_file"


# Usage
from columns import Columns as C

df.select(
    pl.col(C.CUSTOMER_ID),
    pl.col(C.AMOUNT).sum().alias(C.TOTAL_SPEND),
)
```

### Schema Definitions

```python
# schemas.py
CUSTOMER_SCHEMA = {
    "customer_id": pl.Utf8,
    "email": pl.Utf8,
    "created_at": pl.Datetime("us"),
    "is_active": pl.Boolean,
}

TRANSACTION_SCHEMA = {
    "transaction_id": pl.Utf8,
    "customer_id": pl.Utf8,
    "amount": pl.Decimal(18, 2),
    "timestamp": pl.Datetime("us"),
}

# Reuse in readers
def read_customers(path: str) -> pl.LazyFrame:
    return pl.scan_parquet(path).cast(CUSTOMER_SCHEMA)
```

## Expression Libraries

### Build Reusable Expressions

```python
# expressions.py
import polars as pl

class DateExpressions:
    @staticmethod
    def fiscal_quarter(date_col: str = "date") -> pl.Expr:
        """Convert date to fiscal quarter (July start)."""
        month = pl.col(date_col).dt.month()
        return (
            pl.when(month >= 7)
            .then(((month - 7) // 3) + 1)
            .otherwise(((month + 5) // 3) + 1)
        )
    
    @staticmethod
    def days_since(date_col: str, as_of: date | None = None) -> pl.Expr:
        """Calculate days since date column."""
        reference = pl.lit(as_of) if as_of else pl.lit(date.today())
        return (reference - pl.col(date_col)).dt.total_days()


class MoneyExpressions:
    @staticmethod
    def to_cents(amount_col: str) -> pl.Expr:
        """Convert decimal dollars to integer cents."""
        return (pl.col(amount_col) * 100).cast(pl.Int64)
    
    @staticmethod
    def from_cents(cents_col: str) -> pl.Expr:
        """Convert integer cents to decimal dollars."""
        return (pl.col(cents_col) / 100).cast(pl.Decimal(18, 2))


# Usage
from expressions import DateExpressions as DX, MoneyExpressions as MX

df.with_columns(
    DX.fiscal_quarter("order_date").alias("fiscal_qtr"),
    DX.days_since("last_login").alias("days_inactive"),
    MX.to_cents("price").alias("price_cents"),
)
```

## Transform Functions

### Pure Functions with Clear Contracts

```python
def add_customer_tier(
    df: pl.DataFrame,
    spend_col: str = "total_spend",
    output_col: str = "tier",
) -> pl.DataFrame:
    """
    Assign customer tier based on total spend.
    
    Tiers:
        - platinum: >= $10,000
        - gold: >= $5,000
        - silver: >= $1,000
        - bronze: < $1,000
    """
    return df.with_columns(
        pl.when(pl.col(spend_col) >= 10_000).then(pl.lit("platinum"))
        .when(pl.col(spend_col) >= 5_000).then(pl.lit("gold"))
        .when(pl.col(spend_col) >= 1_000).then(pl.lit("silver"))
        .otherwise(pl.lit("bronze"))
        .alias(output_col)
    )
```

### Composable Pipelines

```python
def build_customer_features(lf: pl.LazyFrame) -> pl.LazyFrame:
    """Compose transforms into a single optimized pipeline."""
    return (
        lf
        .pipe(normalize_emails)
        .pipe(add_customer_tier)
        .pipe(calculate_recency_score)
        .pipe(add_audit_columns)
    )
```

## The Rule of Three

**Don't abstract prematurely.** Wait until you have three concrete instances before creating an abstraction.

```python
# First instance: Just write the code
def process_sales():
    df = pl.scan_parquet("sales/*.parquet")
    df.filter(pl.col("date") >= "2024-01-01").collect()

# Second instance: Still just write it
def process_returns():
    df = pl.scan_parquet("returns/*.parquet")
    df.filter(pl.col("date") >= "2024-01-01").collect()

# Third instance: NOW abstract
def process_dated_files(
    path_pattern: str,
    date_col: str = "date",
    min_date: str | None = None,
) -> pl.DataFrame:
    lf = pl.scan_parquet(path_pattern)
    if min_date:
        lf = lf.filter(pl.col(date_col) >= min_date)
    return lf.collect()
```

## Anti-Pattern: Copy-Paste Inheritance

```python
# WRONG: Inheritance for code reuse
class BaseTransform:
    def validate(self): ...
    def transform(self): ...
    def save(self): ...

class CustomerTransform(BaseTransform):  # Inherits just to share code
    def transform(self): ...

# CORRECT: Composition and functions
def run_transform(
    loader: Callable[[], pl.LazyFrame],
    transformer: Callable[[pl.LazyFrame], pl.LazyFrame],
    validator: Callable[[pl.DataFrame], None],
    saver: Callable[[pl.DataFrame], None],
) -> None:
    raw = loader()
    transformed = transformer(raw).collect()
    validator(transformed)
    saver(transformed)
```

## Project Structure

```
project/
├── src/
│   └── pipeline/
│       ├── __init__.py
│       ├── config.py         # All configuration/thresholds
│       ├── columns.py        # Column name constants
│       ├── schemas.py        # Schema definitions
│       ├── expressions.py    # Reusable Polars expressions
│       ├── transforms/       # Transform functions by domain
│       │   ├── __init__.py
│       │   ├── customers.py
│       │   └── transactions.py
│       ├── io/               # Read/write operations
│       │   ├── __init__.py
│       │   ├── readers.py
│       │   └── writers.py
│       └── validation/       # Data quality checks
│           ├── __init__.py
│           └── contracts.py
├── tests/
│   ├── unit/
│   └── integration/
└── pyproject.toml
```

## Import Hygiene

```python
# WRONG: Wildcard imports
from transforms import *

# WRONG: Importing everything
from transforms import (
    func1, func2, func3, func4, func5, func6, ...
)

# CORRECT: Import modules, access via namespace
from pipeline import transforms

result = transforms.normalize_customer(df)

# CORRECT: Import only what's needed for this file
from pipeline.transforms import normalize_customer, add_tier
```

## Test Fixture Refactoring Patterns

### Rule of Three for Fixtures

When you have **3+ fixtures with similar patterns**, refactor to a generic factory:

```python
# WRONG: Three separate fixtures with same pattern
@pytest.fixture
def synthetic_dexa_excel_file(tmp_path_factory):
    # ... 50 lines of Excel generation code ...
    
@pytest.fixture
def synthetic_statin_excel_file(tmp_path_factory):
    # ... 50 lines of similar Excel generation code ...
    
@pytest.fixture
def synthetic_complex_excel_file(tmp_path_factory):
    # ... 50 lines of similar Excel generation code ...

# CORRECT: Generic factory with configuration
def _create_synthetic_excel_file(
    tmp_path_factory,
    data: dict,
    filename: str,
    excel_config: dict | None = None,
) -> Path:
    """
    Generic factory for creating synthetic Excel files with caching.
    
    Args:
        tmp_path_factory: Pytest tmp_path_factory fixture
        data: Dictionary of column_name -> list of values
        filename: Output filename
        excel_config: Configuration for Excel layout:
            - header_row: int (default: 0)
            - metadata_rows: list[dict] | None
            - use_dataframe_hash: bool (default: True)
    """
    # Single implementation handles all cases via config
    # Includes caching, error handling, etc.
    ...

# Then fixtures become thin wrappers:
@pytest.fixture
def synthetic_dexa_excel_file(tmp_path_factory):
    """Create synthetic DEXA-like Excel file."""
    data = {...}
    return _create_synthetic_excel_file(
        tmp_path_factory,
        data,
        "synthetic_dexa.xlsx",
        excel_config={"header_row": 0, "use_dataframe_hash": True},
    )
```

### Factory Pattern Standards

**Factory fixtures** for variations:
- `make_semantic_layer` - Creates SemanticLayer with custom data/config
- `make_cohort_with_categorical` - Creates cohorts with categorical variables
- `make_large_csv` - Generates large CSV strings with caching

**Direct fixtures** for standard cases:
- `sample_cohort` - Standard cohort DataFrame
- `sample_patients_df` - Standard patients DataFrame

**Generic factories** for extensibility:
- `_create_synthetic_excel_file(data, config)` - Handles all Excel formats via config
- `make_large_zip(csv_files)` - Handles any number of CSV files

### Extensibility Patterns

**Configuration-driven**: Accept `config` dict for variations
```python
def create_fixture(data: dict, config: dict | None = None) -> Path:
    config = config or {}
    header_row = config.get("header_row", 0)
    # ... use config for variations
```

**Single source of truth**: One implementation, multiple callers
```python
# One factory function
def _create_excel_file(data, config): ...

# Multiple fixtures use it
@pytest.fixture
def fixture_a(tmp_path_factory):
    return _create_excel_file(data_a, config_a)

@pytest.fixture
def fixture_b(tmp_path_factory):
    return _create_excel_file(data_b, config_b)
```

**Open/Closed Principle**: Extend via configuration, not code changes
```python
# Adding new Excel format doesn't require code changes:
@pytest.fixture
def new_format_excel_file(tmp_path_factory):
    return _create_synthetic_excel_file(
        tmp_path_factory,
        new_data,
        "new_format.xlsx",
        excel_config={"header_row": 2, "metadata_rows": [...]},  # New config
    )
```

### Identified Duplicate Patterns

**Excel Fixtures (3 duplicates)**:
- `synthetic_dexa_excel_file`, `synthetic_statin_excel_file`, `synthetic_complex_excel_file`
- Solution: Generic `_create_synthetic_excel_file(data, excel_config)` factory

**Large CSV Fixtures (6 duplicates)**:
- `large_test_data_csv`, `large_patients_csv`, `large_admissions_csv`, etc.
- Solution: Generic `make_large_csv(columns, num_records)` factory

**Large ZIP Fixtures (2 duplicates)**:
- `large_zip_with_csvs`, `large_zip_with_three_tables`
- Solution: Generic `make_large_zip(csv_files: dict[str, str])` factory

### Test Fixture Extraction: Rule of Two

### Mandatory Extraction Rule

**If ANY test setup code appears in 2+ test functions, extract to fixture immediately.**

**Rationale**:
- Single source of truth: Setup logic exists in ONE place
- Maintainability: Changes require ONE update
- Test clarity: Tests focus on behavior, not setup
- DRY compliance: Eliminates duplicate code

### Extraction Checklist

**Before committing test code**:

1. [ ] Scan all test functions for duplicate setup code
2. [ ] If setup appears 2+ times → Extract to fixture
3. [ ] If setup is similar but varies → Create factory fixture
4. [ ] Place fixtures at module level (before test class)
5. [ ] Update all tests to use fixtures
6. [ ] Verify tests still pass
7. [ ] Run `make pre-commit-check` to verify no violations

### Common Patterns

**Storage/Service Initialization**:
```python
# ❌ WRONG: Duplicate in 2+ tests
def test_one(tmp_path):
    storage = UserDatasetStorage(tmp_path / "uploads")
    # ...

def test_two(tmp_path):
    storage = UserDatasetStorage(tmp_path / "uploads")  # DUPLICATE!
    # ...

# ✅ CORRECT: Extract to fixture
@pytest.fixture
def upload_storage(tmp_path):
    return UserDatasetStorage(upload_dir=tmp_path / "uploads")

def test_one(upload_storage):
    # ...

def test_two(upload_storage):
    # ...
```

**Test Data Creation**:
```python
# ❌ WRONG: Duplicate DataFrame creation
def test_one():
    tables = [{"name": "test", "data": pl.DataFrame(...)}]
    # ...

def test_two():
    tables = [{"name": "test", "data": pl.DataFrame(...)}]  # DUPLICATE!
    # ...

# ✅ CORRECT: Extract to fixture
@pytest.fixture
def sample_test_tables():
    return [{"name": "test", "data": pl.DataFrame(...)}]

def test_one(sample_test_tables):
    # ...

def test_two(sample_test_tables):
    # ...
```

**Factory Pattern for Variations**:
```python
# ✅ CORRECT: Factory fixture for variations
@pytest.fixture
def make_metadata_with_pdf():
    def _make(pdf_content=b"...", pdf_filename="test.pdf"):
        return {
            "dataset_name": "test",
            "external_pdf_bytes": pdf_content,
            "external_pdf_filename": pdf_filename,
        }
    return _make

def test_one(make_metadata_with_pdf):
    metadata = make_metadata_with_pdf(pdf_content=b"content1")
    # ...

def test_two(make_metadata_with_pdf):
    metadata = make_metadata_with_pdf(pdf_content=b"content2")
    # ...
```

### Code Review Checklist

Before approving fixture code:
- [ ] No duplicate implementations (check for similar patterns)
- [ ] Factory patterns used for variations
- [ ] Configuration extracted (no magic values)
- [ ] Single source of truth (one implementation per concern)
- [ ] Extensible (new cases via config, not code changes)
- [ ] **Duplicate test setup extracted to fixtures** (Rule of Two enforced)
